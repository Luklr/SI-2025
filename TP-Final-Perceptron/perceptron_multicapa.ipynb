{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c58837",
   "metadata": {},
   "source": [
    "# Implementación en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581ec229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Perceptron con python: https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/#pyis-cta-modal\n",
    "# Perceptron con PyTorch: https://www.geeksforgeeks.org/what-is-perceptron-the-simplest-artificial-neural-network/\n",
    "# Perceptrones multicapa con PyTorch: https://github.com/rasbt/machine-learning-book/blob/main/ch11/ch11.ipynb\n",
    "# Implementación Forward Propagation: https://www.geeksforgeeks.org/what-is-forward-propagation-in-neural-networks/\n",
    "# Implementación Back Propagation: https://www.geeksforgeeks.org/backpropagation-in-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e47521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/annisin/classification-task\n",
    "# Imagenes blanco y negro\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenes a color RGB\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_cifar10(data_dir='cifar-10-batches-py'):\n",
    "    # Cargar datos de entrenamiento\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        batch = unpickle(os.path.join(data_dir, f'data_batch_{i}'))\n",
    "        data.append(batch[b'data'])\n",
    "        labels.append(batch[b'labels'])\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    labels = np.eye(10)[labels]  # One-hot encoding\n",
    "\n",
    "    # Cargar datos de prueba\n",
    "    test_batch = unpickle(os.path.join(data_dir, 'test_batch'))\n",
    "    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    test_labels = np.eye(10)[test_batch[b'labels']]\n",
    "\n",
    "    return data, labels, test_data, test_labels\n",
    "\n",
    "def load_cifar100(data_dir='cifar-100-python'):\n",
    "    data = unpickle(os.path.join(data_dir, 'train'))\n",
    "    x_train = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_train = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    data = unpickle(os.path.join(data_dir, 'test'))\n",
    "    x_test = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_test = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b7d05",
   "metadata": {},
   "source": [
    "¡Perfecto! Aquí tienes una guía paso a paso para implementar una red neuronal multicapa (MLP) para clasificación de imágenes, siguiendo la consigna y los recursos que mencionaste:\n",
    "\n",
    "1. **Preparar los datos**\n",
    "   - Descarga y carga los datasets (MNIST, CIFAR-10, CIFAR-100).\n",
    "   - Preprocesa: normaliza los valores de píxeles y convierte las etiquetas a formato numérico o one-hot.\n",
    "\n",
    "2. **Implementación en numpy (bajo nivel)**\n",
    "   - Define la arquitectura: decide el número de capas ocultas y neuronas por capa.\n",
    "   - Inicializa los pesos y sesgos aleatoriamente.\n",
    "   - Implementa la propagación hacia adelante (forward propagation) usando funciones de activación (sigmoide, ReLU, etc.).\n",
    "   - Implementa la retropropagación (backpropagation) para actualizar los pesos usando el gradiente descendente.\n",
    "   - Entrena la red: itera sobre los datos, calcula la pérdida y ajusta los pesos.\n",
    "\n",
    "3. **Implementación en PyTorch**\n",
    "   - Usa `torch.nn.Module` para definir la red.\n",
    "   - Usa `torch.utils.data.DataLoader` para manejar los datasets.\n",
    "   - Define la función de pérdida y el optimizador.\n",
    "   - Entrena la red y evalúa el desempeño.\n",
    "\n",
    "4. **Experimentación**\n",
    "   - Prueba diferentes arquitecturas: número de capas, neuronas, funciones de activación, tasas de aprendizaje, momentum, inicialización de pesos.\n",
    "   - Compara los resultados y documenta cuál configuración funciona mejor.\n",
    "\n",
    "5. **Comparación con el paper NoProp**\n",
    "   - Lee el artículo y compara tus resultados con los reportados.\n",
    "   - Si encuentras implementaciones públicas de NoProp, prueba y compara su desempeño.\n",
    "\n",
    "6. **Informe**\n",
    "   - Documenta el proceso, los experimentos y las conclusiones.\n",
    "   - Incluye gráficos de precisión, pérdida, etc.\n",
    "\n",
    "¿Te gustaría que te ayude a armar la estructura inicial del notebook o prefieres avanzar por tu cuenta y consultarme dudas puntuales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e72a32",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86e430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Implementación de un perceptrón simple.\n",
    "    Cada perceptrón tiene sus propios pesos y bias, y utiliza la función de activación sigmoide.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        # Inicialización de pesos y bias con valores pequeños aleatorios\n",
    "        # Esto ayuda a romper la simetría y permite que cada neurona aprenda cosas distintas\n",
    "        # self.weights = np.random.randn(input_size)\n",
    "        # self.bias = np.random.randn()\n",
    "        limit = np.sqrt(6 / (input_size + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, input_size)\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        # Convierte la suma ponderada en un valor entre 0 y 1\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        # https://interactivechaos.com/es/manual/tutorial-de-deep-learning/derivada-de-la-funcion-sigmoide\n",
    "        # Derivada de la sigmoide\n",
    "        # x es el valor de la activación sigmoide. Es decir, se ejecutará esto en realidad => sigmoid(x) * (1 - sigmoid(x))\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante de una sola muestra\n",
    "        # Calcula la suma ponderada y aplica la función de activación\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "\n",
    "    def update(self, x, delta):\n",
    "        # Actualiza los pesos y bias usando el gradiente calculado (delta)\n",
    "        # delta ya incluye la derivada de la sigmoide (por la regla de la cadena)\n",
    "        # La actualización sigue la dirección del gradiente descendente\n",
    "        self.weights += self.learning_rate * delta * x\n",
    "        self.bias += self.learning_rate * delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657721fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Red neuronal multicapa compuesta por capas de perceptrones.\n",
    "    Permite definir cualquier cantidad de capas y neuronas por capa.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1):\n",
    "        # layer_sizes: lista con el tamaño de cada capa, EJEMPLO: [784, 100, 10]\n",
    "        self.layers: list[list[Perceptron]] = []\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            capa = [Perceptron(layer_sizes[i-1], learning_rate) for _ in range(layer_sizes[i])]\n",
    "            # EJEMPLO:\n",
    "            # - Para la primera capa oculta generaria 100 neuronas que aceptan vectores inputs de 784 dimensiones (input layer).\n",
    "            # - Para la capa de input no se crea un perceptrón, ya que es la entrada de la red.\n",
    "            # - La capa de salida generaria 10 neuronas que aceptan vectores inputs de 100 dimensiones (capa oculta).\n",
    "            self.layers.append(capa)\n",
    "        \n",
    "        # Mostrar la estructura de la red\n",
    "        # print(\"Estructura de la red:\")\n",
    "        # for i, capa in enumerate(self.layers):\n",
    "        #     print(f\"Capa {i+1}: {len(capa)} neuronas, cada una con {capa[0].weights.shape[0]} pesos\")\n",
    "        # print(\"\\nPesos de la capa intermedia:\")\n",
    "        # for idx, neuron in enumerate(self.layers[0]):\n",
    "        #     print(f\"Neurona {idx}: {neuron.weights}\")\n",
    "        print(\"\\nPesos de la capa de salida:\")\n",
    "        for idx, neuron in enumerate(self.layers[-1]):\n",
    "            print(f\"Neurona {idx}: {neuron.weights}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante para una muestra\n",
    "        activations = [x]   # Inicialmente arranca con el input (x, vector de entrada)\n",
    "        for capa in self.layers:\n",
    "            salida_capa = np.array([neuron.forward(activations[-1]) for neuron in capa])\n",
    "            activations.append(salida_capa)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predicción para una muestra\n",
    "        activations = self.forward(x)\n",
    "        return activations[-1]\n",
    "\n",
    "    def train(self, X, Y, epochs=10, X_val=None, Y_val=None):\n",
    "        \"\"\"\n",
    "        Entrenamiento usando backpropagation.\n",
    "        Ahora guarda el historial de loss y accuracy para graficar.\n",
    "        Si se pasan X_val, Y_val, también calcula métricas de validación.\n",
    "        \"\"\"\n",
    "        history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}  # Diccionario para guardar métricas\n",
    "        # Entrenamiento usando backpropagation (solo para fines didácticos, no optimizado)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for xi, yi in zip(X, Y):\n",
    "                # Forward\n",
    "                activations = self.forward(xi)  # activations es una lista de arrays, donde cada array es la salida de cada capa (función de activación sigmoidea aplicada)\n",
    "                # Backward\n",
    "                deltas = [None] * len(self.layers)\n",
    "                # Capa de salida\n",
    "                error = yi - activations[-1]    # Resta de vectores\n",
    "                total_loss += np.sum((error) ** 2) / 2  # Suma de errores cuadráticos (MSE)\n",
    "                if np.argmax(activations[-1]) == np.argmax(yi):\n",
    "                    correct += 1\n",
    "                deltas[-1] = error * np.array([\n",
    "                    neuron.activation_derivative(activations[-1][j])\n",
    "                    for j, neuron in enumerate(self.layers[-1])\n",
    "                ])\n",
    "                # Capas ocultas\n",
    "                for l in reversed(range(len(self.layers)-1)):\n",
    "                    delta_next = deltas[l+1]    # lista de deltas de la capa siguiente (son 10 deltas)\n",
    "                    pesos_next = np.array([neuron.weights for neuron in self.layers[l+1]])  # En la primera iteracion para MNIST, es un array de arrays de pesos de la capa siguiente -> matriz de 10 filas y 100 columnas\n",
    "                    # deprecated: deltas[l] = self.layers[l][0].activation_derivative(activations[l+1]) * np.dot(pesos_next.T, delta_next)\n",
    "                    deltas[l] = np.array([\n",
    "                        neuron.activation_derivative(activations[l+1][j])\n",
    "                        for j, neuron in enumerate(self.layers[l])\n",
    "                    ]) * np.dot(pesos_next.T, delta_next)   # el resultado del producto escalar (np.dot) es un vector de 100 dimensiones (deltas de la capa oculta)\n",
    "\n",
    "                # Actualización de pesos\n",
    "                for l, capa in enumerate(self.layers):\n",
    "                    for j, neuron in enumerate(capa):\n",
    "                        neuron.update(activations[l], deltas[l][j])\n",
    "                        # para calcular el delta de cada neurona, se usa la derivada de la activación de esa neurona (activations[l+1][j])\n",
    "                        # pero para actualizar los pesos, cada neurona necesita todas las salidas de la capa anterior, no solo una (activations[l]) -> entonces, actualiza cada peso de cada feature, sumando al vector de pesos el vector corrección\n",
    "\n",
    "            acc = correct / len(X)\n",
    "            history['loss'].append(total_loss / len(X))  # Promedio de loss por muestra\n",
    "            history['accuracy'].append(acc)              # Accuracy de entrenamiento\n",
    "\n",
    "            # Validación (si se pasa X_val, Y_val)\n",
    "            if X_val is not None and Y_val is not None:\n",
    "                val_loss = 0\n",
    "                val_correct = 0\n",
    "                for xi, yi in zip(X_val, Y_val):\n",
    "                    out = self.predict(xi)\n",
    "                    val_loss += np.sum((yi - out) ** 2) / 2\n",
    "                    if np.argmax(out) == np.argmax(yi):\n",
    "                        val_correct += 1\n",
    "                history['val_loss'].append(val_loss / len(X_val))\n",
    "                history['val_accuracy'].append(val_correct / len(X_val))\n",
    "            if epoch % max(1, epochs//10) == 0:\n",
    "                # Imprime métricas cada 10% de las épocas\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {history['loss'][-1]:.4f} - Acc: {acc:.4f}\")\n",
    "        return history  # Devuelve el historial para graficar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c5ec30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos un subconjunto pequeño de MNIST para entrenamiento rápido\n",
    "# Usamos solo las primeras 100 muestras para entrenamiento y 10 para test\n",
    "# X_train = x_train.reshape((x_train.shape[0], -1))[:1000]\n",
    "# y_train = y_train[:1000]\n",
    "# X_test = x_test.reshape((x_test.shape[0], -1))[:100]\n",
    "# y_test = y_test[:100]\n",
    "\n",
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 200 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = x_train.reshape((x_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "# 2. Hacemos un split 80/20 sobre ese subconjunto\n",
    "split = int(0.8 * n_samples)\n",
    "X_train = X[:split]\n",
    "y_train = Y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = Y[split:]\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0a3615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [-0.24895096  0.18231164  0.26439038 -0.10454014 -0.12839986 -0.20053704\n",
      "  0.2244552  -0.08460811  0.00876722  0.08135934 -0.13149591  0.15914951\n",
      " -0.0683819  -0.05694793 -0.20582158  0.02330221  0.19146976  0.07806321\n",
      " -0.17897867  0.28658037  0.16955787  0.17125755  0.08706117 -0.26492833\n",
      " -0.21625729  0.28169113  0.02486031  0.16866392 -0.20334517  0.08997168\n",
      "  0.01797582  0.10925218 -0.02164579  0.00199398  0.09167938  0.13580718\n",
      "  0.18720513 -0.18264432  0.00722254 -0.16626638 -0.21773665 -0.11319622\n",
      " -0.16101295  0.09796155  0.30203983 -0.13969632 -0.24636571 -0.27831457\n",
      " -0.28515012 -0.16290185  0.06750532  0.19109678  0.24801986  0.16283518\n",
      "  0.05561495 -0.24265247  0.19046332  0.0654951  -0.03362366 -0.2352141\n",
      "  0.2385619   0.19920572 -0.024162    0.08049944]\n",
      "Neurona 1: [-0.28118317  0.1842128   0.28460518 -0.15202439 -0.23973676  0.00675756\n",
      " -0.22633488  0.04980833 -0.03859114  0.01396732 -0.29299619  0.09550206\n",
      " -0.0033398   0.16035213 -0.17494721  0.04612238  0.10098873 -0.17059853\n",
      "  0.29341231 -0.0105902  -0.2099293   0.24145531  0.2102626   0.0154168\n",
      " -0.03658441 -0.12214384 -0.00335668 -0.259758    0.04933069  0.07489284\n",
      "  0.0082638   0.04064354  0.07439624 -0.11424952  0.13740012 -0.23678927\n",
      "  0.12743199 -0.15413512 -0.29679637  0.14215372  0.2053129  -0.08985517\n",
      " -0.0574427   0.24168498 -0.06854501 -0.13246879  0.01706275  0.06582553\n",
      " -0.12864502 -0.18073757  0.18605749  0.28392935  0.05604352 -0.21514322\n",
      "  0.16812689 -0.30361004  0.0411471   0.25903829  0.09110745  0.06461418\n",
      " -0.24464615 -0.117295   -0.00820922 -0.1156718 ]\n",
      "Neurona 2: [-0.16102719  0.27856666 -0.18717345  0.11014522  0.14472019 -0.25483743\n",
      " -0.24731915 -0.0975306  -0.16444228  0.18430111 -0.0713512  -0.23728918\n",
      "  0.26502378  0.07826327 -0.09519233 -0.02784568 -0.09856368  0.08339118\n",
      "  0.02391658  0.04897221 -0.1278193   0.07174393 -0.09175856  0.15601381\n",
      "  0.10498916  0.12865562 -0.28856613 -0.26263889  0.032302   -0.10116069\n",
      "  0.1237911   0.03340179 -0.06122744 -0.25115888 -0.04521059 -0.23202492\n",
      " -0.09957912 -0.26006739  0.07864833 -0.05909752  0.09045534  0.26686776\n",
      " -0.265469    0.27067446  0.05357941 -0.28956167 -0.25589415  0.26580456\n",
      "  0.25546228  0.07266527 -0.0970995  -0.07263499 -0.17545181  0.19032888\n",
      " -0.03702028 -0.30310926  0.1358484  -0.1681013  -0.16981995  0.21495332\n",
      " -0.087455   -0.06912796 -0.01330359 -0.01420741]\n",
      "Neurona 3: [-0.08583632 -0.15661744  0.16045123 -0.13982977  0.28363928  0.27095669\n",
      " -0.0129765   0.21752534  0.2362903  -0.30224652 -0.29045463 -0.07518682\n",
      "  0.12781787  0.09229269  0.01013854 -0.2996636   0.15787902 -0.04933434\n",
      "  0.2847353   0.21781332 -0.1687767  -0.27222651  0.28986704 -0.09252392\n",
      " -0.13175832  0.23796777 -0.21616085 -0.2215629   0.1158369  -0.04798236\n",
      " -0.27960648  0.02578847 -0.0231961   0.28401573  0.03297368 -0.06424129\n",
      "  0.24118296  0.11313878 -0.17995351 -0.25573693  0.16177435 -0.2322403\n",
      "  0.18863565  0.21223782  0.04300444 -0.15671295 -0.14129852  0.19938493\n",
      "  0.09824267 -0.03064217 -0.11121328 -0.28203821  0.07602048  0.13321257\n",
      "  0.25984953  0.12021644 -0.06939179 -0.1030019  -0.04549831 -0.22435192\n",
      "  0.2339316   0.25418446  0.2902346   0.22509301]\n",
      "Neurona 4: [ 0.14433023  0.21804528 -0.26289652 -0.05455628  0.2148897   0.20112563\n",
      " -0.02360309 -0.22579181 -0.03798546 -0.04811268  0.17296569 -0.09983318\n",
      "  0.12779344 -0.27636748  0.14105902  0.05384018  0.05197616 -0.11187357\n",
      "  0.17135942  0.22110223 -0.12134426  0.23774594  0.05962783 -0.11367805\n",
      " -0.27823857  0.19557847  0.11120808 -0.07478422 -0.22912663  0.2182974\n",
      "  0.04744006  0.29340964 -0.06933021 -0.26684042  0.17544004 -0.23188306\n",
      "  0.23798069 -0.05981699 -0.22074126  0.00953075  0.16007496 -0.06773968\n",
      "  0.12348767 -0.29518569 -0.1646167  -0.0082977   0.09889193 -0.21555156\n",
      "  0.23812427 -0.13502539  0.01215994  0.22701453 -0.30274074  0.14955537\n",
      "  0.16132209  0.26872092  0.19232297  0.08458933 -0.09737653  0.1310466\n",
      " -0.10294696 -0.25598398  0.14735892  0.1955534 ]\n",
      "Neurona 5: [-0.13870706  0.23446424 -0.21842817 -0.21636914 -0.12887772 -0.20285853\n",
      " -0.08665427 -0.2573649   0.14005837 -0.15447267 -0.17185791  0.26807094\n",
      " -0.29603978 -0.16526158 -0.17733095  0.01500185  0.11260222  0.27546493\n",
      " -0.12650413  0.28002566  0.26060772  0.15392029 -0.12494562 -0.08631349\n",
      " -0.01993924  0.13336379  0.18230851 -0.04039696  0.14567385 -0.17849921\n",
      "  0.09508772  0.18431579 -0.28032212  0.18299903  0.23430128 -0.01503897\n",
      "  0.1137892  -0.29530426 -0.19398527  0.12389718 -0.03544057 -0.18943809\n",
      "  0.11808764 -0.02892734 -0.1496742   0.15834502 -0.23805925 -0.08745985\n",
      "  0.24821342 -0.03533574 -0.12321183 -0.05301307  0.19682074  0.05193534\n",
      " -0.18636975 -0.09539634  0.21732163 -0.07865432 -0.02149435 -0.28786191\n",
      "  0.2914091  -0.07266708 -0.08763511  0.26705885]\n",
      "Neurona 6: [ 0.19060749  0.07404635  0.21867282  0.05110906 -0.20479859  0.15973625\n",
      " -0.21346966  0.2318286  -0.05200061 -0.2100319  -0.15837304  0.05723262\n",
      "  0.17678204 -0.20117228 -0.00439198 -0.10256038  0.1874631   0.04583051\n",
      " -0.21777575 -0.01070811  0.06536234 -0.12592655  0.03678583  0.0219611\n",
      "  0.03000636 -0.06688507  0.06176379  0.21476138  0.27337883 -0.24924541\n",
      " -0.23899487 -0.1824899   0.22275256 -0.2986861  -0.00470241  0.29705056\n",
      " -0.24948076  0.14538567  0.15746205 -0.21265517  0.16153965  0.11217025\n",
      " -0.19901898 -0.23760813 -0.28301122 -0.2295587  -0.2998353  -0.21117783\n",
      "  0.11942562 -0.20155258 -0.28900862  0.12156441 -0.15321543 -0.07043787\n",
      " -0.05897622 -0.29931223  0.04772195 -0.07289231  0.1008958  -0.17562245\n",
      "  0.0414946  -0.1931274  -0.28001231  0.17088833]\n",
      "Neurona 7: [-0.23112081  0.06689964  0.06222079 -0.01642692 -0.05391481  0.21441701\n",
      " -0.07085172  0.11823953  0.00890897 -0.0906445   0.05554899 -0.0424652\n",
      "  0.17764483 -0.28535608  0.2058144   0.21639885  0.02405426 -0.19288517\n",
      " -0.07870463 -0.26777829  0.01959983  0.218989   -0.03116996  0.00750009\n",
      "  0.06746937  0.23641825 -0.18478422 -0.24714538  0.28683681  0.05619065\n",
      "  0.12887925  0.22981741 -0.17810371  0.25915651 -0.04655708 -0.17955615\n",
      " -0.29714724 -0.0745261   0.04926074  0.24161445  0.17563345 -0.03444849\n",
      "  0.11103493  0.0539644  -0.16425274  0.16834951  0.0091204   0.07841512\n",
      " -0.20948702 -0.02833652 -0.16008459 -0.12332208  0.11838275 -0.17204275\n",
      "  0.29675247 -0.2607466  -0.28249085 -0.10512021  0.15208804  0.12875205\n",
      "  0.16292941  0.13591484 -0.27174259 -0.05576149]\n",
      "Neurona 8: [-0.20016962  0.17831738  0.24369663 -0.21226721 -0.13928474  0.22983228\n",
      " -0.2280005   0.20325581 -0.16487974  0.06183751  0.05544277 -0.2653641\n",
      " -0.00556301  0.25172561 -0.25009034  0.19616447 -0.27776938 -0.2555251\n",
      " -0.08178735 -0.22961844 -0.13996999  0.04533693 -0.2086272   0.24176061\n",
      " -0.14403949 -0.23648723  0.2886768   0.02448593 -0.11835365 -0.22603089\n",
      "  0.23484485  0.15204178 -0.08730365 -0.28913935 -0.2288979   0.15913573\n",
      "  0.14099199  0.12064547 -0.19629647 -0.01039967 -0.27505819  0.02533862\n",
      " -0.11256729 -0.00536943  0.04909415  0.01355517  0.02499715  0.06065673\n",
      " -0.04183549  0.30250588 -0.30255002 -0.24818717  0.1803406   0.00495313\n",
      "  0.26944688 -0.16987321 -0.26097791 -0.10183179 -0.1155987   0.22311234\n",
      "  0.05199144  0.1900157  -0.20428927  0.20313814]\n",
      "Neurona 9: [ 0.08810925  0.01231573  0.12241805 -0.00695008  0.01936582  0.04655543\n",
      "  0.06897673 -0.27445215 -0.16659557  0.22971302  0.22455066 -0.23275901\n",
      "  0.11722373 -0.06186504 -0.20791282  0.23985528  0.20681075 -0.06361922\n",
      " -0.10863771  0.26039859 -0.13659291  0.18619904 -0.2881972  -0.14627162\n",
      "  0.09069819 -0.20663697 -0.14568167  0.02568713 -0.07011941 -0.07856556\n",
      "  0.27137792  0.0993725  -0.28173777 -0.02431099  0.14274248  0.17114487\n",
      " -0.1514057   0.17825479  0.18296191  0.06888715 -0.24840948  0.2352319\n",
      "  0.08237417 -0.08879168 -0.01723733 -0.02764908 -0.17748945  0.06046078\n",
      " -0.11356695  0.22072213 -0.09516405 -0.13345859  0.12659024  0.10107488\n",
      "  0.08051536  0.07192578 -0.22319764  0.09207978 -0.15087152 -0.02462276\n",
      "  0.12909186 -0.16263868  0.20131037 -0.14179602]\n",
      "Epoch 1/100 - Loss: 0.3741 - Acc: 0.5546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m mlp \u001b[38;5;241m=\u001b[39m NeuralNetwork([input_size, hidden_size1, output_size], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Entrenamos la red (puede tardar unos minutos)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 80\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, epochs, X_val, Y_val)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l, capa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j, neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(capa):\n\u001b[1;32m---> 80\u001b[0m             neuron\u001b[38;5;241m.\u001b[39mupdate(activations[l], deltas[l][j])\n\u001b[0;32m     81\u001b[0m             \u001b[38;5;66;03m# para calcular el delta de cada neurona, se usa la derivada de la activación de esa neurona (activations[l+1][j])\u001b[39;00m\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;66;03m# pero para actualizar los pesos, cada neurona necesita todas las salidas de la capa anterior, no solo una (activations[l]) -> entonces, actualiza cada peso de cada feature, sumando al vector de pesos el vector corrección\u001b[39;00m\n\u001b[0;32m     84\u001b[0m acc \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura: 1 capa oculta de 32 neuronas\n",
    "input_size = X_train.shape[1]  # 784 para MNIST\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32  # Capa oculta adicional\n",
    "output_size = 10  # 10 clases\n",
    "\n",
    "# Creamos la red\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red (puede tardar unos minutos)\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el subconjunto de test: 1855/2000\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fbaf314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [-0.16401604  0.15567599 -0.11319454 -0.26951802 -0.25081176  0.30184885\n",
      " -0.03771953  0.01156215 -0.27296362 -0.22956053  0.0501357  -0.28427692\n",
      " -0.12471506  0.07558758  0.03796599  0.11384342 -0.14985879  0.24287339\n",
      "  0.01152417 -0.00760764  0.07830051  0.13297823 -0.05689369  0.11614255\n",
      " -0.30314096  0.1777578   0.11927604 -0.06869325 -0.14110632  0.02734894\n",
      " -0.04284664 -0.26420878 -0.10174603 -0.30239587  0.26631594  0.13283783\n",
      " -0.05866858  0.14054796 -0.14668488  0.02876585 -0.30229597  0.11565026\n",
      " -0.05797583  0.07881717 -0.2282057  -0.17488418 -0.18769659 -0.07524423\n",
      " -0.03822435 -0.22074759  0.17856357 -0.26741681 -0.03345555  0.10400523\n",
      "  0.13140356  0.21428683 -0.01722493  0.17592196 -0.17844831 -0.04996013\n",
      " -0.2174864  -0.14600714 -0.03578082 -0.10970115]\n",
      "Neurona 1: [-0.27896888 -0.25989599 -0.07427193 -0.18849805  0.1313567  -0.2354803\n",
      "  0.26749975 -0.04025628 -0.15213493 -0.00664127 -0.28708317 -0.16200029\n",
      "  0.27604522 -0.14489774  0.12805314  0.19267011 -0.04409064  0.01544763\n",
      "  0.20669073  0.16283175  0.02868389  0.13702797 -0.01806251 -0.27299976\n",
      " -0.1240012  -0.25164524 -0.15452748  0.07044957 -0.01381702 -0.17161751\n",
      "  0.29905219  0.17649384  0.28815292 -0.14379217  0.13532722 -0.19752223\n",
      " -0.10324777 -0.05918902 -0.29368255  0.21135939  0.1356063  -0.03794781\n",
      " -0.06390487  0.2158575  -0.19204493  0.22346577  0.17198085 -0.15449595\n",
      "  0.04484771 -0.10828447 -0.23617039 -0.12893292  0.09820601 -0.29408687\n",
      "  0.00410614  0.17149393  0.21793121 -0.23410349  0.05887763  0.11041837\n",
      " -0.20723146  0.16954431  0.13543072  0.21472938]\n",
      "Neurona 2: [ 0.22352784 -0.05061843 -0.18593497  0.26502759  0.15024799  0.01061537\n",
      " -0.22341754  0.05809526  0.1064654  -0.02082948  0.18888416 -0.09758045\n",
      "  0.0380161  -0.06638411 -0.01889144  0.01123111  0.04409305  0.09211596\n",
      " -0.19406975  0.1005289  -0.29643455  0.1999157  -0.10889215  0.14623682\n",
      "  0.28995467  0.12074861 -0.25213107 -0.14199425 -0.00111178 -0.19412922\n",
      "  0.2578771  -0.08099121 -0.20386653  0.12964468 -0.07795621  0.29342238\n",
      "  0.24426917  0.27969056  0.14434736  0.06158329 -0.09316913 -0.13354645\n",
      "  0.11750794 -0.07765971  0.28577846 -0.20398427  0.03173705 -0.05427744\n",
      " -0.28271987  0.09540027  0.0223756   0.09204708  0.28654222 -0.06001119\n",
      " -0.05060006 -0.06421915 -0.10759994 -0.12338016  0.10580152 -0.00588094\n",
      "  0.08930046  0.03091952 -0.14722818 -0.19981441]\n",
      "Neurona 3: [-0.05533559 -0.02062488  0.09211554 -0.24101541 -0.08208119  0.02500991\n",
      " -0.22233976 -0.07430864  0.21164135  0.21081866 -0.29148861  0.05935176\n",
      "  0.30093839  0.26537281 -0.18622476  0.22103573  0.14300421  0.29296982\n",
      "  0.2555764   0.06367332 -0.21569249  0.13955017 -0.01583695 -0.17446572\n",
      " -0.03381284 -0.12333347  0.13053254 -0.29387152 -0.06795242  0.12292147\n",
      " -0.17419788  0.26841486  0.12049653  0.10947651 -0.25540702 -0.28919677\n",
      " -0.02534556  0.25775503  0.12293796 -0.13339678 -0.0475651  -0.12104419\n",
      "  0.22972951 -0.02694182 -0.28326244 -0.23020072  0.1157658   0.12070468\n",
      "  0.17238148 -0.14195702 -0.22259962  0.2053581   0.01442718  0.20211084\n",
      "  0.23646457  0.05878236 -0.25478932 -0.15616933 -0.28413463 -0.11151988\n",
      "  0.27437063  0.06609791 -0.05203464 -0.25239713]\n",
      "Neurona 4: [-0.2774044  -0.28875418  0.15330594 -0.07400325  0.16119035  0.27852975\n",
      " -0.17638159  0.02049272 -0.30150205 -0.06787802 -0.2792651  -0.27738511\n",
      " -0.04743475 -0.12897463  0.18412193  0.07739233  0.29668646  0.2081736\n",
      " -0.23227392 -0.27658699  0.20308862  0.29675496 -0.12911514  0.2995133\n",
      " -0.06200244  0.09643093 -0.29799716  0.02515219  0.16522256  0.03654605\n",
      " -0.23470714 -0.1884954  -0.25510786 -0.09205068 -0.02446797  0.08040822\n",
      "  0.10676465  0.08247174  0.23597977 -0.22894376 -0.2803193  -0.30103725\n",
      "  0.30305    -0.28448485 -0.0919594   0.18892413 -0.00486308 -0.0807312\n",
      "  0.17785629  0.01863091 -0.23749798 -0.09882658 -0.16228066  0.0846471\n",
      " -0.07641209  0.25035554 -0.28499541 -0.20734014 -0.12275754  0.27213603\n",
      "  0.21136905  0.24042217 -0.04954727  0.02331849]\n",
      "Neurona 5: [-0.19809119  0.13709921 -0.02069103 -0.20185478  0.22660508 -0.09845666\n",
      " -0.15223325  0.17714018  0.24371862  0.30148486  0.01699996 -0.21879583\n",
      "  0.06781945  0.2357704  -0.16679794  0.06288241  0.01621928  0.1730644\n",
      "  0.01771542 -0.12063987 -0.0792379  -0.19046773  0.1515659   0.22060265\n",
      "  0.11520458 -0.0299326   0.01922606  0.20827893  0.24526203  0.11185879\n",
      " -0.01990399 -0.17401424 -0.2609275  -0.01390301 -0.08585491  0.16787671\n",
      "  0.0356289  -0.03121803 -0.29887344  0.11744481 -0.26490033 -0.24442404\n",
      "  0.12223289 -0.24346205  0.1610432  -0.2628492   0.13514246 -0.1547947\n",
      "  0.19740289 -0.11434392 -0.24324238  0.25156201 -0.13842302 -0.08639114\n",
      " -0.26708523 -0.29807959 -0.23028958  0.02233221  0.0813901   0.06089914\n",
      "  0.23313976  0.05996916  0.24238549  0.16393941]\n",
      "Neurona 6: [ 0.07220503 -0.10322928 -0.21183327 -0.13182441 -0.05673561 -0.23674757\n",
      " -0.12489769  0.21481426  0.06263145  0.16302393 -0.07918906 -0.22012709\n",
      "  0.10591539  0.24939711 -0.12419939  0.17378689  0.01722664 -0.12148024\n",
      "  0.03872106  0.17127521  0.00631159 -0.10651991  0.24840813  0.01228717\n",
      " -0.09851105  0.01750553 -0.27947487 -0.29427872 -0.2902258   0.23355\n",
      " -0.08445813  0.15147235 -0.01875897  0.11446517 -0.0117935  -0.26986461\n",
      "  0.0399769  -0.12142407 -0.07497295  0.09171022 -0.13694184 -0.24620186\n",
      "  0.16518386  0.1950884  -0.17276243 -0.29552636  0.25258583 -0.2980823\n",
      "  0.12562136 -0.25530367  0.07310512  0.03188657 -0.07491847  0.28461671\n",
      " -0.06419494 -0.0116016   0.02596251 -0.02954797  0.0940929   0.09122826\n",
      " -0.2598132  -0.08983424  0.13034756 -0.09799047]\n",
      "Neurona 7: [-0.19346169 -0.27874803 -0.27919569 -0.06433136  0.28247213  0.06500323\n",
      " -0.07867308 -0.0699594  -0.03314022  0.09973775  0.03611511 -0.29508217\n",
      " -0.05560416 -0.24823436 -0.1009728  -0.18008659  0.28482537 -0.22848558\n",
      " -0.22935643 -0.11554679 -0.09612447  0.23384974  0.03610958 -0.17472902\n",
      " -0.14276531 -0.08381581  0.14453659 -0.07597792  0.058173   -0.29879747\n",
      "  0.17123147 -0.20466476  0.00083988  0.27217726  0.12218668 -0.06574049\n",
      "  0.22201951  0.21156946 -0.04819804 -0.27944026  0.08524527  0.24488005\n",
      "  0.28455901  0.13496346 -0.1988886   0.03655358  0.01545396  0.19956653\n",
      "  0.02257447 -0.24088204  0.27886217 -0.04865353 -0.30373384 -0.17625804\n",
      " -0.26727154 -0.04399139  0.24145011  0.25092136 -0.00276941  0.08724865\n",
      " -0.1847583  -0.11745302  0.13713299  0.30135813]\n",
      "Neurona 8: [-0.18924295 -0.10830955  0.03408565  0.10771098 -0.25060111 -0.25361972\n",
      "  0.01822589 -0.10383631 -0.12955419 -0.03321114  0.13738403 -0.02485888\n",
      "  0.18366199 -0.02020353  0.01954851  0.10577218  0.12719674 -0.22809986\n",
      " -0.2590841  -0.24063728 -0.02957053 -0.2605179   0.17899353  0.22450856\n",
      " -0.2646033  -0.00571563  0.1086644  -0.1593171  -0.11478451  0.30283776\n",
      "  0.08626118  0.27945885  0.13822005  0.21540704 -0.16319019 -0.19228893\n",
      "  0.04464995  0.03797634  0.08063216  0.22384825 -0.26410177 -0.17450413\n",
      "  0.11376452  0.12636302 -0.29673548  0.1764691   0.10260333  0.02330341\n",
      " -0.08491914 -0.04841988 -0.14901262  0.03933724 -0.10134057  0.17413403\n",
      "  0.03521431 -0.24705523 -0.0367     -0.12852871 -0.05969924 -0.01386567\n",
      " -0.01070088  0.12236508  0.04990124 -0.00456531]\n",
      "Neurona 9: [-0.28781567  0.00909066  0.28270662  0.07700113  0.13430839 -0.04342801\n",
      " -0.02045293 -0.27737248 -0.27371579  0.14305588  0.10646904 -0.02463613\n",
      " -0.16565228 -0.12626094 -0.09620339 -0.07349453 -0.10675999  0.18621377\n",
      " -0.01627809 -0.21478341  0.01602868  0.22855374 -0.23610327  0.30057276\n",
      "  0.24932741 -0.00742086  0.17291281 -0.0813651   0.00169793  0.27388398\n",
      "  0.07347987  0.25007404  0.29938316 -0.28925333  0.06445014  0.28417561\n",
      "  0.1897627  -0.12247533 -0.00964304  0.17621431 -0.24161675 -0.15933986\n",
      " -0.20324264 -0.13127621 -0.22032966  0.2988954   0.2760731  -0.18910595\n",
      " -0.21793317  0.1520894   0.10043442  0.22281274 -0.11536465  0.10527873\n",
      "  0.08088051  0.17526668  0.12369169 -0.25527565  0.27690592  0.24935855\n",
      "  0.271871   -0.22163717  0.18938441  0.302421  ]\n",
      "Epoch 1/50 - Loss: 0.3761 - Acc: 0.5344\n",
      "Epoch 6/50 - Loss: 0.1125 - Acc: 0.8935\n",
      "Epoch 11/50 - Loss: 0.0842 - Acc: 0.9160\n",
      "Epoch 16/50 - Loss: 0.0716 - Acc: 0.9273\n",
      "Epoch 21/50 - Loss: 0.0635 - Acc: 0.9350\n",
      "Epoch 26/50 - Loss: 0.0575 - Acc: 0.9403\n",
      "Epoch 31/50 - Loss: 0.0528 - Acc: 0.9450\n",
      "Epoch 36/50 - Loss: 0.0489 - Acc: 0.9489\n",
      "Epoch 41/50 - Loss: 0.0456 - Acc: 0.9523\n",
      "Epoch 46/50 - Loss: 0.0428 - Acc: 0.9549\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 10000 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = x_train.reshape((x_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "# 2. Hacemos un split 80/20 sobre ese subconjunto\n",
    "split = int(0.8 * n_samples)\n",
    "X_train = X[:split]\n",
    "y_train = Y[:split]\n",
    "X_val = X[split:]\n",
    "y_val = Y[split:]\n",
    "\n",
    "# Definimos la arquitectura: 1 capa oculta de 64 neuronas\n",
    "input_size = X_train.shape[1]  # 784 para MNIST\n",
    "hidden_size1 = 64\n",
    "output_size = 10  # 10 clases\n",
    "\n",
    "# Creamos la red\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red y guardamos el historial\n",
    "history = mlp.train(X_train, y_train, epochs=50, X_val=X_val, Y_val=y_val)  # epochs puede ajustarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- GRAFICAR LOSS Y ACCURACY ---\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history['loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Evolución de la pérdida')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history['accuracy'], label='Train Acc')\n",
    "plt.plot(history['val_accuracy'], label='Val Acc')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Evolución del accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [ 0.1477677  -0.41738491 -0.08644095 -0.11484555 -0.2707198  -0.19403095\n",
      "  0.35054335  0.22706385 -0.32193181 -0.00324284  0.42421315  0.42534733\n",
      "  0.32508967 -0.30588869  0.35708655  0.37950966 -0.38865308  0.22606246\n",
      " -0.26550155  0.33108974  0.3384983   0.29505282  0.21240173 -0.26799807\n",
      " -0.11307818  0.42165967  0.27484448 -0.1775188  -0.06866606 -0.01617937\n",
      " -0.3635152   0.13564947]\n",
      "Neurona 1: [-0.18194418 -0.1253528  -0.40260105  0.32044135 -0.35735428 -0.02679601\n",
      "  0.00326597  0.2129688   0.03247055 -0.22898612  0.31743093  0.33709559\n",
      "  0.00900901 -0.39691356 -0.29829105  0.18550242  0.34028104  0.33018925\n",
      " -0.11611717  0.01621315 -0.18162352 -0.32001144  0.03703345  0.09502639\n",
      "  0.38825189 -0.38272885 -0.2738584  -0.34856596 -0.40607581  0.3883104\n",
      "  0.3052759   0.30937379]\n",
      "Neurona 2: [-0.32583216 -0.36905487  0.29055989 -0.00968656  0.21323516  0.1680995\n",
      "  0.18045828 -0.01450863  0.36893808 -0.40191485  0.15677245 -0.39201369\n",
      "  0.0178203  -0.04664014  0.34618929  0.25763183 -0.22614211 -0.08269955\n",
      "  0.11854516 -0.14290453 -0.23681153 -0.04762594 -0.10159835 -0.23421209\n",
      " -0.42244128 -0.01435714 -0.19265999 -0.18048474 -0.31649444  0.2213923\n",
      "  0.2463371  -0.01005082]\n",
      "Neurona 3: [ 0.4003017   0.14605393 -0.11112788 -0.24832577  0.34182493 -0.38021794\n",
      "  0.36211594 -0.33767806  0.40691296 -0.40313646 -0.3254123  -0.2310727\n",
      " -0.33997975  0.40799039 -0.2095409   0.129922    0.1006314   0.36407759\n",
      " -0.21107249  0.22197306 -0.36159092 -0.01801864 -0.33930478 -0.23290451\n",
      "  0.00090052  0.32827403 -0.21914782 -0.21311462 -0.4127471   0.12153078\n",
      " -0.28870667  0.3480244 ]\n",
      "Neurona 4: [ 0.29346682 -0.34849168  0.26858904 -0.11772532  0.1743276  -0.06247599\n",
      " -0.4006896   0.34921727  0.30528836 -0.03557723  0.20364227 -0.16817143\n",
      " -0.25917509 -0.07603538  0.30707242  0.0511297   0.11379694  0.21986085\n",
      "  0.24120204 -0.3001363   0.22298996 -0.06621713  0.41572611 -0.40845845\n",
      "  0.06693588 -0.06415109 -0.10051435  0.17415981  0.23243888 -0.36711537\n",
      "  0.16712435 -0.23562779]\n",
      "Neurona 5: [-0.21696903  0.40645922 -0.27798965  0.23490233 -0.0056537  -0.24651222\n",
      " -0.38634627  0.04585843 -0.18508259 -0.08487356 -0.00900208 -0.12654406\n",
      "  0.22176422 -0.25046326 -0.30102763 -0.38160562 -0.27176187  0.15863204\n",
      "  0.11297445 -0.0577168   0.39655374  0.1119176  -0.33448806 -0.15223914\n",
      " -0.366352    0.36696855  0.19079678  0.19937035 -0.09485589 -0.28953454\n",
      "  0.10105247 -0.12507497]\n",
      "Neurona 6: [ 0.38444546 -0.37028223  0.27580306 -0.38712865 -0.27518318 -0.29281971\n",
      " -0.38891706 -0.2405312  -0.21954873  0.01296963  0.3017904   0.11567448\n",
      " -0.06118473  0.16236303  0.40227255 -0.07509476  0.08353281  0.09494017\n",
      "  0.35812918  0.00369025  0.33158241 -0.18389407  0.3128315   0.01976359\n",
      " -0.10160426 -0.1029511   0.14634908 -0.02719063  0.00581238  0.2879292\n",
      "  0.17584404  0.10770797]\n",
      "Neurona 7: [-0.34898872 -0.29956116 -0.23235649  0.07801239  0.13566876  0.37991967\n",
      " -0.3134419  -0.38316447  0.18082638  0.10540557 -0.4234673  -0.37976378\n",
      "  0.37088118 -0.33713632  0.16128446  0.01573336 -0.35581203  0.08943236\n",
      "  0.31917783 -0.25491406 -0.26076622  0.30684267  0.11557036 -0.3582773\n",
      "  0.36212066 -0.14439187 -0.42121605  0.23014703 -0.20726286  0.13412211\n",
      " -0.06494728  0.26307675]\n",
      "Neurona 8: [-0.41422753 -0.17957961  0.19046246 -0.03136898  0.17117781 -0.01711336\n",
      "  0.11051256 -0.3437284   0.24913937  0.16785154 -0.1064571  -0.10519133\n",
      " -0.11918265  0.21953288  0.3469934   0.00536209 -0.33455346  0.40531052\n",
      "  0.30767177  0.39883222  0.2866534  -0.19727931 -0.26742819 -0.35860485\n",
      "  0.03794816 -0.42583912 -0.30982721  0.30964992  0.24367565  0.16785095\n",
      "  0.15544658  0.24854975]\n",
      "Neurona 9: [ 0.03844826  0.06895538 -0.16703827 -0.20024051 -0.13686425 -0.41402511\n",
      " -0.16162889 -0.06191777 -0.04723862 -0.10815641  0.01815477  0.25770524\n",
      "  0.00708289 -0.11774733 -0.01692108  0.39496387  0.04399948 -0.32385259\n",
      " -0.37260487 -0.1026476  -0.05855291 -0.39878578 -0.36306557 -0.30635966\n",
      " -0.15158856  0.26469733  0.14826202  0.02040478 -0.23386741 -0.36685945\n",
      " -0.36732924 -0.12970747]\n",
      "Epoch 1/100 completada\n",
      "Epoch 11/100 completada\n",
      "Epoch 21/100 completada\n",
      "Epoch 31/100 completada\n",
      "Epoch 41/100 completada\n",
      "Epoch 51/100 completada\n",
      "Epoch 61/100 completada\n",
      "Epoch 71/100 completada\n",
      "Epoch 81/100 completada\n",
      "Epoch 91/100 completada\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m mlp\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# (Opcional) Evaluar en el conjunto de prueba\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(X_test, y_test)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "X_train, y_train, X_test, y_test = load_cifar10(f\"data/CIFAR-10\")\n",
    "\n",
    "# Aplanamos las imágenes (de 32x32x3 a 3072)\n",
    "X_train = X_train.reshape(-1, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 200 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = X_train.reshape((X_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "X_train = X\n",
    "y_train = Y\n",
    "\n",
    "# Definimos la arquitectura: dos capas ocultas\n",
    "input_size = X_train.shape[1]  # 3072 para CIFAR-10\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 10  # 10 clases para CIFAR-10\n",
    "\n",
    "# Creamos la red con dos capas ocultas\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el subconjunto de test: 4135/10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [ 0.16149942  0.23627933 -0.28908691  0.39345024 -0.08104062 -0.23324945\n",
      "  0.21863577 -0.28295031  0.1518131  -0.16635039 -0.26036811 -0.12841487\n",
      " -0.2764827  -0.39290738 -0.04446165 -0.34524737  0.30438739  0.29481635\n",
      "  0.15002793  0.42369625  0.40029829 -0.05680715  0.07344367  0.02879389\n",
      " -0.35461295 -0.12476387 -0.05514851  0.29654483  0.27802241 -0.13946806\n",
      " -0.22407028  0.16049434]\n",
      "Neurona 1: [-0.0872708   0.03684712 -0.30554699 -0.39983352 -0.3891758  -0.1882959\n",
      " -0.28902896  0.11698839 -0.25335911  0.33672191  0.13416491  0.34177103\n",
      "  0.40010698  0.11538929 -0.4118487  -0.08633582  0.38075734 -0.34908716\n",
      "  0.38490133  0.25667329 -0.08898306 -0.27569107 -0.11044568  0.27236445\n",
      "  0.16801309  0.20289735 -0.39247359 -0.09478912 -0.13081586 -0.10912207\n",
      "  0.11434673  0.16672428]\n",
      "Neurona 2: [-0.38758939  0.19497116 -0.15027481 -0.00564935 -0.3759918   0.23775133\n",
      "  0.21311233 -0.10136635 -0.21455555  0.02503643 -0.04981712  0.21271956\n",
      " -0.08552419 -0.10182447 -0.30269942 -0.29543758  0.08488379  0.16148413\n",
      " -0.14901046  0.13943315 -0.27854034  0.31011032  0.10808611  0.33172045\n",
      " -0.03216529 -0.17780562  0.25013965 -0.23332769 -0.09970911 -0.27294663\n",
      "  0.22787987  0.01249133]\n",
      "Neurona 3: [-0.12253215  0.14758078  0.13523443  0.22377427  0.07245415  0.19546487\n",
      "  0.3811419   0.2193844   0.29391876 -0.34223118 -0.23259313  0.2499628\n",
      " -0.32632315  0.23509965  0.07145295  0.06497855  0.22462583 -0.14139144\n",
      "  0.01519772  0.06899833  0.29631053  0.09736207 -0.09957339 -0.2501773\n",
      " -0.1851507   0.08760898 -0.36715277 -0.11567671 -0.3252279   0.40368034\n",
      "  0.13509068  0.32996006]\n",
      "Neurona 4: [ 0.41281512 -0.34258864  0.30861785 -0.07119236  0.05655011 -0.07504624\n",
      "  0.32047962 -0.33728149 -0.35642868  0.03396567 -0.05512883 -0.21586375\n",
      "  0.04200523 -0.41845765  0.08359332  0.26624441 -0.25677367 -0.09955138\n",
      "  0.24998689 -0.30413634 -0.35795733  0.28314947  0.02993463 -0.39317111\n",
      " -0.40126269  0.42399707 -0.24613802  0.07651634  0.18839131 -0.0866498\n",
      " -0.37050109 -0.00167082]\n",
      "Neurona 5: [ 0.08423715 -0.00234732  0.08303357 -0.08979693  0.26099359  0.12109273\n",
      " -0.028218    0.15265807  0.20601661  0.42524312 -0.20424253  0.29252221\n",
      " -0.25160794 -0.10620879  0.11244411  0.22868239  0.32718189 -0.35948379\n",
      "  0.17508839 -0.35988689  0.28048196 -0.18808482 -0.13261604 -0.16323436\n",
      " -0.24358132  0.11300064 -0.00792408 -0.32014588 -0.3220051  -0.0223054\n",
      "  0.23641113 -0.08185693]\n",
      "Neurona 6: [-0.23549206  0.18842981 -0.28990195 -0.38518981  0.34925119  0.37526973\n",
      " -0.10911776  0.12673634 -0.38062437  0.23917723 -0.33070254  0.34925224\n",
      " -0.09457569  0.16554089 -0.42632273  0.20723571 -0.06465043  0.01568371\n",
      "  0.01766345 -0.17463636 -0.41156238  0.14371662 -0.13491953  0.01782264\n",
      " -0.28525548  0.42531927 -0.05603128 -0.25282117  0.05193004  0.07040249\n",
      " -0.32469897 -0.09154813]\n",
      "Neurona 7: [-0.08380679  0.35596545  0.36347334  0.42462399  0.11050316 -0.1273428\n",
      " -0.3318779   0.16348491 -0.11752713  0.31762947  0.40840213 -0.34714602\n",
      " -0.41183589  0.33620996  0.04511321 -0.42442533  0.37647454  0.22149842\n",
      " -0.33428121 -0.27163286  0.21577078  0.18546796 -0.1479032  -0.01653277\n",
      " -0.07183721  0.27571344  0.35827921 -0.25319733 -0.04436694 -0.02585578\n",
      "  0.07862302 -0.34809102]\n",
      "Neurona 8: [-0.33054308 -0.39495243  0.24385088  0.13575258  0.14495817  0.25407343\n",
      "  0.20719675 -0.24359282 -0.32392463 -0.10018147  0.36022962 -0.25162802\n",
      "  0.28995446  0.39693934 -0.22860992  0.07317375 -0.07116408 -0.17224362\n",
      "  0.35093883 -0.07430175  0.32776473 -0.15889474 -0.05800578 -0.07340723\n",
      " -0.07107307  0.05814439 -0.12553788 -0.12716154  0.10066279 -0.38502539\n",
      " -0.30179974  0.31677469]\n",
      "Neurona 9: [-0.21983946  0.25227566  0.3630711   0.08946049  0.10661245  0.22650431\n",
      "  0.29440065  0.39496072  0.19345193 -0.11307751 -0.14580511  0.27217807\n",
      "  0.22364081  0.06842396 -0.29282807 -0.23722297 -0.2684741  -0.12685373\n",
      "  0.3637253  -0.16549676  0.26843627  0.11824576 -0.37906219 -0.25729507\n",
      "  0.24588492  0.32842354 -0.05032981  0.20681804  0.32230336 -0.11225822\n",
      " -0.3158074  -0.23853766]\n",
      "Neurona 10: [-0.3153758   0.33105897 -0.03606532 -0.37446648  0.32841867 -0.37982359\n",
      " -0.07503984 -0.10074895 -0.26208187 -0.31791792  0.17944517  0.31649668\n",
      " -0.20843288  0.18753979 -0.24788479 -0.16784488  0.37345245 -0.19751567\n",
      " -0.24671621 -0.36821409  0.13491851 -0.36210284  0.04369539  0.15623178\n",
      "  0.20985261 -0.28563307 -0.2529076   0.31424686 -0.12472364 -0.16295074\n",
      " -0.4002096  -0.23176904]\n",
      "Neurona 11: [ 0.42013913  0.31217642 -0.21547842 -0.20372373  0.34146519  0.02509388\n",
      "  0.13213764  0.03898934  0.35486512  0.22585737  0.34218111 -0.24999447\n",
      "  0.06584539  0.08430242 -0.15010796 -0.26544173 -0.26354024  0.14184786\n",
      " -0.11808818 -0.1196473  -0.23951833 -0.24502985 -0.31769829  0.14682725\n",
      "  0.01898549  0.20516031 -0.13510516 -0.0416656   0.41811646 -0.09904056\n",
      " -0.02081563 -0.25366848]\n",
      "Neurona 12: [-0.3339423   0.1253592  -0.24731864 -0.17686217 -0.39388113 -0.12792393\n",
      " -0.39140931 -0.37506556 -0.24684519 -0.09106522 -0.0511516   0.01773466\n",
      "  0.37356255  0.30633201 -0.25498906 -0.08739708 -0.26346795  0.13052105\n",
      "  0.26951554 -0.19310513  0.26245599  0.26781612  0.05991766 -0.42637349\n",
      " -0.21052514  0.31981867  0.34643778  0.37250815 -0.35069638  0.37147172\n",
      "  0.24614443 -0.03896285]\n",
      "Neurona 13: [ 0.20771655 -0.17732677  0.34413035 -0.27039718  0.14287948  0.01192537\n",
      "  0.02791203 -0.37130223  0.0846274   0.07018306 -0.10527209 -0.19748384\n",
      "  0.26641245 -0.33306317 -0.02730443 -0.42182302  0.26816221  0.27931533\n",
      "  0.16226141  0.22466026  0.33076612 -0.38365235 -0.24403293  0.35069499\n",
      " -0.07608051  0.05955713  0.23406743  0.04999234  0.16899141 -0.08532702\n",
      "  0.02427918  0.35791583]\n",
      "Neurona 14: [-0.212919   -0.05244554  0.35630574  0.0958042   0.05679439  0.17419848\n",
      " -0.25235724  0.29228667 -0.12732618 -0.14425661 -0.41562376 -0.10204769\n",
      "  0.21325012 -0.23400417  0.19806523 -0.09233511 -0.32255877  0.3997638\n",
      "  0.1274915  -0.06176439  0.03030838  0.14370517 -0.22884555 -0.33141579\n",
      "  0.30110847  0.20806298 -0.21099258 -0.0426908   0.00363046  0.11306458\n",
      "  0.40405061  0.08663874]\n",
      "Neurona 15: [-0.29324955  0.42303584 -0.12936119  0.32791969 -0.10605897  0.16821443\n",
      "  0.19634177  0.26924792  0.39335503 -0.21300358 -0.19041376  0.09664851\n",
      "  0.29444415  0.09408293 -0.21160513  0.10848216 -0.12630322  0.3324195\n",
      " -0.00995856 -0.38589747 -0.38753507 -0.41485927 -0.20146125  0.15177875\n",
      " -0.38826349 -0.4081956  -0.02148975 -0.02325736 -0.125233    0.04835764\n",
      "  0.01468335 -0.07429221]\n",
      "Neurona 16: [ 0.11374311 -0.31436835  0.05570348  0.04093862  0.20610814 -0.37806915\n",
      " -0.40598408  0.32856972 -0.1005014  -0.0786508   0.11021668 -0.04283571\n",
      "  0.21471656 -0.20698142  0.14520852  0.37352858 -0.34110746  0.29267433\n",
      " -0.02682598  0.22807065  0.22317478  0.21250114 -0.18013624  0.09141155\n",
      "  0.00188194 -0.07422337  0.14561109 -0.16851284 -0.0426916  -0.25653745\n",
      "  0.41949213 -0.1733788 ]\n",
      "Neurona 17: [-0.15790378  0.1873729   0.32477257 -0.253566   -0.09643577  0.13099807\n",
      "  0.16166122 -0.06318214  0.35850828  0.10575696  0.01069811  0.25947307\n",
      "  0.01839066  0.17463729 -0.35263967 -0.25613722  0.13154599  0.04523065\n",
      "  0.29155666  0.07829902  0.27042471 -0.31228952 -0.02042396  0.14595136\n",
      "  0.08796308  0.3625798   0.06956114 -0.33669743 -0.10167724  0.35582719\n",
      "  0.37913434  0.05376982]\n",
      "Neurona 18: [-0.31510375  0.01834518  0.10185245 -0.0643463   0.16231326  0.12486048\n",
      " -0.41107229 -0.16399667 -0.38792081  0.24184849  0.03720046 -0.30975912\n",
      "  0.29830246  0.42015324 -0.09104904 -0.03015492  0.03913742 -0.34320886\n",
      " -0.02790626  0.10067149 -0.35367034 -0.40851605  0.41829222  0.25500586\n",
      "  0.26978002  0.29826464  0.36022721 -0.37061906  0.03000936 -0.35108861\n",
      "  0.18921443  0.20923091]\n",
      "Neurona 19: [ 0.25663699 -0.23439838 -0.02102599  0.33012043 -0.40608593 -0.12026546\n",
      "  0.02642985 -0.32432006 -0.05692074  0.33231653  0.24220112  0.35706943\n",
      "  0.23397881 -0.40188684  0.27100557  0.26449055 -0.00501393 -0.04361641\n",
      " -0.04462065 -0.1578704   0.20835411 -0.22464263 -0.36940598  0.32409823\n",
      "  0.01470549 -0.05316442 -0.18342645 -0.28968412 -0.02055571 -0.15439892\n",
      " -0.06098868 -0.41309518]\n",
      "Neurona 20: [-0.16065177 -0.24714003  0.11722378 -0.30164845  0.12879298 -0.33607407\n",
      "  0.20274619  0.13792032  0.19883333 -0.06206782  0.08109298 -0.41284458\n",
      " -0.0480006   0.14208741  0.42138619 -0.24288172 -0.26200517 -0.3943166\n",
      " -0.1905042  -0.11977691 -0.28433699  0.33732036  0.22112477  0.10768417\n",
      " -0.30791543  0.05384586 -0.41379076  0.39233233 -0.27475814  0.09391363\n",
      "  0.09150332 -0.05197449]\n",
      "Neurona 21: [-0.01840177 -0.01036461 -0.03319713  0.13518238  0.18066196 -0.38196439\n",
      " -0.23882495  0.17889732 -0.22311751  0.31080343 -0.35339847 -0.318734\n",
      "  0.41904031 -0.08691342 -0.40263347 -0.03978252 -0.40974169 -0.23194452\n",
      "  0.20745264 -0.15534374  0.20917334  0.21593957  0.37676028 -0.01036914\n",
      "  0.03259381 -0.34888101  0.37055471  0.42298482 -0.24956    -0.27772897\n",
      "  0.26497178 -0.09372659]\n",
      "Neurona 22: [-0.18535303  0.07231323  0.40051797 -0.02050398 -0.41422936  0.14103689\n",
      " -0.37150837  0.21850274 -0.04758703 -0.19813228 -0.04699055 -0.06305393\n",
      " -0.24374016 -0.12222329  0.18439382 -0.30083887 -0.00332872 -0.0315065\n",
      "  0.05518901  0.41219033  0.35780418 -0.06555465  0.30280065 -0.40818636\n",
      "  0.04718758  0.00774856 -0.14536235 -0.37355439 -0.0578027  -0.14162758\n",
      " -0.34730048 -0.35068809]\n",
      "Neurona 23: [-0.31232401 -0.10406043 -0.05849139 -0.4022313   0.02388262  0.08696032\n",
      " -0.12211216 -0.08976306 -0.00865878 -0.39572573 -0.15603209  0.41820871\n",
      "  0.04314398 -0.15237558  0.1672622  -0.08944998  0.05122097  0.15308201\n",
      "  0.22979892 -0.17411591 -0.01255961  0.21864743 -0.39673947  0.14091071\n",
      "  0.39176316 -0.04197024  0.05952832 -0.21763526 -0.28779888  0.41555635\n",
      " -0.33347909  0.01451488]\n",
      "Neurona 24: [ 0.09827148  0.23559786 -0.12847037  0.25870819 -0.12076646 -0.036296\n",
      "  0.05633581  0.15432319 -0.00102394 -0.34182693 -0.38791616  0.2549792\n",
      "  0.27214771  0.31611323  0.2907906  -0.38242513  0.30126592  0.31680353\n",
      " -0.41291858 -0.0900024  -0.29412969 -0.07725996  0.00194227 -0.17183613\n",
      "  0.04515613  0.12172531  0.37319174 -0.03576256 -0.01229906  0.01991067\n",
      " -0.37582845  0.32076245]\n",
      "Neurona 25: [-0.14265584  0.27857658  0.1612484  -0.22743136  0.0005503   0.36560289\n",
      " -0.04830889  0.19640348  0.16537068 -0.15282013  0.35485408 -0.15589183\n",
      "  0.33890375 -0.23440427 -0.04839187  0.1339119   0.13620271 -0.33339343\n",
      " -0.36148528  0.40272885  0.1013027   0.0870513  -0.29213557 -0.1883941\n",
      "  0.08292766 -0.06121341  0.0597046  -0.17748258 -0.35790503  0.13554047\n",
      " -0.04741611  0.23208223]\n",
      "Neurona 26: [ 0.36689982 -0.06867119 -0.12616231 -0.02075738  0.15485354  0.05106845\n",
      "  0.22092871 -0.0268905   0.37865515  0.04850969  0.02132475  0.34517279\n",
      " -0.13692993 -0.03483382 -0.16448244  0.09273762 -0.40333346 -0.0910623\n",
      "  0.40333414 -0.42241905 -0.11346418  0.11273398  0.1410115  -0.30205575\n",
      " -0.32029271 -0.03828515 -0.06875728  0.10486998  0.37340694 -0.23751213\n",
      "  0.39712495  0.24581891]\n",
      "Neurona 27: [-3.06314776e-01 -1.58076286e-01  2.23651223e-01  1.21390797e-01\n",
      "  6.43976781e-02 -3.52944770e-04  1.72500149e-01  3.10385214e-01\n",
      "  2.97298410e-01 -7.60936805e-02  3.08018140e-01  4.14994264e-01\n",
      "  1.31397463e-01  3.43261343e-01 -7.89617094e-02  3.74159662e-01\n",
      "  1.17783691e-01 -1.39903974e-01  3.02575751e-02  7.95718759e-02\n",
      "  7.44906759e-02  8.93383569e-02 -1.38525744e-02  2.03092158e-02\n",
      "  2.47502445e-01  4.15728058e-01 -3.91904289e-01 -1.31019424e-01\n",
      "  1.45428466e-01  1.21361026e-03 -3.79491905e-01 -9.59401551e-03]\n",
      "Neurona 28: [-0.04203201  0.00654509 -0.39833965 -0.41779453 -0.39292259  0.286396\n",
      " -0.24235996  0.2300549  -0.18719919 -0.1073009  -0.29485183 -0.15493866\n",
      "  0.35117994 -0.065313    0.36788898  0.29964399 -0.34037077  0.16807522\n",
      "  0.40287859  0.12060656 -0.18849572  0.13371353 -0.2841804  -0.29845069\n",
      "  0.08344302 -0.39055298 -0.15700856  0.16318022  0.42595225 -0.12358414\n",
      "  0.1731843  -0.12171088]\n",
      "Neurona 29: [ 0.19420831 -0.10833393  0.23812345  0.15053486 -0.37311046  0.39024644\n",
      "  0.33671642  0.17250969 -0.30367726  0.05679555 -0.35582773  0.24147391\n",
      "  0.16535618  0.35122282 -0.23594969 -0.13081583 -0.26340213  0.31310029\n",
      "  0.39168145 -0.02211297  0.15484826  0.0452833   0.34096759  0.20932664\n",
      "  0.13843016 -0.17086017  0.39138023 -0.04919549 -0.32899319 -0.30258727\n",
      " -0.36124447  0.26421974]\n",
      "Neurona 30: [ 0.12724884  0.31210173  0.34128078 -0.21364555  0.1961153   0.416779\n",
      " -0.13535006  0.14790819  0.2082574  -0.19337527  0.20226482  0.28683265\n",
      " -0.33863494 -0.35707085  0.10534054  0.32102406  0.33742565 -0.13685627\n",
      "  0.37772771  0.11910706  0.16909221  0.02309448 -0.1815652  -0.13994225\n",
      "  0.3295665   0.10882804  0.18973826  0.00611803 -0.42294128  0.10853449\n",
      "  0.39286457  0.2100467 ]\n",
      "Neurona 31: [ 0.28672868  0.28140845  0.14079426  0.27347352  0.20060671 -0.14602958\n",
      " -0.417149   -0.04521006  0.35509842 -0.03147212 -0.01747454 -0.0728309\n",
      "  0.15485959 -0.02384197  0.10406336  0.38383925  0.24053259 -0.11892711\n",
      "  0.28573893 -0.27475433  0.28699821  0.03437473 -0.29102646  0.34809617\n",
      " -0.37139242  0.40501215  0.16638793 -0.01509442  0.3303723   0.18656172\n",
      "  0.00782893  0.31220432]\n",
      "Neurona 32: [-0.2955915  -0.1924028   0.40466572  0.3327947  -0.08044149 -0.30794963\n",
      " -0.23328688 -0.25230899 -0.36859711  0.34851095 -0.42265997 -0.18578908\n",
      "  0.14083458  0.3329841  -0.38326517  0.05312908  0.41103404 -0.42501172\n",
      " -0.04720876 -0.33112486  0.1042103  -0.3652277  -0.07600419  0.21468944\n",
      " -0.41451009  0.29234073  0.19754374  0.39507995 -0.16336039  0.0761675\n",
      " -0.32193243  0.14390591]\n",
      "Neurona 33: [-0.27925447 -0.11229766 -0.4025903   0.40674631  0.32547108 -0.21666382\n",
      "  0.13048842 -0.1358554  -0.17215337 -0.32798665  0.17540585  0.34599892\n",
      "  0.10187306 -0.07496892 -0.17991169 -0.26642565  0.12613905  0.1819392\n",
      " -0.29478038  0.03769062  0.3293169  -0.19539536  0.27213676 -0.04287892\n",
      "  0.22026437 -0.39193065  0.14648622 -0.13133425 -0.07071406 -0.23513909\n",
      " -0.25953932 -0.3615577 ]\n",
      "Neurona 34: [-0.10593625  0.16367069 -0.3637875   0.02295126 -0.26073025  0.14011432\n",
      " -0.18029919 -0.18436036  0.28658007  0.04811446 -0.4213292  -0.15691437\n",
      "  0.29670489  0.1499993   0.4034288   0.10447622  0.38473466 -0.00069836\n",
      " -0.31423412  0.01086703 -0.08164715 -0.01987689 -0.12832416  0.41043748\n",
      "  0.17798687 -0.18113815 -0.30124164 -0.05083137 -0.3987406  -0.11539483\n",
      " -0.20887648 -0.05649043]\n",
      "Neurona 35: [-0.36920885  0.00862276  0.29723974  0.03156092  0.12863805  0.24158483\n",
      "  0.41251347 -0.1492627   0.04304592 -0.08023937 -0.00530341 -0.13441855\n",
      " -0.40730219 -0.4139799   0.30764082  0.33706315  0.1447918   0.39672303\n",
      "  0.36798595 -0.04921186 -0.17690073  0.03861339  0.38438958  0.11433658\n",
      "  0.30722381  0.04874948 -0.05742332  0.16889815  0.03918876 -0.32564594\n",
      " -0.36678551  0.16868424]\n",
      "Neurona 36: [-0.42322613 -0.24680686  0.27449111  0.00609524 -0.41235347 -0.1870933\n",
      " -0.31260135 -0.12916517 -0.07652223  0.38742528  0.32534115 -0.00839928\n",
      " -0.39302653 -0.28629753  0.2480214  -0.38205103 -0.25685444 -0.38773957\n",
      " -0.38091103 -0.00073641 -0.2086553   0.05720201 -0.01480147 -0.01362137\n",
      "  0.18821286  0.13509552  0.09151185  0.03892528  0.06688426 -0.22147676\n",
      " -0.10571563  0.23770026]\n",
      "Neurona 37: [ 0.06249093 -0.11430076  0.2745185   0.34912175 -0.33131184  0.05875758\n",
      "  0.10125433 -0.28702195 -0.29493522  0.16305471 -0.31493567 -0.20730009\n",
      "  0.05498309 -0.12818494 -0.4177568   0.11656011  0.35417689  0.28697302\n",
      " -0.37948661 -0.02192917 -0.41576703  0.19516519 -0.13159577  0.28793645\n",
      "  0.36600253 -0.36926586 -0.18885916  0.42539369 -0.30207884 -0.31036215\n",
      " -0.1048411  -0.38946443]\n",
      "Neurona 38: [-0.41032942  0.24126298  0.39676972 -0.05083019  0.38553536 -0.29239033\n",
      "  0.04226073  0.1890041  -0.06627694  0.06691948 -0.17791183  0.41124557\n",
      " -0.29636724 -0.25849934  0.38268201 -0.07801311 -0.30329378  0.29947223\n",
      "  0.06467227 -0.09850845  0.28972086  0.31639685 -0.17771768 -0.0346373\n",
      "  0.21346284 -0.12099393 -0.42357216  0.25180969  0.0041994   0.40146014\n",
      "  0.36040487  0.3980054 ]\n",
      "Neurona 39: [ 0.22993238 -0.18375821 -0.07250211  0.32967766 -0.18371188 -0.21959008\n",
      " -0.34774087  0.01229296  0.27605404  0.17092901  0.296247    0.34070291\n",
      " -0.14989349  0.22820739 -0.39242282  0.05395831 -0.33771481 -0.18998041\n",
      " -0.08085733  0.01931834  0.4095583  -0.01291305 -0.02440034  0.30270845\n",
      "  0.04865769 -0.23840898  0.41349334  0.30631007 -0.23806368  0.27166637\n",
      "  0.03240833 -0.20993654]\n",
      "Neurona 40: [ 0.15297331  0.41456    -0.20390099 -0.20469167  0.1427567  -0.09444988\n",
      " -0.29988866  0.4224975   0.42620454  0.06047621 -0.02302052  0.28464115\n",
      "  0.24695733 -0.35508372 -0.32900861 -0.00939275 -0.29917406  0.29991974\n",
      "  0.33891562 -0.06009077  0.26568075 -0.39469364 -0.16270255 -0.11045186\n",
      " -0.37269934  0.2432289  -0.34017575  0.25097826 -0.0605949   0.3336176\n",
      " -0.05047655  0.23981391]\n",
      "Neurona 41: [ 0.38281924 -0.39234215 -0.03915428  0.15521693  0.27080473  0.06285902\n",
      "  0.34945503  0.2552741  -0.09552308  0.17462823 -0.41465716  0.39057327\n",
      "  0.11976481  0.23625814  0.31451199 -0.21459682 -0.3570053   0.28895376\n",
      "  0.16855085 -0.27841566 -0.42297889 -0.08599432  0.04322603 -0.21272131\n",
      " -0.22025819  0.15698916 -0.06560213  0.02755424 -0.30754052 -0.17369385\n",
      " -0.09438859  0.3756106 ]\n",
      "Neurona 42: [-0.12240349  0.31085993  0.18630076  0.15713708  0.14770316  0.35506636\n",
      " -0.23247066  0.16516467 -0.36367172 -0.37532173  0.23464986 -0.29194869\n",
      "  0.17207359  0.39039703  0.17589985 -0.04319434 -0.4168065  -0.3024226\n",
      "  0.04477476  0.20219487  0.1999362   0.0601006   0.01890344  0.20976352\n",
      "  0.27369046  0.20472224  0.09398789 -0.04604258 -0.34854769 -0.29336214\n",
      " -0.02813717  0.1986536 ]\n",
      "Neurona 43: [-0.38432088 -0.24514502  0.097012    0.03370653 -0.15399795 -0.10883902\n",
      "  0.12536818 -0.21905614 -0.14613407  0.27320646 -0.26962757 -0.35840363\n",
      "  0.23645998  0.22447733  0.10233846 -0.22769113  0.381336   -0.09723387\n",
      "  0.23961125 -0.09947832  0.13424041 -0.37765121 -0.27247746 -0.06675204\n",
      " -0.39405314 -0.0500119   0.18350847  0.06592282  0.41650366  0.36882441\n",
      " -0.03961857 -0.29773627]\n",
      "Neurona 44: [-0.26773434  0.34573996  0.00305207 -0.12686718 -0.25987729  0.12170487\n",
      "  0.1029739  -0.02395669 -0.1933618  -0.27412137  0.23891212 -0.27083913\n",
      "  0.24988965 -0.09421641 -0.14577385 -0.41098477 -0.04126472 -0.00423712\n",
      "  0.02706192  0.38899612  0.18301125 -0.2680567   0.28281864  0.3202704\n",
      "  0.2456388   0.3120259  -0.05885362 -0.18237083 -0.08294191  0.41991028\n",
      "  0.15699402  0.23122513]\n",
      "Neurona 45: [-0.1256598  -0.08113645 -0.03595568  0.26445925  0.30037941 -0.05030206\n",
      "  0.41687236  0.23591657  0.36369156 -0.03665034  0.11346062  0.10025476\n",
      "  0.33031168 -0.18224318 -0.34606492 -0.18265318  0.40767203 -0.29109671\n",
      " -0.03836947  0.34318067 -0.08014659 -0.30432318  0.0525067   0.25106291\n",
      " -0.36601704 -0.07034196 -0.02997368  0.32516754  0.02785192  0.04468834\n",
      " -0.07597791 -0.0255615 ]\n",
      "Neurona 46: [ 0.15225391  0.13071664  0.16724017 -0.36920916  0.36491891  0.13625896\n",
      "  0.32580366 -0.13555071  0.0262224  -0.14355403  0.39964367  0.14112275\n",
      "  0.16809478 -0.38519266 -0.05796479  0.0569389   0.36046876 -0.22040133\n",
      "  0.19599095 -0.29797706  0.10841489 -0.07959538  0.01591222 -0.3378901\n",
      "  0.05815458 -0.41643662  0.32885322  0.23316687  0.11124969  0.09683624\n",
      "  0.38261318  0.39396621]\n",
      "Neurona 47: [-0.10478298  0.14684949 -0.15217131  0.35697105 -0.25431843 -0.28680084\n",
      "  0.1608829   0.03367171  0.42599694  0.35405733 -0.36716921 -0.37628514\n",
      "  0.18545352 -0.22042118 -0.4100763   0.27453164  0.30933323 -0.34833847\n",
      "  0.10375436  0.11809248  0.22558027 -0.20597452 -0.01891561  0.0416596\n",
      " -0.12450024 -0.09033564  0.17667609 -0.38241848  0.05193023 -0.22989139\n",
      "  0.18901975 -0.36321878]\n",
      "Neurona 48: [-0.11349737  0.15637437  0.27109973  0.40177149 -0.26529719  0.07758479\n",
      " -0.28145221  0.42215598  0.1396877  -0.19625851 -0.03491855  0.37436974\n",
      "  0.1674953   0.06853954  0.35949051  0.2803267  -0.23493571  0.31402996\n",
      "  0.06906933  0.41742599  0.06633002  0.36249426  0.19724705  0.29790251\n",
      " -0.41996787 -0.09652527  0.3298475   0.38323828  0.23041314  0.33900476\n",
      " -0.15161773  0.06393551]\n",
      "Neurona 49: [-0.20145927 -0.42604266 -0.02490833 -0.03113749  0.38690153 -0.27131812\n",
      " -0.07747745  0.01772197  0.20061377 -0.30149092  0.25598806 -0.40272344\n",
      " -0.10898506  0.24576298 -0.37692685 -0.15461107  0.29090925  0.02314521\n",
      " -0.34214071  0.30030338 -0.00422827  0.30223021 -0.16391378  0.30807523\n",
      " -0.38645165  0.01403765  0.01845677 -0.37411795  0.16828221  0.13937396\n",
      " -0.38132779  0.32203112]\n",
      "Neurona 50: [ 0.39153396  0.06161125  0.06966408  0.1707752   0.08652016 -0.17999135\n",
      "  0.08642023 -0.30326448  0.18023057 -0.11266727  0.31670944 -0.11618686\n",
      " -0.4028277  -0.3923167   0.11280044 -0.05436706  0.03734632 -0.40443974\n",
      "  0.3083297   0.17257303 -0.36748004 -0.22459991  0.09261979  0.01976863\n",
      " -0.30587062  0.3863592   0.23183426 -0.16398877  0.11082504  0.2441408\n",
      "  0.36603006 -0.12226726]\n",
      "Neurona 51: [ 0.21581536 -0.07353265 -0.10706526  0.11361802 -0.20011931  0.00988646\n",
      "  0.15153183  0.1136291  -0.14286206  0.37328849 -0.04896182 -0.24722895\n",
      "  0.06611657 -0.29319605  0.16802015 -0.36884888  0.15337172  0.22249586\n",
      " -0.30576869  0.15099363  0.01279747 -0.29415612  0.13535306 -0.32974971\n",
      "  0.20647457  0.42038719  0.0956625   0.40730048 -0.14692238 -0.00432512\n",
      "  0.08497955 -0.38993197]\n",
      "Neurona 52: [-0.21857611 -0.21627719  0.19237812  0.27518865  0.14861426 -0.32330219\n",
      "  0.02791585 -0.34625759 -0.18372917 -0.31355224 -0.30603332 -0.20041215\n",
      " -0.17610032  0.05588505 -0.19710572 -0.30361129 -0.23484934  0.31760715\n",
      "  0.18172654  0.01834198 -0.07116106 -0.09571801 -0.28453995 -0.14719183\n",
      " -0.11296642  0.06171551 -0.19835128 -0.35311034  0.38739567 -0.30046382\n",
      "  0.08390658  0.17685505]\n",
      "Neurona 53: [ 0.37572396 -0.03747733  0.03617357 -0.36122802 -0.11645131 -0.05727269\n",
      " -0.38008305  0.07894929 -0.41064942  0.19461913  0.094909   -0.00799098\n",
      " -0.37627462 -0.05873359 -0.12589711  0.34244529  0.36646953  0.18462895\n",
      " -0.36360248  0.39635998  0.35905758 -0.12804158 -0.26547675 -0.02977482\n",
      "  0.07540535 -0.30430844 -0.00267096 -0.09083696  0.16082758  0.0230563\n",
      "  0.40226222 -0.33632141]\n",
      "Neurona 54: [ 0.28812357 -0.07794005 -0.13077131 -0.06861884  0.08869991 -0.25913763\n",
      " -0.07969839 -0.13234526  0.10819408  0.42307324 -0.29370008 -0.08911776\n",
      " -0.02156031 -0.31974222  0.29924492  0.2001584   0.28205497 -0.24687755\n",
      "  0.3831926  -0.07189746 -0.07456032  0.23241653 -0.11818151 -0.42146214\n",
      "  0.37967941 -0.24362459 -0.06644494 -0.04131065 -0.19653079  0.21701353\n",
      "  0.20998595  0.13155245]\n",
      "Neurona 55: [-0.31740221  0.21758714 -0.26468933  0.33010943 -0.1485547  -0.37846126\n",
      "  0.28688245  0.01983126  0.40505668 -0.32110786  0.38010729 -0.27481398\n",
      "  0.37287562  0.03362087 -0.20658866 -0.06971234 -0.00098631 -0.01118543\n",
      "  0.39394095  0.20351449 -0.38808955 -0.26345333 -0.00437439 -0.35897675\n",
      "  0.26453617  0.3393249   0.33820175 -0.36848677  0.17435608 -0.00833726\n",
      " -0.12792293  0.36572825]\n",
      "Neurona 56: [-0.32361753  0.02400969 -0.19556651 -0.12721228  0.40523379 -0.26962138\n",
      "  0.08121687  0.05820896  0.00387493  0.31089058 -0.20712552  0.11337744\n",
      "  0.2130043   0.11852208  0.03556125 -0.35991059 -0.12608137 -0.35250409\n",
      " -0.13725878 -0.3905467  -0.28424319  0.06036318 -0.22497674 -0.33007577\n",
      "  0.34905582 -0.18088083  0.20857063 -0.22242259  0.11114226  0.23281161\n",
      "  0.09720395 -0.42265884]\n",
      "Neurona 57: [ 0.07582589  0.20493701 -0.06808814 -0.0293642   0.06884597  0.2299367\n",
      "  0.27868644  0.30442927  0.41007733  0.1844827  -0.38401032  0.36976923\n",
      "  0.0890912   0.37113208  0.14861621 -0.12978228  0.19274246  0.39859769\n",
      "  0.26629657 -0.14001862  0.05331778 -0.19299931 -0.05125934 -0.16104171\n",
      "  0.17091146 -0.28726859  0.12904361  0.02838205 -0.30807325  0.0142639\n",
      "  0.06842785 -0.12602603]\n",
      "Neurona 58: [-0.39880187  0.39808211  0.36507364  0.32656984  0.28256261 -0.04614088\n",
      " -0.38403634 -0.31102653  0.24779812 -0.29044318  0.03050457 -0.30611627\n",
      "  0.16641012  0.41022764  0.05211681  0.23935636 -0.37236021  0.10985848\n",
      " -0.17760862 -0.37078572  0.22693103 -0.40369213  0.19802098  0.07270815\n",
      " -0.31300595 -0.2709043  -0.05759853 -0.39730251 -0.3286107  -0.14610602\n",
      "  0.29750195  0.23568988]\n",
      "Neurona 59: [-0.25267111  0.12213044 -0.02743804  0.02508178  0.1521845  -0.29092453\n",
      "  0.18319872 -0.10613381  0.29666689 -0.07292732  0.26336161 -0.39338673\n",
      "  0.23622614 -0.01977374 -0.4047945  -0.35352721 -0.10849594  0.07475871\n",
      "  0.24818167  0.41960234 -0.34387259 -0.30439782  0.15618849  0.35600479\n",
      " -0.03737796 -0.28390536 -0.11228715 -0.04660689 -0.35888959  0.27603056\n",
      " -0.19300405 -0.23268705]\n",
      "Neurona 60: [ 0.4180686   0.22518429 -0.14773925  0.23444003  0.13700956  0.24521065\n",
      " -0.3971569   0.34552349 -0.33099736 -0.36439072  0.17392012  0.42296571\n",
      " -0.39059548  0.00539998  0.16248114  0.28946729 -0.02778295 -0.22013911\n",
      " -0.25575465  0.20961137 -0.21066196  0.04995775  0.29099086 -0.15319366\n",
      " -0.2215268  -0.39609679  0.28226317  0.0474713   0.07220167  0.05548241\n",
      "  0.03784073 -0.26508661]\n",
      "Neurona 61: [-0.20810726  0.30836061 -0.17172931 -0.15867024  0.02958973 -0.39564314\n",
      "  0.30292096  0.07887349 -0.32042094  0.37683307 -0.23432613  0.37823323\n",
      " -0.22261847 -0.04905075  0.18720663  0.27855967 -0.3695827  -0.19669031\n",
      " -0.31398506 -0.20372294  0.33473287 -0.0526156  -0.28377354 -0.32417201\n",
      "  0.38984421 -0.18836567 -0.1674205   0.36340431 -0.29512969 -0.16909237\n",
      "  0.13606933  0.42170367]\n",
      "Neurona 62: [-0.03716906 -0.12938928  0.25591224  0.18776457  0.16511403  0.30979979\n",
      " -0.0246476   0.06053814 -0.15179991  0.12442043  0.14987452 -0.36923688\n",
      " -0.22973443 -0.30343203  0.01755099 -0.08543606 -0.11321669  0.03042987\n",
      " -0.27817513 -0.22920809  0.01904637  0.33220735  0.39720273  0.31523666\n",
      " -0.12295245  0.05891195 -0.21424582  0.32713148  0.05650027  0.02176454\n",
      "  0.42350903 -0.10519324]\n",
      "Neurona 63: [-0.28178267 -0.04495422  0.41792286 -0.22892991  0.16984352 -0.04380691\n",
      " -0.22902262 -0.25116205  0.11782017  0.38034015  0.29916252 -0.12846907\n",
      " -0.26450185 -0.41469881  0.36282688  0.12775555  0.31343736  0.21158316\n",
      "  0.36075499  0.25983553 -0.12490049 -0.32951596  0.25769212  0.37338908\n",
      " -0.08933344  0.22293163  0.25301915  0.36616783 -0.20629931  0.19398885\n",
      "  0.15323281  0.07299445]\n",
      "Neurona 64: [-0.11071721  0.17855895 -0.35633339  0.41236802  0.13986288 -0.39465773\n",
      "  0.00160164  0.26467053  0.11041449  0.18315423 -0.35663612  0.34915475\n",
      " -0.33712512 -0.15676021  0.16494215  0.34611836 -0.1569121   0.20780914\n",
      "  0.06116947 -0.03810153  0.03502069  0.28428861 -0.13997393  0.02569573\n",
      "  0.17114845  0.35382135  0.21485643 -0.20544885  0.31700931  0.33630782\n",
      "  0.2953272  -0.36829094]\n",
      "Neurona 65: [ 0.16351184 -0.39673311  0.01808325  0.32529434  0.27572606  0.18125597\n",
      "  0.31471425  0.12399555 -0.14450351  0.37501693 -0.10863626  0.10330096\n",
      "  0.00382155  0.17709953 -0.09605895  0.10351627 -0.39701654  0.23087923\n",
      "  0.27882859 -0.20101767  0.01669701  0.23076165 -0.32794533 -0.40272089\n",
      "  0.17429932 -0.07951534 -0.39942743  0.00163831 -0.04300832  0.32765323\n",
      "  0.13857514  0.20618104]\n",
      "Neurona 66: [-0.31490316  0.09906415  0.34179132  0.12736172  0.19584733 -0.25771893\n",
      " -0.03855095  0.16390906  0.37466333  0.16042874  0.39947667 -0.02626483\n",
      "  0.05931498  0.14438793  0.28658551  0.37457     0.34546669 -0.04680574\n",
      "  0.02531394 -0.38902446 -0.41598126  0.39355031 -0.29118475  0.3518118\n",
      " -0.36803923 -0.3958322   0.12036994  0.18235191  0.02979443 -0.25995469\n",
      "  0.28705332 -0.20724903]\n",
      "Neurona 67: [-0.30621763 -0.2714942   0.01297565  0.32690151 -0.37794478  0.11597158\n",
      " -0.38618447 -0.07575346 -0.15050598 -0.07097197 -0.01581137  0.1668472\n",
      " -0.34807224  0.1368972   0.13807424 -0.1808847   0.14309742 -0.11667681\n",
      "  0.20799489  0.36067969  0.03373155 -0.01968111  0.35503659 -0.00902441\n",
      " -0.16621332 -0.10611576  0.25995589  0.40295088 -0.03980248 -0.37320281\n",
      " -0.12600656 -0.38514317]\n",
      "Neurona 68: [-0.1767462   0.04077035  0.05896468 -0.0522953   0.0817423  -0.29053498\n",
      "  0.22110201 -0.13386824  0.03041107  0.25846322  0.18665567  0.35841438\n",
      "  0.33038877 -0.08010126  0.2814935  -0.13963609 -0.22003007 -0.21610081\n",
      "  0.27691952  0.30829095  0.30396491 -0.05924124  0.42407316  0.23039776\n",
      "  0.34363362  0.22997127 -0.16922165  0.42629221 -0.28690004 -0.42634848\n",
      " -0.0324155  -0.24721538]\n",
      "Neurona 69: [ 0.33828441 -0.22785675  0.21715071 -0.27031168 -0.41618504  0.42122558\n",
      " -0.35843516  0.15342951  0.06921198  0.22211269 -0.38863315 -0.14077499\n",
      " -0.2291843   0.3583968   0.40814845 -0.31046633  0.37367009  0.10833277\n",
      " -0.23831441  0.37586014 -0.18093403  0.39818293 -0.10935415 -0.29605453\n",
      " -0.22816939  0.10619958 -0.10368821  0.3072143  -0.22370539 -0.24234685\n",
      "  0.23203692 -0.33180539]\n",
      "Neurona 70: [-0.14695476  0.00954985 -0.24446343  0.3577616  -0.21600934  0.2365855\n",
      "  0.20954232 -0.10601147  0.16081485  0.40267423 -0.39713979 -0.2456032\n",
      "  0.03202318 -0.30046777  0.40286188  0.37767337  0.40272317  0.02197768\n",
      " -0.16839969  0.380264    0.14203416 -0.07377265  0.0971244  -0.20459037\n",
      "  0.23347133  0.39223331 -0.24300676  0.27410891 -0.13509153 -0.20296641\n",
      "  0.12519959 -0.16451293]\n",
      "Neurona 71: [ 0.09730098  0.15791684  0.15925134 -0.00275607 -0.19973453 -0.04183284\n",
      " -0.29054544  0.06037514 -0.33412125 -0.16598858  0.04921173 -0.41791439\n",
      " -0.37732777  0.25826221  0.20737737  0.24174262 -0.04729288 -0.20131715\n",
      " -0.11627216 -0.12183736  0.38839829  0.17053047  0.11496711 -0.40985873\n",
      "  0.02055232 -0.32951371 -0.38216265  0.30670214 -0.02391497  0.42358549\n",
      " -0.20925275  0.11826567]\n",
      "Neurona 72: [ 0.29294992  0.10187313  0.12189927  0.2434986   0.09987296 -0.17443598\n",
      " -0.19981754 -0.17951861  0.24726674  0.33832575 -0.19416455  0.12711826\n",
      " -0.17364453 -0.25621418 -0.2997728  -0.09496791 -0.07150984 -0.30063096\n",
      " -0.12504906  0.00352627  0.08334198 -0.36992373 -0.0503139   0.16423389\n",
      " -0.24403505  0.18236969 -0.08823947  0.38705481  0.21274965  0.06836703\n",
      "  0.00958002  0.0291303 ]\n",
      "Neurona 73: [ 0.35225336  0.13562668 -0.10487124 -0.08734971 -0.01059285  0.34763036\n",
      " -0.19021505 -0.01405206 -0.3094215  -0.19793558  0.39743019 -0.13903244\n",
      " -0.15437239 -0.12531007  0.33076726  0.02855437 -0.28935364 -0.01042036\n",
      "  0.11591089 -0.00907078  0.40999051 -0.38695155  0.2934421  -0.35865265\n",
      " -0.14075193 -0.26964136 -0.31706747 -0.26930186  0.02747586  0.03079667\n",
      " -0.40713838  0.12327531]\n",
      "Neurona 74: [ 2.50095919e-02  3.65347338e-01  1.60521902e-02 -2.26963653e-01\n",
      " -2.59049340e-01 -1.50732893e-01  1.54474472e-01 -9.69684064e-02\n",
      "  4.16355599e-01 -4.19912405e-01 -2.51799208e-01 -5.29169141e-02\n",
      "  2.18106111e-02 -3.24816848e-01 -6.25647624e-02  2.98164739e-01\n",
      " -3.66432123e-01  1.26844138e-01  1.83119339e-01  3.11785323e-01\n",
      " -1.45151471e-01 -3.28808903e-01 -4.82573258e-03  1.67117598e-01\n",
      " -1.30870039e-04  2.33921342e-01 -4.21814447e-01 -3.70218509e-01\n",
      " -1.21067299e-02  2.84214533e-01  1.82191866e-01 -2.10707520e-01]\n",
      "Neurona 75: [-0.18788833 -0.1555846  -0.28792099 -0.01140622  0.24820291  0.07552789\n",
      " -0.01386546 -0.40154886 -0.02858091 -0.05749885  0.02188531 -0.2852442\n",
      "  0.10633509 -0.29444913 -0.05009694 -0.21418982  0.13470473  0.31019338\n",
      "  0.39168651 -0.36538921 -0.41721264  0.31600079 -0.42560292  0.36600597\n",
      " -0.40680561 -0.19297117  0.07574958 -0.13027164 -0.38308563  0.3395018\n",
      " -0.2985146  -0.14748769]\n",
      "Neurona 76: [ 0.12505356 -0.07595733 -0.2766405   0.18477815  0.27778255 -0.25042808\n",
      "  0.4026184  -0.3782403   0.25062663  0.251656    0.17865125  0.01654657\n",
      " -0.20637672  0.17125064 -0.23885967 -0.38158577 -0.07942772  0.2706812\n",
      " -0.06444547  0.07764061  0.24805851 -0.08203077 -0.07689181 -0.34272322\n",
      "  0.3487042   0.17261296  0.32345867  0.1022      0.42120953  0.39565716\n",
      " -0.03480064 -0.09626809]\n",
      "Neurona 77: [ 0.02076972  0.05552611  0.39255799  0.04311757 -0.12389399 -0.15518713\n",
      "  0.25570931  0.38370472 -0.09471701 -0.41045028  0.12247846  0.0804591\n",
      "  0.10288247  0.06267182  0.1351536  -0.23821205 -0.2503935  -0.25465077\n",
      " -0.41053725 -0.17399277 -0.40140593 -0.33574562 -0.12597578  0.15418089\n",
      "  0.3702978   0.17430984  0.16343841 -0.12505008 -0.28492495  0.1222099\n",
      " -0.24064735  0.36405163]\n",
      "Neurona 78: [-0.31674737  0.15582436 -0.39075688  0.27833035  0.01740234 -0.24856634\n",
      "  0.18437234  0.06101299 -0.13164568 -0.38492508 -0.12475914  0.15733819\n",
      " -0.3603628  -0.2058561  -0.03329425  0.09203137 -0.12677944 -0.15821442\n",
      " -0.2732147  -0.00498429  0.0422823   0.14415879  0.27742201  0.13155578\n",
      " -0.20818917  0.24922482  0.24823349 -0.1774771  -0.20905246 -0.41815289\n",
      " -0.08029117  0.30630408]\n",
      "Neurona 79: [-0.0741183   0.32526448 -0.39887116 -0.07437829  0.16691541 -0.17051043\n",
      " -0.0430337  -0.09972578  0.20195436 -0.03485032  0.16566345  0.24218181\n",
      " -0.06302749  0.34941629 -0.13190866 -0.0825674   0.11774282 -0.05516054\n",
      "  0.01478256  0.42370619 -0.14237855 -0.17216794 -0.20954893  0.30197085\n",
      " -0.20087922 -0.25404889 -0.1537577  -0.22090624 -0.14577996  0.28380196\n",
      " -0.09970928  0.27007905]\n",
      "Neurona 80: [-0.03709911 -0.42525984  0.38929935 -0.07133471 -0.18373817  0.27197817\n",
      " -0.07092811  0.08714449  0.41415878  0.25941226 -0.04126171 -0.16345846\n",
      " -0.27254217  0.12290138  0.18565565 -0.36375425  0.15965897  0.10915237\n",
      " -0.08857299  0.01831902  0.31753991  0.31303914  0.02481035  0.15512493\n",
      " -0.30526466 -0.02444952 -0.42132612 -0.35385442  0.19479637  0.07132516\n",
      " -0.11357656  0.00641752]\n",
      "Neurona 81: [ 0.19345537  0.23692316  0.1555606   0.02353277 -0.01765507  0.21826525\n",
      " -0.2895946   0.18859292  0.3994729  -0.17575658  0.23890605 -0.2230599\n",
      " -0.22098828  0.29263463 -0.23580335 -0.19490768  0.15862884 -0.22667804\n",
      " -0.09648048  0.16888933 -0.31957616 -0.08776665  0.11800384 -0.27657211\n",
      "  0.08285055  0.13610265  0.09733202 -0.00318621 -0.33364854 -0.29635528\n",
      " -0.2984405  -0.27201919]\n",
      "Neurona 82: [ 0.27269944 -0.39812002 -0.2975919   0.04334057  0.23974801  0.0164778\n",
      " -0.02095519  0.19371203 -0.40549954 -0.05131799 -0.09074085 -0.10967525\n",
      " -0.02864129 -0.34753412  0.01946932  0.267389    0.40412438 -0.02509007\n",
      "  0.30568574  0.42261282  0.03087392  0.1638335  -0.41969174  0.37795309\n",
      "  0.296544   -0.388057    0.28985478  0.31576658 -0.08549552 -0.30726636\n",
      " -0.19942187  0.29781277]\n",
      "Neurona 83: [-0.39167812 -0.41611082 -0.00532489  0.05805155  0.26509412 -0.0418554\n",
      "  0.36331963 -0.34462436  0.07698547  0.29842688  0.20337909  0.38186584\n",
      " -0.3207065  -0.09803921  0.27924709  0.27160361 -0.41571812 -0.0557454\n",
      "  0.06657602  0.2820591   0.25313327 -0.15121     0.40631735 -0.0048306\n",
      " -0.38932616 -0.29022486 -0.38140603  0.38179783  0.21072127 -0.11054473\n",
      "  0.38829804 -0.10605029]\n",
      "Neurona 84: [-0.18180016  0.36799044  0.08782973 -0.05975827  0.04986987  0.11822006\n",
      " -0.30600903 -0.00600832 -0.08137872  0.2083107  -0.20350866 -0.28511054\n",
      " -0.10077164 -0.07125108 -0.3614637  -0.03739206  0.31321763  0.30171541\n",
      "  0.40745271  0.2466636  -0.3876495  -0.02784438  0.23166961  0.26312677\n",
      "  0.32016282 -0.2729662   0.02282952  0.2359956  -0.32665196 -0.09789683\n",
      "  0.32570087  0.35629026]\n",
      "Neurona 85: [-0.3336562  -0.15141594  0.17396454 -0.09528343  0.1953726  -0.26352868\n",
      "  0.20141608  0.0171594  -0.05306424  0.29822499  0.3031949  -0.22599254\n",
      " -0.05654627  0.35474007  0.1119583   0.32995231  0.39219362  0.29956246\n",
      "  0.21586498  0.2396999   0.17234155 -0.26068701  0.24008561  0.06651811\n",
      "  0.3304189  -0.23317454  0.22754333 -0.17210125 -0.254961    0.13662998\n",
      "  0.32218352 -0.16698779]\n",
      "Neurona 86: [ 0.28068591  0.37653575 -0.18421892 -0.37417438  0.07983976 -0.14979298\n",
      " -0.16116246  0.3635281   0.39401613 -0.19510047 -0.2762459   0.23917205\n",
      " -0.19628634 -0.37185878  0.23658651 -0.42576696 -0.08314549 -0.03000721\n",
      " -0.04071228 -0.24237056  0.14117149  0.27312683  0.41026293 -0.04145619\n",
      " -0.32847471  0.17459427 -0.14022081 -0.35323892 -0.13570109  0.38493687\n",
      " -0.21643346  0.10844009]\n",
      "Neurona 87: [-0.32186338  0.03574658  0.34846119 -0.32752669  0.28669215  0.0498712\n",
      " -0.38547571  0.41797166 -0.40156646 -0.41633171 -0.12924186  0.08165015\n",
      " -0.06044873  0.0279995   0.1534009   0.09747232  0.05599276 -0.00947931\n",
      "  0.39303582 -0.42315952 -0.01451327  0.17124038  0.41819303  0.11090647\n",
      " -0.31843504 -0.11124707  0.34426877 -0.20973669  0.30056456  0.39742785\n",
      "  0.37700713 -0.40024147]\n",
      "Neurona 88: [-0.12783044 -0.04591846  0.22866164  0.39237379 -0.29195169 -0.13228705\n",
      "  0.27008294  0.11016689  0.28916414 -0.03324114  0.42231696 -0.38059082\n",
      " -0.39702536  0.25523884 -0.20273484  0.05148483  0.00717209  0.18887483\n",
      "  0.06375872  0.1599204  -0.2239817   0.32401418  0.10106869 -0.06108008\n",
      "  0.2596369   0.3877445  -0.074973   -0.32553144  0.16398673 -0.2160955\n",
      "  0.16211989 -0.32500969]\n",
      "Neurona 89: [ 0.40155009 -0.00358124  0.25281265  0.213951    0.06907409 -0.30347343\n",
      "  0.02731484  0.29033894 -0.08920389 -0.01079277  0.35651165 -0.01109793\n",
      " -0.22335946  0.16089112  0.39354808 -0.26785588 -0.01001739 -0.05287736\n",
      " -0.11893536 -0.27754874 -0.4117857  -0.12504156 -0.1512548  -0.08076705\n",
      " -0.4157261   0.20016015 -0.35881628 -0.03267944  0.04153951  0.39713742\n",
      "  0.11346329 -0.25436603]\n",
      "Neurona 90: [-0.1105349  -0.33527038 -0.33526263 -0.20357592  0.28698384  0.1492435\n",
      "  0.06211739  0.25060047 -0.38714407  0.14457883  0.06860843  0.31564771\n",
      "  0.23088343 -0.29471542 -0.01598176 -0.21875082  0.19816395 -0.11923636\n",
      " -0.35872463 -0.20699971 -0.22784502 -0.28406791  0.13810016  0.04982944\n",
      "  0.00582934 -0.24591322  0.40827358  0.19908988  0.0693779   0.20636892\n",
      "  0.15705485 -0.38268214]\n",
      "Neurona 91: [ 0.34717365 -0.32576327 -0.35961899 -0.1817532   0.26682755  0.16473108\n",
      "  0.37427573  0.17008484  0.39599399 -0.1295991   0.42260055  0.40288762\n",
      " -0.05024953  0.39317835 -0.01257732  0.02617683 -0.22169939  0.26265623\n",
      " -0.17911697 -0.27129092  0.08601754  0.12731666  0.20078009  0.33516259\n",
      "  0.3350538  -0.27876208 -0.21204082  0.3589588   0.37492242  0.00253389\n",
      "  0.22366702  0.09542923]\n",
      "Neurona 92: [-0.20277628 -0.05384991 -0.23204605  0.20511102 -0.30843945  0.10566453\n",
      "  0.20529054  0.24108966  0.40077545 -0.3788499   0.11642681  0.08557857\n",
      "  0.12027132 -0.28508144  0.29173765  0.10968767  0.31569893 -0.27404857\n",
      "  0.21932719  0.02251562 -0.02477059 -0.03233335  0.41374187  0.02132301\n",
      "  0.41285811  0.25709125 -0.35955788 -0.05719116 -0.11989905  0.13373608\n",
      "  0.06552256 -0.2233194 ]\n",
      "Neurona 93: [-0.00594642 -0.08028256 -0.00607463  0.08996938 -0.34027026 -0.13866055\n",
      " -0.18510234 -0.09296945 -0.42031795 -0.33051627  0.30962024  0.34119783\n",
      "  0.34766779 -0.07837561 -0.04223725 -0.09629914 -0.29508125  0.06133316\n",
      " -0.36097629 -0.29857603 -0.36562221  0.41453054 -0.18968576 -0.20981016\n",
      "  0.35998129  0.04274249 -0.37583843 -0.02284187 -0.23377144  0.42490122\n",
      " -0.16527871  0.28027669]\n",
      "Neurona 94: [ 0.37557311  0.24896286 -0.13687086 -0.10973556  0.39311702 -0.17768513\n",
      " -0.41675366 -0.15635253 -0.17579877  0.41486189  0.04327513 -0.15638868\n",
      " -0.10152128  0.32891542  0.29568236 -0.09812031 -0.05951777 -0.01587168\n",
      "  0.05154786  0.20771912 -0.20130332 -0.34576696  0.16684263 -0.17417389\n",
      " -0.07782916  0.31078698  0.02038487  0.22069884  0.30066313 -0.17919355\n",
      " -0.10179715 -0.13410764]\n",
      "Neurona 95: [-0.02907467 -0.37630826 -0.10992641  0.33126133  0.1105894   0.18131513\n",
      " -0.35525311  0.4071025   0.28184323  0.32921638 -0.41143574  0.08809206\n",
      " -0.32593224  0.08610197  0.02761924 -0.30617046  0.05788007 -0.08172135\n",
      " -0.20921598 -0.40543616  0.18151532 -0.07076283  0.24760086 -0.27175991\n",
      " -0.2413387   0.25497216 -0.30873351 -0.40510614 -0.0535705   0.19157418\n",
      " -0.39289675 -0.14020722]\n",
      "Neurona 96: [ 0.39429746  0.19703814  0.00772216  0.13231031  0.1338776  -0.0810927\n",
      " -0.11310851  0.21870451 -0.25917744 -0.1767757   0.27995294 -0.05791357\n",
      "  0.17281446  0.10679924  0.31971419  0.34882264  0.06136564  0.35954985\n",
      " -0.04328216  0.23328618  0.32619449 -0.31484774 -0.07831274  0.18149927\n",
      " -0.21363463 -0.06375953  0.21698564  0.2991311  -0.41632079  0.00661008\n",
      "  0.03204231 -0.16360331]\n",
      "Neurona 97: [-0.1745303  -0.02466123  0.2068112   0.25390981 -0.17342285  0.36378636\n",
      "  0.36029808 -0.19120475 -0.36263986 -0.01150927  0.0026549   0.17050848\n",
      " -0.12211356  0.37363222 -0.01759317  0.41861161 -0.36112303 -0.40038748\n",
      "  0.41800915 -0.21133592  0.22561225  0.42121401 -0.07441118 -0.14515042\n",
      " -0.41932225  0.2799499   0.24406832 -0.30445068  0.30645561 -0.35964592\n",
      " -0.1871147  -0.22192989]\n",
      "Neurona 98: [ 0.19935924  0.23032571  0.27838054 -0.40712448 -0.27101867 -0.24841185\n",
      " -0.05440814 -0.06975567 -0.01575099 -0.1730384  -0.08103335  0.18668264\n",
      " -0.13774347  0.40609919  0.40513083  0.27970319  0.18639173  0.22962803\n",
      " -0.26269314 -0.23772009 -0.03120076 -0.20659712 -0.09066975  0.12102104\n",
      "  0.2373968  -0.03702763 -0.33932302  0.14819121 -0.06776493  0.25046724\n",
      " -0.14729889 -0.07997881]\n",
      "Neurona 99: [-0.32728328  0.10310348  0.27490657  0.07870853 -0.37955165  0.21878108\n",
      " -0.34794607  0.28918307  0.05704703 -0.28877954  0.40608888 -0.06114893\n",
      "  0.40186204 -0.40955411  0.16164316  0.38571538  0.08544129 -0.0630706\n",
      " -0.33345613  0.33295503  0.37483808  0.26558733 -0.33191439  0.01942221\n",
      "  0.12045439  0.35826249 -0.28916364 -0.21910643  0.29930736  0.30970968\n",
      " -0.00974666 -0.07097468]\n",
      "Epoch 1/100 completada\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m mlp \u001b[38;5;241m=\u001b[39m NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Entrenamos la red\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, epochs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m         activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# activations es una lista de arrays, donde cada array es la salida de cada capa (función de activación sigmoidea aplicada)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         deltas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m activations \u001b[38;5;241m=\u001b[39m [x]   \u001b[38;5;66;03m# Inicialmente arranca con el input (x, vector de entrada)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m capa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 33\u001b[0m     salida_capa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m capa])\n\u001b[0;32m     34\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(salida_capa)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mPerceptron.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Propagación hacia adelante de una sola muestra\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Calcula la suma ponderada y aplica la función de activación\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "X_train, y_train, X_test, y_test = load_cifar100(f\"data/CIFAR-100\")\n",
    "\n",
    "# Aplanamos las imágenes (de 32x32x3 a 3072)\n",
    "X_train = X_train.reshape(-1, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = X_train.reshape((X_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "X_train = X\n",
    "y_train = Y\n",
    "\n",
    "# Definimos la arquitectura: dos capas ocultas\n",
    "input_size = X_train.shape[1]  # 3072 para CIFAR-100\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 100  # 100 clases para CIFAR-100\n",
    "\n",
    "# Creamos la red con dos capas ocultas\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación con Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d5cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360782e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/annisin/classification-task\n",
    "# Imagenes blanco y negro\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkPyTorch:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, device=None):\n",
    "        \"\"\"\n",
    "        layer_sizes: lista con tamaño de capas, e.g., [3072, 64, 32, 100]\n",
    "        \"\"\"\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.build_model(layer_sizes).to(self.device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def build_model(self, sizes):\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            layers.append(nn.Sigmoid())  # Usamos Sigmoid para imitar tu perceptrón\n",
    "        # Capa final (sin Sigmoid, CrossEntropy incluye softmax internamente)\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def train(self, X, Y, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        X: tensor (n_samples x input_dim)\n",
    "        Y: tensor de etiquetas (n_samples,) tipo long (no one-hot)\n",
    "        \"\"\"\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_Y in loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_Y = batch_Y.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.loss_fn(outputs, batch_Y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % max(1, epochs // 10) == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: tensor (n_samples x input_dim)\n",
    "        return: tensor con índices de clases predichas\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X = X.to(self.device)\n",
    "            outputs = self.model(X)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "        return predictions\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 200 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = x_train.reshape((x_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "# 2. Hacemos un split 80/20 sobre ese subconjunto\n",
    "split = int(0.8 * n_samples)\n",
    "X_train = X[:split]\n",
    "y_train = Y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = Y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 2.3000\n",
      "Epoch 11/100 - Loss: 1.7372\n",
      "Epoch 21/100 - Loss: 1.0731\n",
      "Epoch 31/100 - Loss: 0.7597\n",
      "Epoch 41/100 - Loss: 0.6038\n",
      "Epoch 51/100 - Loss: 0.5137\n",
      "Epoch 61/100 - Loss: 0.4557\n",
      "Epoch 71/100 - Loss: 0.4154\n",
      "Epoch 81/100 - Loss: 0.3855\n",
      "Epoch 91/100 - Loss: 0.3625\n"
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura: 1 capa oculta de 32 neuronas\n",
    "input_size = X_train.shape[1]  # 784 para MNIST\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32  # Capa oculta adicional\n",
    "output_size = 10  # 10 clases\n",
    "\n",
    "# Creamos la red\n",
    "mlp = NeuralNetworkPyTorch([input_size, hidden_size1, output_size], learning_rate=0.01)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
    "\n",
    "# Entrenamos la red (puede tardar unos minutos)\n",
    "mlp.train(X_train_tensor, y_train_tensor, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el subconjunto de test: 88.75%\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(X_test_tensor)\n",
    "accuracy = (preds == y_test_tensor).float().mean().item()\n",
    "print(f\"Precisión en el subconjunto de test: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
