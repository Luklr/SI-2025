{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c58837",
   "metadata": {},
   "source": [
    "# Implementación en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581ec229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Perceptron con python: https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/#pyis-cta-modal\n",
    "# Perceptron con PyTorch: https://www.geeksforgeeks.org/what-is-perceptron-the-simplest-artificial-neural-network/\n",
    "# Perceptrones multicapa con PyTorch: https://github.com/rasbt/machine-learning-book/blob/main/ch11/ch11.ipynb\n",
    "# Implementación Forward Propagation: https://www.geeksforgeeks.org/what-is-forward-propagation-in-neural-networks/\n",
    "# Implementación Back Propagation: https://www.geeksforgeeks.org/backpropagation-in-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e47521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/annisin/classification-task\n",
    "# Imagenes blanco y negro\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenes a color RGB\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_cifar10(data_dir='cifar-10-batches-py'):\n",
    "    # Cargar datos de entrenamiento\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        batch = unpickle(os.path.join(data_dir, f'data_batch_{i}'))\n",
    "        data.append(batch[b'data'])\n",
    "        labels.append(batch[b'labels'])\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    labels = np.eye(10)[labels]  # One-hot encoding\n",
    "\n",
    "    # Cargar datos de prueba\n",
    "    test_batch = unpickle(os.path.join(data_dir, 'test_batch'))\n",
    "    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    test_labels = np.eye(10)[test_batch[b'labels']]\n",
    "\n",
    "    return data, labels, test_data, test_labels\n",
    "\n",
    "def load_cifar100(data_dir='cifar-100-python'):\n",
    "    data = unpickle(os.path.join(data_dir, 'train'))\n",
    "    x_train = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_train = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    data = unpickle(os.path.join(data_dir, 'test'))\n",
    "    x_test = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_test = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b7d05",
   "metadata": {},
   "source": [
    "¡Perfecto! Aquí tienes una guía paso a paso para implementar una red neuronal multicapa (MLP) para clasificación de imágenes, siguiendo la consigna y los recursos que mencionaste:\n",
    "\n",
    "1. **Preparar los datos**\n",
    "   - Descarga y carga los datasets (MNIST, CIFAR-10, CIFAR-100).\n",
    "   - Preprocesa: normaliza los valores de píxeles y convierte las etiquetas a formato numérico o one-hot.\n",
    "\n",
    "2. **Implementación en numpy (bajo nivel)**\n",
    "   - Define la arquitectura: decide el número de capas ocultas y neuronas por capa.\n",
    "   - Inicializa los pesos y sesgos aleatoriamente.\n",
    "   - Implementa la propagación hacia adelante (forward propagation) usando funciones de activación (sigmoide, ReLU, etc.).\n",
    "   - Implementa la retropropagación (backpropagation) para actualizar los pesos usando el gradiente descendente.\n",
    "   - Entrena la red: itera sobre los datos, calcula la pérdida y ajusta los pesos.\n",
    "\n",
    "3. **Implementación en PyTorch**\n",
    "   - Usa `torch.nn.Module` para definir la red.\n",
    "   - Usa `torch.utils.data.DataLoader` para manejar los datasets.\n",
    "   - Define la función de pérdida y el optimizador.\n",
    "   - Entrena la red y evalúa el desempeño.\n",
    "\n",
    "4. **Experimentación**\n",
    "   - Prueba diferentes arquitecturas: número de capas, neuronas, funciones de activación, tasas de aprendizaje, momentum, inicialización de pesos.\n",
    "   - Compara los resultados y documenta cuál configuración funciona mejor.\n",
    "\n",
    "5. **Comparación con el paper NoProp**\n",
    "   - Lee el artículo y compara tus resultados con los reportados.\n",
    "   - Si encuentras implementaciones públicas de NoProp, prueba y compara su desempeño.\n",
    "\n",
    "6. **Informe**\n",
    "   - Documenta el proceso, los experimentos y las conclusiones.\n",
    "   - Incluye gráficos de precisión, pérdida, etc.\n",
    "\n",
    "¿Te gustaría que te ayude a armar la estructura inicial del notebook o prefieres avanzar por tu cuenta y consultarme dudas puntuales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e72a32",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Implementación de un perceptrón simple.\n",
    "    Cada perceptrón tiene sus propios pesos y bias, y utiliza la función de activación sigmoide.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        # Inicialización de pesos y bias con valores pequeños aleatorios\n",
    "        # Esto ayuda a romper la simetría y permite que cada neurona aprenda cosas distintas\n",
    "        # self.weights = np.random.randn(input_size)\n",
    "        # self.bias = np.random.randn()\n",
    "        limit = np.sqrt(6 / (input_size + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, input_size)\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        # Convierte la suma ponderada en un valor entre 0 y 1\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        # https://interactivechaos.com/es/manual/tutorial-de-deep-learning/derivada-de-la-funcion-sigmoide\n",
    "        # Derivada de la sigmoide\n",
    "        # x es el valor de la activación sigmoide. Es decir, se ejecutará esto en realidad => sigmoid(x) * (1 - sigmoid(x))\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante de una sola muestra\n",
    "        # Calcula la suma ponderada y aplica la función de activación\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "\n",
    "    def update(self, x, delta):\n",
    "        # Actualiza los pesos y bias usando el gradiente calculado (delta)\n",
    "        # delta ya incluye la derivada de la sigmoide (por la regla de la cadena)\n",
    "        # La actualización sigue la dirección del gradiente descendente\n",
    "        self.weights += self.learning_rate * delta * x\n",
    "        self.bias += self.learning_rate * delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657721fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Red neuronal multicapa compuesta por capas de perceptrones.\n",
    "    Permite definir cualquier cantidad de capas y neuronas por capa.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1):\n",
    "        # layer_sizes: lista con el tamaño de cada capa, EJEMPLO: [784, 100, 10]\n",
    "        self.layers: list[list[Perceptron]] = []\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            capa = [Perceptron(layer_sizes[i-1], learning_rate) for _ in range(layer_sizes[i])]\n",
    "            # EJEMPLO:\n",
    "            # - Para la primera capa oculta generaria 100 neuronas que aceptan vectores inputs de 784 dimensiones (input layer).\n",
    "            # - Para la capa de input no se crea un perceptrón, ya que es la entrada de la red.\n",
    "            # - La capa de salida generaria 10 neuronas que aceptan vectores inputs de 100 dimensiones (capa oculta).\n",
    "            self.layers.append(capa)\n",
    "        \n",
    "        # Mostrar la estructura de la red\n",
    "        # print(\"Estructura de la red:\")\n",
    "        # for i, capa in enumerate(self.layers):\n",
    "        #     print(f\"Capa {i+1}: {len(capa)} neuronas, cada una con {capa[0].weights.shape[0]} pesos\")\n",
    "        # print(\"\\nPesos de la capa intermedia:\")\n",
    "        # for idx, neuron in enumerate(self.layers[0]):\n",
    "        #     print(f\"Neurona {idx}: {neuron.weights}\")\n",
    "        print(\"\\nPesos de la capa de salida:\")\n",
    "        for idx, neuron in enumerate(self.layers[-1]):\n",
    "            print(f\"Neurona {idx}: {neuron.weights}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante para una muestra\n",
    "        activations = [x]   # Inicialmente arranca con el input (x, vector de entrada)\n",
    "        for capa in self.layers:\n",
    "            salida_capa = np.array([neuron.forward(activations[-1]) for neuron in capa])\n",
    "            activations.append(salida_capa)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predicción para una muestra\n",
    "        activations = self.forward(x)\n",
    "        return activations[-1]\n",
    "\n",
    "    def train(self, X, Y, epochs=10):\n",
    "        # Entrenamiento usando backpropagation (solo para fines didácticos, no optimizado)\n",
    "        for epoch in range(epochs):\n",
    "            for xi, yi in zip(X, Y):\n",
    "                # Forward\n",
    "                activations = self.forward(xi)  # activations es una lista de arrays, donde cada array es la salida de cada capa (función de activación sigmoidea aplicada)\n",
    "                # Backward\n",
    "                deltas = [None] * len(self.layers)\n",
    "                # Capa de salida\n",
    "                error = yi - activations[-1]    # Resta de vectores\n",
    "                deltas[-1] = error * np.array([\n",
    "                    neuron.activation_derivative(activations[-1][j])\n",
    "                    for j, neuron in enumerate(self.layers[-1])\n",
    "                ])\n",
    "                # Capas ocultas\n",
    "                for l in reversed(range(len(self.layers)-1)):\n",
    "                    delta_next = deltas[l+1]    # lista de deltas de la capa siguiente (son 10 deltas)\n",
    "                    pesos_next = np.array([neuron.weights for neuron in self.layers[l+1]])  # En la primera iteracion para MNIST, es un array de arrays de pesos de la capa siguiente -> matriz de 10 filas y 100 columnas\n",
    "                    # deprecated: deltas[l] = self.layers[l][0].activation_derivative(activations[l+1]) * np.dot(pesos_next.T, delta_next)\n",
    "                    deltas[l] = np.array([\n",
    "                        neuron.activation_derivative(activations[l+1][j])\n",
    "                        for j, neuron in enumerate(self.layers[l])\n",
    "                    ]) * np.dot(pesos_next.T, delta_next)   # el resultado del producto escalar (np.dot) es un vector de 100 dimensiones (deltas de la capa oculta)\n",
    "\n",
    "                # Actualización de pesos\n",
    "                for l, capa in enumerate(self.layers):\n",
    "                    for j, neuron in enumerate(capa):\n",
    "                        neuron.update(activations[l], deltas[l][j])\n",
    "                        # para calcular el delta de cada neurona, se usa la derivada de la activación de esa neurona (activations[l+1][j])\n",
    "                        # pero para actualizar los pesos, cada neurona necesita todas las salidas de la capa anterior, no solo una (activations[l]) -> entonces, actualiza cada peso de cada feature, sumando al vector de pesos el vector corrección\n",
    "                        \n",
    "            if epoch % max(1, epochs//10) == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c5ec30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos un subconjunto pequeño de MNIST para entrenamiento rápido\n",
    "# Usamos solo las primeras 100 muestras para entrenamiento y 10 para test\n",
    "# X_train = x_train.reshape((x_train.shape[0], -1))[:1000]\n",
    "# y_train = y_train[:1000]\n",
    "# X_test = x_test.reshape((x_test.shape[0], -1))[:100]\n",
    "# y_test = y_test[:100]\n",
    "\n",
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 200 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = x_train.reshape((x_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "# 2. Hacemos un split 80/20 sobre ese subconjunto\n",
    "split = int(0.8 * n_samples)\n",
    "X_train = X[:split]\n",
    "y_train = Y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = Y[split:]\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0a3615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [-0.19805709  0.15424981 -0.24551772  0.10654836 -0.23330917 -0.26145001\n",
      " -0.04960539  0.14852468 -0.18331771  0.28395843  0.19415854  0.15754679\n",
      " -0.27353537 -0.11193941  0.21350944  0.1528984  -0.22681684  0.13035958\n",
      "  0.01056203 -0.18882484  0.23096213 -0.23858041 -0.07044071 -0.00635055\n",
      "  0.06326608 -0.00867777  0.13676615  0.03516835  0.02419805  0.18667652\n",
      " -0.238331    0.10690785  0.15765948 -0.27619888  0.28673489 -0.06488751\n",
      "  0.29886517 -0.04726109 -0.12559562 -0.24616064 -0.07365406 -0.23619327\n",
      "  0.17610547 -0.27126195  0.05108046 -0.10742403 -0.09088203  0.12240938\n",
      " -0.13803952  0.28355517  0.249369    0.06527658 -0.24264378 -0.2095564\n",
      " -0.11570625 -0.12939745  0.23151554 -0.10172382 -0.27923403 -0.23130512\n",
      " -0.06423971  0.28777738  0.18048799  0.05128585]\n",
      "Neurona 1: [-0.21269436 -0.29302231  0.13682931 -0.20559124 -0.2330068   0.24892737\n",
      "  0.01658648 -0.14044959  0.25950926  0.13148782 -0.30061761  0.13170669\n",
      "  0.22689231 -0.12197144 -0.09432168 -0.08422294 -0.28630926 -0.0382417\n",
      " -0.19285909 -0.15447836 -0.26146595 -0.07090591 -0.29314086  0.12064519\n",
      " -0.17575974 -0.26188888  0.18296831  0.21552762 -0.27504522  0.26003484\n",
      " -0.24537114 -0.16010979  0.29498377 -0.20142298 -0.15972679  0.12136305\n",
      "  0.06721708  0.07981651  0.29767691  0.07271217  0.09367499 -0.00064189\n",
      "  0.20399013 -0.25458467 -0.28584268  0.25283387  0.06885517 -0.0242167\n",
      " -0.13488854 -0.13755147 -0.22734294  0.24805729 -0.14821105 -0.02657823\n",
      " -0.15064086 -0.16652137  0.09114749 -0.15933888  0.0585893  -0.11098506\n",
      "  0.04982845 -0.20222896 -0.1068814   0.2236103 ]\n",
      "Neurona 2: [ 0.00187914 -0.13016064  0.14077002  0.14469915  0.21582395  0.01378169\n",
      " -0.27821138  0.30088392 -0.07377584  0.23073858  0.20323031  0.03314552\n",
      " -0.30028754 -0.11564508 -0.03475945 -0.16872665 -0.24695578 -0.03390329\n",
      "  0.24581503  0.19544795  0.02482477  0.18126929  0.24083317  0.28716167\n",
      "  0.14393147  0.11988504  0.17782504 -0.21580001  0.13481889  0.04386787\n",
      " -0.0028658  -0.09411144 -0.11229731  0.03326675 -0.22020428 -0.14364316\n",
      "  0.28542904 -0.08066052 -0.2631036  -0.02704441 -0.12142322 -0.07975672\n",
      "  0.17703596 -0.20116499  0.30288546  0.09133645 -0.29318554 -0.2622799\n",
      " -0.0598606   0.12818515  0.20793002 -0.0179055   0.27831364  0.15268676\n",
      "  0.3001588  -0.28649537 -0.07490412  0.02836802 -0.06540329 -0.30212188\n",
      " -0.13505154  0.09016311 -0.02740004  0.29966323]\n",
      "Neurona 3: [ 0.22883534 -0.10232569 -0.24862847  0.17767916 -0.23343117 -0.26472329\n",
      "  0.2336192  -0.00467736  0.01159886  0.17486065  0.28833084  0.15319807\n",
      " -0.28645606 -0.10085073  0.04187325 -0.25778793 -0.04843205 -0.22502486\n",
      " -0.12724444 -0.20469512 -0.20242937 -0.1240486  -0.17370334 -0.18024676\n",
      "  0.20511766  0.04206225 -0.05516587  0.08992932 -0.11838752 -0.09933205\n",
      "  0.0755589  -0.0529222  -0.03570708  0.03430977  0.16737629  0.12544215\n",
      " -0.15248677 -0.19729143 -0.2168138   0.25711628 -0.11171291  0.0561712\n",
      " -0.14639087 -0.19045989  0.2753669   0.07717566  0.29480293  0.18464579\n",
      " -0.12312691  0.09075466  0.13220862 -0.07884364 -0.27432344 -0.13025451\n",
      " -0.04019696  0.11204673 -0.04197406 -0.12716883  0.11497689 -0.07137278\n",
      "  0.13905666 -0.17152229 -0.14945389 -0.13425583]\n",
      "Neurona 4: [-0.04723801 -0.21486095 -0.17594584 -0.09119487 -0.00464254  0.02052822\n",
      " -0.14241684  0.22200431 -0.17165567  0.12491752 -0.23131681 -0.13535554\n",
      " -0.03779908  0.0756187   0.20466464 -0.19052677 -0.30046016  0.19878241\n",
      "  0.05793864  0.0076567  -0.17960646 -0.15160963 -0.20159279  0.3037789\n",
      "  0.16805473 -0.23561476 -0.10984796 -0.07485852 -0.27120791  0.22772605\n",
      " -0.1060935   0.21691499 -0.06841215 -0.13289523 -0.26187658  0.16710978\n",
      "  0.28718909 -0.05213567 -0.11058759 -0.08282217 -0.2822715  -0.19111663\n",
      "  0.04458794 -0.03007273  0.10680433 -0.25830954 -0.06571037  0.16911496\n",
      "  0.09956328 -0.01533082  0.0199074  -0.17475129  0.28328754  0.03751084\n",
      "  0.07876218  0.18659271  0.07090272  0.18703983 -0.00047595 -0.17011278\n",
      "  0.09649864  0.25547544 -0.14459822 -0.1622117 ]\n",
      "Neurona 5: [-0.25308828 -0.20990554  0.23557405  0.07337099  0.12677457  0.03193101\n",
      "  0.14498906  0.22593353  0.05073082 -0.112573   -0.29890836  0.16904017\n",
      " -0.24910339  0.27990979 -0.01027425 -0.22409775  0.0219475  -0.25827306\n",
      "  0.13987791  0.25947842 -0.02511851  0.22814855 -0.15759668  0.07491166\n",
      "  0.11393179 -0.14753905 -0.07061616 -0.23307323 -0.01508285 -0.28045927\n",
      " -0.28999171 -0.014033    0.11918194 -0.09237821  0.17924874  0.22104406\n",
      " -0.24354418  0.28260282 -0.01624858 -0.17662524  0.10798951  0.14360093\n",
      "  0.05464976 -0.1820576   0.15455962  0.0693553   0.26936663  0.14171618\n",
      "  0.00446407  0.10494113 -0.12625884  0.10637651  0.13930362  0.29596566\n",
      "  0.18823232  0.28200264  0.09346723  0.18187615 -0.06086397  0.23335846\n",
      " -0.17067555  0.12671828 -0.27638169  0.1085508 ]\n",
      "Neurona 6: [ 0.08309211  0.00115264  0.04329224  0.27102905 -0.16073688  0.25819878\n",
      " -0.25006391 -0.08398356  0.03305395  0.20092027  0.19389573  0.1466769\n",
      " -0.27938886 -0.14092081  0.09480407 -0.14607086 -0.19575581  0.15834319\n",
      " -0.19167647  0.13182869 -0.1493663  -0.135055   -0.12071506  0.23190016\n",
      "  0.00292419 -0.26084175  0.0298522  -0.14012854  0.17265142 -0.25876938\n",
      "  0.01857144 -0.2213887   0.225041    0.21632846  0.17254812  0.09793933\n",
      "  0.21890645 -0.29885751  0.03253668 -0.00688534 -0.21398545  0.14371204\n",
      "  0.16858105  0.148224    0.25604841 -0.2185533   0.30283537  0.02478057\n",
      " -0.06337586 -0.09011898 -0.20764516  0.20197003 -0.23866882  0.20012237\n",
      " -0.2190009   0.17232557  0.10942541 -0.07858121  0.27850074 -0.23468362\n",
      " -0.10990453  0.0439228  -0.26255523  0.01674482]\n",
      "Neurona 7: [-0.11179682  0.2697201   0.14326702  0.2776187   0.17994289  0.12520059\n",
      "  0.04425916  0.04698497  0.18220257 -0.11802775 -0.0546703  -0.18030558\n",
      "  0.07965165 -0.24213135  0.02928559  0.03214773 -0.24547761 -0.05180168\n",
      "  0.26694499 -0.09773076  0.08321285 -0.08388906  0.01393721 -0.27952996\n",
      "  0.03517195 -0.03509804  0.03008631  0.03686301  0.17520216 -0.1408245\n",
      " -0.21136509  0.1686164   0.26158078 -0.00565745  0.12178693  0.08570317\n",
      " -0.29431997  0.1278998  -0.18932987  0.02483735  0.06152989  0.20828879\n",
      "  0.28339952 -0.22726454 -0.24240292  0.16550162 -0.19072524  0.05555692\n",
      "  0.13264887  0.00224099  0.14284459 -0.24161393  0.15736525 -0.27115644\n",
      " -0.15885616  0.13939411  0.21961178 -0.13629927 -0.20988242  0.09268145\n",
      " -0.08578512 -0.01587299  0.08843114  0.08016041]\n",
      "Neurona 8: [-5.43649685e-02 -2.44060694e-01 -2.84319368e-01 -2.23386615e-01\n",
      "  2.13486255e-01 -1.49514046e-01  1.07713761e-01  2.21756513e-02\n",
      " -2.21169116e-01  2.82280253e-01 -6.68286374e-02  1.99205813e-01\n",
      "  2.14758020e-01 -2.99023419e-02  2.45163484e-01 -2.76752433e-01\n",
      "  2.41514605e-04 -1.75948188e-01  2.37721798e-01 -2.29819877e-02\n",
      " -1.92379616e-01  2.99793180e-01  2.28328121e-01 -2.46537905e-01\n",
      " -5.98900611e-02  1.88802110e-01 -9.03389516e-02  1.05224878e-01\n",
      " -8.15619888e-02  7.18070134e-02  4.84043161e-02 -1.41335484e-01\n",
      "  2.29311508e-01  2.75391166e-01  7.29024039e-02 -2.33262042e-01\n",
      "  1.32283590e-01  2.56018560e-02  2.08667951e-02  2.24810312e-01\n",
      "  2.99741443e-01  6.95906831e-02  2.57078008e-01  2.71293598e-01\n",
      " -1.66727292e-01  8.75933148e-02  2.00860435e-01 -1.12399479e-01\n",
      "  1.17998657e-01 -2.55889052e-01  2.92334856e-02  1.01518933e-01\n",
      "  2.59311550e-01  4.58951243e-03  2.31724343e-01  1.76992686e-01\n",
      "  1.38066593e-01 -3.98216847e-02 -1.54710148e-01 -1.94008769e-01\n",
      "  2.33478276e-01 -1.86189311e-01  3.00042864e-01 -1.54241132e-01]\n",
      "Neurona 9: [ 0.13871932  0.01849071  0.00428578  0.26852048  0.1930833   0.24851438\n",
      "  0.07321756  0.05432932  0.1890763   0.00555163 -0.10440115 -0.2334798\n",
      " -0.0817105   0.26436099 -0.03475131 -0.07691217  0.20093107 -0.13262739\n",
      " -0.2223574   0.21441361 -0.26269357 -0.14438311 -0.16583816 -0.07794222\n",
      "  0.27607195 -0.01213093 -0.12487531 -0.13004233  0.1702066  -0.1127956\n",
      "  0.1922053  -0.04374095  0.29709012  0.09352322  0.00258744  0.05178378\n",
      "  0.17667276 -0.28703828  0.08789169  0.29392756 -0.02440123 -0.0099719\n",
      " -0.25212111 -0.12387539 -0.0344135   0.26104196 -0.2966686  -0.14028183\n",
      " -0.20691166 -0.00400795  0.13970237  0.16755393  0.2022884   0.17076486\n",
      "  0.19508281  0.13489355  0.01493443 -0.05658264  0.01886562 -0.13780832\n",
      " -0.20309388  0.06461449  0.22075754  0.07215808]\n",
      "Epoch 1/100 completada\n",
      "Epoch 11/100 completada\n",
      "Epoch 21/100 completada\n",
      "Epoch 31/100 completada\n",
      "Epoch 41/100 completada\n",
      "Epoch 51/100 completada\n",
      "Epoch 61/100 completada\n",
      "Epoch 71/100 completada\n",
      "Epoch 81/100 completada\n",
      "Epoch 91/100 completada\n"
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura: 1 capa oculta de 32 neuronas\n",
    "input_size = X_train.shape[1]  # 784 para MNIST\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32  # Capa oculta adicional\n",
    "output_size = 10  # 10 clases\n",
    "\n",
    "# Creamos la red\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red (puede tardar unos minutos)\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3d8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el subconjunto de test: 1855/2000\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [ 0.1477677  -0.41738491 -0.08644095 -0.11484555 -0.2707198  -0.19403095\n",
      "  0.35054335  0.22706385 -0.32193181 -0.00324284  0.42421315  0.42534733\n",
      "  0.32508967 -0.30588869  0.35708655  0.37950966 -0.38865308  0.22606246\n",
      " -0.26550155  0.33108974  0.3384983   0.29505282  0.21240173 -0.26799807\n",
      " -0.11307818  0.42165967  0.27484448 -0.1775188  -0.06866606 -0.01617937\n",
      " -0.3635152   0.13564947]\n",
      "Neurona 1: [-0.18194418 -0.1253528  -0.40260105  0.32044135 -0.35735428 -0.02679601\n",
      "  0.00326597  0.2129688   0.03247055 -0.22898612  0.31743093  0.33709559\n",
      "  0.00900901 -0.39691356 -0.29829105  0.18550242  0.34028104  0.33018925\n",
      " -0.11611717  0.01621315 -0.18162352 -0.32001144  0.03703345  0.09502639\n",
      "  0.38825189 -0.38272885 -0.2738584  -0.34856596 -0.40607581  0.3883104\n",
      "  0.3052759   0.30937379]\n",
      "Neurona 2: [-0.32583216 -0.36905487  0.29055989 -0.00968656  0.21323516  0.1680995\n",
      "  0.18045828 -0.01450863  0.36893808 -0.40191485  0.15677245 -0.39201369\n",
      "  0.0178203  -0.04664014  0.34618929  0.25763183 -0.22614211 -0.08269955\n",
      "  0.11854516 -0.14290453 -0.23681153 -0.04762594 -0.10159835 -0.23421209\n",
      " -0.42244128 -0.01435714 -0.19265999 -0.18048474 -0.31649444  0.2213923\n",
      "  0.2463371  -0.01005082]\n",
      "Neurona 3: [ 0.4003017   0.14605393 -0.11112788 -0.24832577  0.34182493 -0.38021794\n",
      "  0.36211594 -0.33767806  0.40691296 -0.40313646 -0.3254123  -0.2310727\n",
      " -0.33997975  0.40799039 -0.2095409   0.129922    0.1006314   0.36407759\n",
      " -0.21107249  0.22197306 -0.36159092 -0.01801864 -0.33930478 -0.23290451\n",
      "  0.00090052  0.32827403 -0.21914782 -0.21311462 -0.4127471   0.12153078\n",
      " -0.28870667  0.3480244 ]\n",
      "Neurona 4: [ 0.29346682 -0.34849168  0.26858904 -0.11772532  0.1743276  -0.06247599\n",
      " -0.4006896   0.34921727  0.30528836 -0.03557723  0.20364227 -0.16817143\n",
      " -0.25917509 -0.07603538  0.30707242  0.0511297   0.11379694  0.21986085\n",
      "  0.24120204 -0.3001363   0.22298996 -0.06621713  0.41572611 -0.40845845\n",
      "  0.06693588 -0.06415109 -0.10051435  0.17415981  0.23243888 -0.36711537\n",
      "  0.16712435 -0.23562779]\n",
      "Neurona 5: [-0.21696903  0.40645922 -0.27798965  0.23490233 -0.0056537  -0.24651222\n",
      " -0.38634627  0.04585843 -0.18508259 -0.08487356 -0.00900208 -0.12654406\n",
      "  0.22176422 -0.25046326 -0.30102763 -0.38160562 -0.27176187  0.15863204\n",
      "  0.11297445 -0.0577168   0.39655374  0.1119176  -0.33448806 -0.15223914\n",
      " -0.366352    0.36696855  0.19079678  0.19937035 -0.09485589 -0.28953454\n",
      "  0.10105247 -0.12507497]\n",
      "Neurona 6: [ 0.38444546 -0.37028223  0.27580306 -0.38712865 -0.27518318 -0.29281971\n",
      " -0.38891706 -0.2405312  -0.21954873  0.01296963  0.3017904   0.11567448\n",
      " -0.06118473  0.16236303  0.40227255 -0.07509476  0.08353281  0.09494017\n",
      "  0.35812918  0.00369025  0.33158241 -0.18389407  0.3128315   0.01976359\n",
      " -0.10160426 -0.1029511   0.14634908 -0.02719063  0.00581238  0.2879292\n",
      "  0.17584404  0.10770797]\n",
      "Neurona 7: [-0.34898872 -0.29956116 -0.23235649  0.07801239  0.13566876  0.37991967\n",
      " -0.3134419  -0.38316447  0.18082638  0.10540557 -0.4234673  -0.37976378\n",
      "  0.37088118 -0.33713632  0.16128446  0.01573336 -0.35581203  0.08943236\n",
      "  0.31917783 -0.25491406 -0.26076622  0.30684267  0.11557036 -0.3582773\n",
      "  0.36212066 -0.14439187 -0.42121605  0.23014703 -0.20726286  0.13412211\n",
      " -0.06494728  0.26307675]\n",
      "Neurona 8: [-0.41422753 -0.17957961  0.19046246 -0.03136898  0.17117781 -0.01711336\n",
      "  0.11051256 -0.3437284   0.24913937  0.16785154 -0.1064571  -0.10519133\n",
      " -0.11918265  0.21953288  0.3469934   0.00536209 -0.33455346  0.40531052\n",
      "  0.30767177  0.39883222  0.2866534  -0.19727931 -0.26742819 -0.35860485\n",
      "  0.03794816 -0.42583912 -0.30982721  0.30964992  0.24367565  0.16785095\n",
      "  0.15544658  0.24854975]\n",
      "Neurona 9: [ 0.03844826  0.06895538 -0.16703827 -0.20024051 -0.13686425 -0.41402511\n",
      " -0.16162889 -0.06191777 -0.04723862 -0.10815641  0.01815477  0.25770524\n",
      "  0.00708289 -0.11774733 -0.01692108  0.39496387  0.04399948 -0.32385259\n",
      " -0.37260487 -0.1026476  -0.05855291 -0.39878578 -0.36306557 -0.30635966\n",
      " -0.15158856  0.26469733  0.14826202  0.02040478 -0.23386741 -0.36685945\n",
      " -0.36732924 -0.12970747]\n",
      "Epoch 1/100 completada\n",
      "Epoch 11/100 completada\n",
      "Epoch 21/100 completada\n",
      "Epoch 31/100 completada\n",
      "Epoch 41/100 completada\n",
      "Epoch 51/100 completada\n",
      "Epoch 61/100 completada\n",
      "Epoch 71/100 completada\n",
      "Epoch 81/100 completada\n",
      "Epoch 91/100 completada\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m mlp\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# (Opcional) Evaluar en el conjunto de prueba\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(X_test, y_test)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "X_train, y_train, X_test, y_test = load_cifar10(f\"data/CIFAR-10\")\n",
    "\n",
    "# Aplanamos las imágenes (de 32x32x3 a 3072)\n",
    "X_train = X_train.reshape(-1, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 200 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = X_train.reshape((X_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "X_train = X\n",
    "y_train = Y\n",
    "\n",
    "# Definimos la arquitectura: dos capas ocultas\n",
    "input_size = X_train.shape[1]  # 3072 para CIFAR-10\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 10  # 10 clases para CIFAR-10\n",
    "\n",
    "# Creamos la red con dos capas ocultas\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el subconjunto de test: 4135/10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [ 0.16149942  0.23627933 -0.28908691  0.39345024 -0.08104062 -0.23324945\n",
      "  0.21863577 -0.28295031  0.1518131  -0.16635039 -0.26036811 -0.12841487\n",
      " -0.2764827  -0.39290738 -0.04446165 -0.34524737  0.30438739  0.29481635\n",
      "  0.15002793  0.42369625  0.40029829 -0.05680715  0.07344367  0.02879389\n",
      " -0.35461295 -0.12476387 -0.05514851  0.29654483  0.27802241 -0.13946806\n",
      " -0.22407028  0.16049434]\n",
      "Neurona 1: [-0.0872708   0.03684712 -0.30554699 -0.39983352 -0.3891758  -0.1882959\n",
      " -0.28902896  0.11698839 -0.25335911  0.33672191  0.13416491  0.34177103\n",
      "  0.40010698  0.11538929 -0.4118487  -0.08633582  0.38075734 -0.34908716\n",
      "  0.38490133  0.25667329 -0.08898306 -0.27569107 -0.11044568  0.27236445\n",
      "  0.16801309  0.20289735 -0.39247359 -0.09478912 -0.13081586 -0.10912207\n",
      "  0.11434673  0.16672428]\n",
      "Neurona 2: [-0.38758939  0.19497116 -0.15027481 -0.00564935 -0.3759918   0.23775133\n",
      "  0.21311233 -0.10136635 -0.21455555  0.02503643 -0.04981712  0.21271956\n",
      " -0.08552419 -0.10182447 -0.30269942 -0.29543758  0.08488379  0.16148413\n",
      " -0.14901046  0.13943315 -0.27854034  0.31011032  0.10808611  0.33172045\n",
      " -0.03216529 -0.17780562  0.25013965 -0.23332769 -0.09970911 -0.27294663\n",
      "  0.22787987  0.01249133]\n",
      "Neurona 3: [-0.12253215  0.14758078  0.13523443  0.22377427  0.07245415  0.19546487\n",
      "  0.3811419   0.2193844   0.29391876 -0.34223118 -0.23259313  0.2499628\n",
      " -0.32632315  0.23509965  0.07145295  0.06497855  0.22462583 -0.14139144\n",
      "  0.01519772  0.06899833  0.29631053  0.09736207 -0.09957339 -0.2501773\n",
      " -0.1851507   0.08760898 -0.36715277 -0.11567671 -0.3252279   0.40368034\n",
      "  0.13509068  0.32996006]\n",
      "Neurona 4: [ 0.41281512 -0.34258864  0.30861785 -0.07119236  0.05655011 -0.07504624\n",
      "  0.32047962 -0.33728149 -0.35642868  0.03396567 -0.05512883 -0.21586375\n",
      "  0.04200523 -0.41845765  0.08359332  0.26624441 -0.25677367 -0.09955138\n",
      "  0.24998689 -0.30413634 -0.35795733  0.28314947  0.02993463 -0.39317111\n",
      " -0.40126269  0.42399707 -0.24613802  0.07651634  0.18839131 -0.0866498\n",
      " -0.37050109 -0.00167082]\n",
      "Neurona 5: [ 0.08423715 -0.00234732  0.08303357 -0.08979693  0.26099359  0.12109273\n",
      " -0.028218    0.15265807  0.20601661  0.42524312 -0.20424253  0.29252221\n",
      " -0.25160794 -0.10620879  0.11244411  0.22868239  0.32718189 -0.35948379\n",
      "  0.17508839 -0.35988689  0.28048196 -0.18808482 -0.13261604 -0.16323436\n",
      " -0.24358132  0.11300064 -0.00792408 -0.32014588 -0.3220051  -0.0223054\n",
      "  0.23641113 -0.08185693]\n",
      "Neurona 6: [-0.23549206  0.18842981 -0.28990195 -0.38518981  0.34925119  0.37526973\n",
      " -0.10911776  0.12673634 -0.38062437  0.23917723 -0.33070254  0.34925224\n",
      " -0.09457569  0.16554089 -0.42632273  0.20723571 -0.06465043  0.01568371\n",
      "  0.01766345 -0.17463636 -0.41156238  0.14371662 -0.13491953  0.01782264\n",
      " -0.28525548  0.42531927 -0.05603128 -0.25282117  0.05193004  0.07040249\n",
      " -0.32469897 -0.09154813]\n",
      "Neurona 7: [-0.08380679  0.35596545  0.36347334  0.42462399  0.11050316 -0.1273428\n",
      " -0.3318779   0.16348491 -0.11752713  0.31762947  0.40840213 -0.34714602\n",
      " -0.41183589  0.33620996  0.04511321 -0.42442533  0.37647454  0.22149842\n",
      " -0.33428121 -0.27163286  0.21577078  0.18546796 -0.1479032  -0.01653277\n",
      " -0.07183721  0.27571344  0.35827921 -0.25319733 -0.04436694 -0.02585578\n",
      "  0.07862302 -0.34809102]\n",
      "Neurona 8: [-0.33054308 -0.39495243  0.24385088  0.13575258  0.14495817  0.25407343\n",
      "  0.20719675 -0.24359282 -0.32392463 -0.10018147  0.36022962 -0.25162802\n",
      "  0.28995446  0.39693934 -0.22860992  0.07317375 -0.07116408 -0.17224362\n",
      "  0.35093883 -0.07430175  0.32776473 -0.15889474 -0.05800578 -0.07340723\n",
      " -0.07107307  0.05814439 -0.12553788 -0.12716154  0.10066279 -0.38502539\n",
      " -0.30179974  0.31677469]\n",
      "Neurona 9: [-0.21983946  0.25227566  0.3630711   0.08946049  0.10661245  0.22650431\n",
      "  0.29440065  0.39496072  0.19345193 -0.11307751 -0.14580511  0.27217807\n",
      "  0.22364081  0.06842396 -0.29282807 -0.23722297 -0.2684741  -0.12685373\n",
      "  0.3637253  -0.16549676  0.26843627  0.11824576 -0.37906219 -0.25729507\n",
      "  0.24588492  0.32842354 -0.05032981  0.20681804  0.32230336 -0.11225822\n",
      " -0.3158074  -0.23853766]\n",
      "Neurona 10: [-0.3153758   0.33105897 -0.03606532 -0.37446648  0.32841867 -0.37982359\n",
      " -0.07503984 -0.10074895 -0.26208187 -0.31791792  0.17944517  0.31649668\n",
      " -0.20843288  0.18753979 -0.24788479 -0.16784488  0.37345245 -0.19751567\n",
      " -0.24671621 -0.36821409  0.13491851 -0.36210284  0.04369539  0.15623178\n",
      "  0.20985261 -0.28563307 -0.2529076   0.31424686 -0.12472364 -0.16295074\n",
      " -0.4002096  -0.23176904]\n",
      "Neurona 11: [ 0.42013913  0.31217642 -0.21547842 -0.20372373  0.34146519  0.02509388\n",
      "  0.13213764  0.03898934  0.35486512  0.22585737  0.34218111 -0.24999447\n",
      "  0.06584539  0.08430242 -0.15010796 -0.26544173 -0.26354024  0.14184786\n",
      " -0.11808818 -0.1196473  -0.23951833 -0.24502985 -0.31769829  0.14682725\n",
      "  0.01898549  0.20516031 -0.13510516 -0.0416656   0.41811646 -0.09904056\n",
      " -0.02081563 -0.25366848]\n",
      "Neurona 12: [-0.3339423   0.1253592  -0.24731864 -0.17686217 -0.39388113 -0.12792393\n",
      " -0.39140931 -0.37506556 -0.24684519 -0.09106522 -0.0511516   0.01773466\n",
      "  0.37356255  0.30633201 -0.25498906 -0.08739708 -0.26346795  0.13052105\n",
      "  0.26951554 -0.19310513  0.26245599  0.26781612  0.05991766 -0.42637349\n",
      " -0.21052514  0.31981867  0.34643778  0.37250815 -0.35069638  0.37147172\n",
      "  0.24614443 -0.03896285]\n",
      "Neurona 13: [ 0.20771655 -0.17732677  0.34413035 -0.27039718  0.14287948  0.01192537\n",
      "  0.02791203 -0.37130223  0.0846274   0.07018306 -0.10527209 -0.19748384\n",
      "  0.26641245 -0.33306317 -0.02730443 -0.42182302  0.26816221  0.27931533\n",
      "  0.16226141  0.22466026  0.33076612 -0.38365235 -0.24403293  0.35069499\n",
      " -0.07608051  0.05955713  0.23406743  0.04999234  0.16899141 -0.08532702\n",
      "  0.02427918  0.35791583]\n",
      "Neurona 14: [-0.212919   -0.05244554  0.35630574  0.0958042   0.05679439  0.17419848\n",
      " -0.25235724  0.29228667 -0.12732618 -0.14425661 -0.41562376 -0.10204769\n",
      "  0.21325012 -0.23400417  0.19806523 -0.09233511 -0.32255877  0.3997638\n",
      "  0.1274915  -0.06176439  0.03030838  0.14370517 -0.22884555 -0.33141579\n",
      "  0.30110847  0.20806298 -0.21099258 -0.0426908   0.00363046  0.11306458\n",
      "  0.40405061  0.08663874]\n",
      "Neurona 15: [-0.29324955  0.42303584 -0.12936119  0.32791969 -0.10605897  0.16821443\n",
      "  0.19634177  0.26924792  0.39335503 -0.21300358 -0.19041376  0.09664851\n",
      "  0.29444415  0.09408293 -0.21160513  0.10848216 -0.12630322  0.3324195\n",
      " -0.00995856 -0.38589747 -0.38753507 -0.41485927 -0.20146125  0.15177875\n",
      " -0.38826349 -0.4081956  -0.02148975 -0.02325736 -0.125233    0.04835764\n",
      "  0.01468335 -0.07429221]\n",
      "Neurona 16: [ 0.11374311 -0.31436835  0.05570348  0.04093862  0.20610814 -0.37806915\n",
      " -0.40598408  0.32856972 -0.1005014  -0.0786508   0.11021668 -0.04283571\n",
      "  0.21471656 -0.20698142  0.14520852  0.37352858 -0.34110746  0.29267433\n",
      " -0.02682598  0.22807065  0.22317478  0.21250114 -0.18013624  0.09141155\n",
      "  0.00188194 -0.07422337  0.14561109 -0.16851284 -0.0426916  -0.25653745\n",
      "  0.41949213 -0.1733788 ]\n",
      "Neurona 17: [-0.15790378  0.1873729   0.32477257 -0.253566   -0.09643577  0.13099807\n",
      "  0.16166122 -0.06318214  0.35850828  0.10575696  0.01069811  0.25947307\n",
      "  0.01839066  0.17463729 -0.35263967 -0.25613722  0.13154599  0.04523065\n",
      "  0.29155666  0.07829902  0.27042471 -0.31228952 -0.02042396  0.14595136\n",
      "  0.08796308  0.3625798   0.06956114 -0.33669743 -0.10167724  0.35582719\n",
      "  0.37913434  0.05376982]\n",
      "Neurona 18: [-0.31510375  0.01834518  0.10185245 -0.0643463   0.16231326  0.12486048\n",
      " -0.41107229 -0.16399667 -0.38792081  0.24184849  0.03720046 -0.30975912\n",
      "  0.29830246  0.42015324 -0.09104904 -0.03015492  0.03913742 -0.34320886\n",
      " -0.02790626  0.10067149 -0.35367034 -0.40851605  0.41829222  0.25500586\n",
      "  0.26978002  0.29826464  0.36022721 -0.37061906  0.03000936 -0.35108861\n",
      "  0.18921443  0.20923091]\n",
      "Neurona 19: [ 0.25663699 -0.23439838 -0.02102599  0.33012043 -0.40608593 -0.12026546\n",
      "  0.02642985 -0.32432006 -0.05692074  0.33231653  0.24220112  0.35706943\n",
      "  0.23397881 -0.40188684  0.27100557  0.26449055 -0.00501393 -0.04361641\n",
      " -0.04462065 -0.1578704   0.20835411 -0.22464263 -0.36940598  0.32409823\n",
      "  0.01470549 -0.05316442 -0.18342645 -0.28968412 -0.02055571 -0.15439892\n",
      " -0.06098868 -0.41309518]\n",
      "Neurona 20: [-0.16065177 -0.24714003  0.11722378 -0.30164845  0.12879298 -0.33607407\n",
      "  0.20274619  0.13792032  0.19883333 -0.06206782  0.08109298 -0.41284458\n",
      " -0.0480006   0.14208741  0.42138619 -0.24288172 -0.26200517 -0.3943166\n",
      " -0.1905042  -0.11977691 -0.28433699  0.33732036  0.22112477  0.10768417\n",
      " -0.30791543  0.05384586 -0.41379076  0.39233233 -0.27475814  0.09391363\n",
      "  0.09150332 -0.05197449]\n",
      "Neurona 21: [-0.01840177 -0.01036461 -0.03319713  0.13518238  0.18066196 -0.38196439\n",
      " -0.23882495  0.17889732 -0.22311751  0.31080343 -0.35339847 -0.318734\n",
      "  0.41904031 -0.08691342 -0.40263347 -0.03978252 -0.40974169 -0.23194452\n",
      "  0.20745264 -0.15534374  0.20917334  0.21593957  0.37676028 -0.01036914\n",
      "  0.03259381 -0.34888101  0.37055471  0.42298482 -0.24956    -0.27772897\n",
      "  0.26497178 -0.09372659]\n",
      "Neurona 22: [-0.18535303  0.07231323  0.40051797 -0.02050398 -0.41422936  0.14103689\n",
      " -0.37150837  0.21850274 -0.04758703 -0.19813228 -0.04699055 -0.06305393\n",
      " -0.24374016 -0.12222329  0.18439382 -0.30083887 -0.00332872 -0.0315065\n",
      "  0.05518901  0.41219033  0.35780418 -0.06555465  0.30280065 -0.40818636\n",
      "  0.04718758  0.00774856 -0.14536235 -0.37355439 -0.0578027  -0.14162758\n",
      " -0.34730048 -0.35068809]\n",
      "Neurona 23: [-0.31232401 -0.10406043 -0.05849139 -0.4022313   0.02388262  0.08696032\n",
      " -0.12211216 -0.08976306 -0.00865878 -0.39572573 -0.15603209  0.41820871\n",
      "  0.04314398 -0.15237558  0.1672622  -0.08944998  0.05122097  0.15308201\n",
      "  0.22979892 -0.17411591 -0.01255961  0.21864743 -0.39673947  0.14091071\n",
      "  0.39176316 -0.04197024  0.05952832 -0.21763526 -0.28779888  0.41555635\n",
      " -0.33347909  0.01451488]\n",
      "Neurona 24: [ 0.09827148  0.23559786 -0.12847037  0.25870819 -0.12076646 -0.036296\n",
      "  0.05633581  0.15432319 -0.00102394 -0.34182693 -0.38791616  0.2549792\n",
      "  0.27214771  0.31611323  0.2907906  -0.38242513  0.30126592  0.31680353\n",
      " -0.41291858 -0.0900024  -0.29412969 -0.07725996  0.00194227 -0.17183613\n",
      "  0.04515613  0.12172531  0.37319174 -0.03576256 -0.01229906  0.01991067\n",
      " -0.37582845  0.32076245]\n",
      "Neurona 25: [-0.14265584  0.27857658  0.1612484  -0.22743136  0.0005503   0.36560289\n",
      " -0.04830889  0.19640348  0.16537068 -0.15282013  0.35485408 -0.15589183\n",
      "  0.33890375 -0.23440427 -0.04839187  0.1339119   0.13620271 -0.33339343\n",
      " -0.36148528  0.40272885  0.1013027   0.0870513  -0.29213557 -0.1883941\n",
      "  0.08292766 -0.06121341  0.0597046  -0.17748258 -0.35790503  0.13554047\n",
      " -0.04741611  0.23208223]\n",
      "Neurona 26: [ 0.36689982 -0.06867119 -0.12616231 -0.02075738  0.15485354  0.05106845\n",
      "  0.22092871 -0.0268905   0.37865515  0.04850969  0.02132475  0.34517279\n",
      " -0.13692993 -0.03483382 -0.16448244  0.09273762 -0.40333346 -0.0910623\n",
      "  0.40333414 -0.42241905 -0.11346418  0.11273398  0.1410115  -0.30205575\n",
      " -0.32029271 -0.03828515 -0.06875728  0.10486998  0.37340694 -0.23751213\n",
      "  0.39712495  0.24581891]\n",
      "Neurona 27: [-3.06314776e-01 -1.58076286e-01  2.23651223e-01  1.21390797e-01\n",
      "  6.43976781e-02 -3.52944770e-04  1.72500149e-01  3.10385214e-01\n",
      "  2.97298410e-01 -7.60936805e-02  3.08018140e-01  4.14994264e-01\n",
      "  1.31397463e-01  3.43261343e-01 -7.89617094e-02  3.74159662e-01\n",
      "  1.17783691e-01 -1.39903974e-01  3.02575751e-02  7.95718759e-02\n",
      "  7.44906759e-02  8.93383569e-02 -1.38525744e-02  2.03092158e-02\n",
      "  2.47502445e-01  4.15728058e-01 -3.91904289e-01 -1.31019424e-01\n",
      "  1.45428466e-01  1.21361026e-03 -3.79491905e-01 -9.59401551e-03]\n",
      "Neurona 28: [-0.04203201  0.00654509 -0.39833965 -0.41779453 -0.39292259  0.286396\n",
      " -0.24235996  0.2300549  -0.18719919 -0.1073009  -0.29485183 -0.15493866\n",
      "  0.35117994 -0.065313    0.36788898  0.29964399 -0.34037077  0.16807522\n",
      "  0.40287859  0.12060656 -0.18849572  0.13371353 -0.2841804  -0.29845069\n",
      "  0.08344302 -0.39055298 -0.15700856  0.16318022  0.42595225 -0.12358414\n",
      "  0.1731843  -0.12171088]\n",
      "Neurona 29: [ 0.19420831 -0.10833393  0.23812345  0.15053486 -0.37311046  0.39024644\n",
      "  0.33671642  0.17250969 -0.30367726  0.05679555 -0.35582773  0.24147391\n",
      "  0.16535618  0.35122282 -0.23594969 -0.13081583 -0.26340213  0.31310029\n",
      "  0.39168145 -0.02211297  0.15484826  0.0452833   0.34096759  0.20932664\n",
      "  0.13843016 -0.17086017  0.39138023 -0.04919549 -0.32899319 -0.30258727\n",
      " -0.36124447  0.26421974]\n",
      "Neurona 30: [ 0.12724884  0.31210173  0.34128078 -0.21364555  0.1961153   0.416779\n",
      " -0.13535006  0.14790819  0.2082574  -0.19337527  0.20226482  0.28683265\n",
      " -0.33863494 -0.35707085  0.10534054  0.32102406  0.33742565 -0.13685627\n",
      "  0.37772771  0.11910706  0.16909221  0.02309448 -0.1815652  -0.13994225\n",
      "  0.3295665   0.10882804  0.18973826  0.00611803 -0.42294128  0.10853449\n",
      "  0.39286457  0.2100467 ]\n",
      "Neurona 31: [ 0.28672868  0.28140845  0.14079426  0.27347352  0.20060671 -0.14602958\n",
      " -0.417149   -0.04521006  0.35509842 -0.03147212 -0.01747454 -0.0728309\n",
      "  0.15485959 -0.02384197  0.10406336  0.38383925  0.24053259 -0.11892711\n",
      "  0.28573893 -0.27475433  0.28699821  0.03437473 -0.29102646  0.34809617\n",
      " -0.37139242  0.40501215  0.16638793 -0.01509442  0.3303723   0.18656172\n",
      "  0.00782893  0.31220432]\n",
      "Neurona 32: [-0.2955915  -0.1924028   0.40466572  0.3327947  -0.08044149 -0.30794963\n",
      " -0.23328688 -0.25230899 -0.36859711  0.34851095 -0.42265997 -0.18578908\n",
      "  0.14083458  0.3329841  -0.38326517  0.05312908  0.41103404 -0.42501172\n",
      " -0.04720876 -0.33112486  0.1042103  -0.3652277  -0.07600419  0.21468944\n",
      " -0.41451009  0.29234073  0.19754374  0.39507995 -0.16336039  0.0761675\n",
      " -0.32193243  0.14390591]\n",
      "Neurona 33: [-0.27925447 -0.11229766 -0.4025903   0.40674631  0.32547108 -0.21666382\n",
      "  0.13048842 -0.1358554  -0.17215337 -0.32798665  0.17540585  0.34599892\n",
      "  0.10187306 -0.07496892 -0.17991169 -0.26642565  0.12613905  0.1819392\n",
      " -0.29478038  0.03769062  0.3293169  -0.19539536  0.27213676 -0.04287892\n",
      "  0.22026437 -0.39193065  0.14648622 -0.13133425 -0.07071406 -0.23513909\n",
      " -0.25953932 -0.3615577 ]\n",
      "Neurona 34: [-0.10593625  0.16367069 -0.3637875   0.02295126 -0.26073025  0.14011432\n",
      " -0.18029919 -0.18436036  0.28658007  0.04811446 -0.4213292  -0.15691437\n",
      "  0.29670489  0.1499993   0.4034288   0.10447622  0.38473466 -0.00069836\n",
      " -0.31423412  0.01086703 -0.08164715 -0.01987689 -0.12832416  0.41043748\n",
      "  0.17798687 -0.18113815 -0.30124164 -0.05083137 -0.3987406  -0.11539483\n",
      " -0.20887648 -0.05649043]\n",
      "Neurona 35: [-0.36920885  0.00862276  0.29723974  0.03156092  0.12863805  0.24158483\n",
      "  0.41251347 -0.1492627   0.04304592 -0.08023937 -0.00530341 -0.13441855\n",
      " -0.40730219 -0.4139799   0.30764082  0.33706315  0.1447918   0.39672303\n",
      "  0.36798595 -0.04921186 -0.17690073  0.03861339  0.38438958  0.11433658\n",
      "  0.30722381  0.04874948 -0.05742332  0.16889815  0.03918876 -0.32564594\n",
      " -0.36678551  0.16868424]\n",
      "Neurona 36: [-0.42322613 -0.24680686  0.27449111  0.00609524 -0.41235347 -0.1870933\n",
      " -0.31260135 -0.12916517 -0.07652223  0.38742528  0.32534115 -0.00839928\n",
      " -0.39302653 -0.28629753  0.2480214  -0.38205103 -0.25685444 -0.38773957\n",
      " -0.38091103 -0.00073641 -0.2086553   0.05720201 -0.01480147 -0.01362137\n",
      "  0.18821286  0.13509552  0.09151185  0.03892528  0.06688426 -0.22147676\n",
      " -0.10571563  0.23770026]\n",
      "Neurona 37: [ 0.06249093 -0.11430076  0.2745185   0.34912175 -0.33131184  0.05875758\n",
      "  0.10125433 -0.28702195 -0.29493522  0.16305471 -0.31493567 -0.20730009\n",
      "  0.05498309 -0.12818494 -0.4177568   0.11656011  0.35417689  0.28697302\n",
      " -0.37948661 -0.02192917 -0.41576703  0.19516519 -0.13159577  0.28793645\n",
      "  0.36600253 -0.36926586 -0.18885916  0.42539369 -0.30207884 -0.31036215\n",
      " -0.1048411  -0.38946443]\n",
      "Neurona 38: [-0.41032942  0.24126298  0.39676972 -0.05083019  0.38553536 -0.29239033\n",
      "  0.04226073  0.1890041  -0.06627694  0.06691948 -0.17791183  0.41124557\n",
      " -0.29636724 -0.25849934  0.38268201 -0.07801311 -0.30329378  0.29947223\n",
      "  0.06467227 -0.09850845  0.28972086  0.31639685 -0.17771768 -0.0346373\n",
      "  0.21346284 -0.12099393 -0.42357216  0.25180969  0.0041994   0.40146014\n",
      "  0.36040487  0.3980054 ]\n",
      "Neurona 39: [ 0.22993238 -0.18375821 -0.07250211  0.32967766 -0.18371188 -0.21959008\n",
      " -0.34774087  0.01229296  0.27605404  0.17092901  0.296247    0.34070291\n",
      " -0.14989349  0.22820739 -0.39242282  0.05395831 -0.33771481 -0.18998041\n",
      " -0.08085733  0.01931834  0.4095583  -0.01291305 -0.02440034  0.30270845\n",
      "  0.04865769 -0.23840898  0.41349334  0.30631007 -0.23806368  0.27166637\n",
      "  0.03240833 -0.20993654]\n",
      "Neurona 40: [ 0.15297331  0.41456    -0.20390099 -0.20469167  0.1427567  -0.09444988\n",
      " -0.29988866  0.4224975   0.42620454  0.06047621 -0.02302052  0.28464115\n",
      "  0.24695733 -0.35508372 -0.32900861 -0.00939275 -0.29917406  0.29991974\n",
      "  0.33891562 -0.06009077  0.26568075 -0.39469364 -0.16270255 -0.11045186\n",
      " -0.37269934  0.2432289  -0.34017575  0.25097826 -0.0605949   0.3336176\n",
      " -0.05047655  0.23981391]\n",
      "Neurona 41: [ 0.38281924 -0.39234215 -0.03915428  0.15521693  0.27080473  0.06285902\n",
      "  0.34945503  0.2552741  -0.09552308  0.17462823 -0.41465716  0.39057327\n",
      "  0.11976481  0.23625814  0.31451199 -0.21459682 -0.3570053   0.28895376\n",
      "  0.16855085 -0.27841566 -0.42297889 -0.08599432  0.04322603 -0.21272131\n",
      " -0.22025819  0.15698916 -0.06560213  0.02755424 -0.30754052 -0.17369385\n",
      " -0.09438859  0.3756106 ]\n",
      "Neurona 42: [-0.12240349  0.31085993  0.18630076  0.15713708  0.14770316  0.35506636\n",
      " -0.23247066  0.16516467 -0.36367172 -0.37532173  0.23464986 -0.29194869\n",
      "  0.17207359  0.39039703  0.17589985 -0.04319434 -0.4168065  -0.3024226\n",
      "  0.04477476  0.20219487  0.1999362   0.0601006   0.01890344  0.20976352\n",
      "  0.27369046  0.20472224  0.09398789 -0.04604258 -0.34854769 -0.29336214\n",
      " -0.02813717  0.1986536 ]\n",
      "Neurona 43: [-0.38432088 -0.24514502  0.097012    0.03370653 -0.15399795 -0.10883902\n",
      "  0.12536818 -0.21905614 -0.14613407  0.27320646 -0.26962757 -0.35840363\n",
      "  0.23645998  0.22447733  0.10233846 -0.22769113  0.381336   -0.09723387\n",
      "  0.23961125 -0.09947832  0.13424041 -0.37765121 -0.27247746 -0.06675204\n",
      " -0.39405314 -0.0500119   0.18350847  0.06592282  0.41650366  0.36882441\n",
      " -0.03961857 -0.29773627]\n",
      "Neurona 44: [-0.26773434  0.34573996  0.00305207 -0.12686718 -0.25987729  0.12170487\n",
      "  0.1029739  -0.02395669 -0.1933618  -0.27412137  0.23891212 -0.27083913\n",
      "  0.24988965 -0.09421641 -0.14577385 -0.41098477 -0.04126472 -0.00423712\n",
      "  0.02706192  0.38899612  0.18301125 -0.2680567   0.28281864  0.3202704\n",
      "  0.2456388   0.3120259  -0.05885362 -0.18237083 -0.08294191  0.41991028\n",
      "  0.15699402  0.23122513]\n",
      "Neurona 45: [-0.1256598  -0.08113645 -0.03595568  0.26445925  0.30037941 -0.05030206\n",
      "  0.41687236  0.23591657  0.36369156 -0.03665034  0.11346062  0.10025476\n",
      "  0.33031168 -0.18224318 -0.34606492 -0.18265318  0.40767203 -0.29109671\n",
      " -0.03836947  0.34318067 -0.08014659 -0.30432318  0.0525067   0.25106291\n",
      " -0.36601704 -0.07034196 -0.02997368  0.32516754  0.02785192  0.04468834\n",
      " -0.07597791 -0.0255615 ]\n",
      "Neurona 46: [ 0.15225391  0.13071664  0.16724017 -0.36920916  0.36491891  0.13625896\n",
      "  0.32580366 -0.13555071  0.0262224  -0.14355403  0.39964367  0.14112275\n",
      "  0.16809478 -0.38519266 -0.05796479  0.0569389   0.36046876 -0.22040133\n",
      "  0.19599095 -0.29797706  0.10841489 -0.07959538  0.01591222 -0.3378901\n",
      "  0.05815458 -0.41643662  0.32885322  0.23316687  0.11124969  0.09683624\n",
      "  0.38261318  0.39396621]\n",
      "Neurona 47: [-0.10478298  0.14684949 -0.15217131  0.35697105 -0.25431843 -0.28680084\n",
      "  0.1608829   0.03367171  0.42599694  0.35405733 -0.36716921 -0.37628514\n",
      "  0.18545352 -0.22042118 -0.4100763   0.27453164  0.30933323 -0.34833847\n",
      "  0.10375436  0.11809248  0.22558027 -0.20597452 -0.01891561  0.0416596\n",
      " -0.12450024 -0.09033564  0.17667609 -0.38241848  0.05193023 -0.22989139\n",
      "  0.18901975 -0.36321878]\n",
      "Neurona 48: [-0.11349737  0.15637437  0.27109973  0.40177149 -0.26529719  0.07758479\n",
      " -0.28145221  0.42215598  0.1396877  -0.19625851 -0.03491855  0.37436974\n",
      "  0.1674953   0.06853954  0.35949051  0.2803267  -0.23493571  0.31402996\n",
      "  0.06906933  0.41742599  0.06633002  0.36249426  0.19724705  0.29790251\n",
      " -0.41996787 -0.09652527  0.3298475   0.38323828  0.23041314  0.33900476\n",
      " -0.15161773  0.06393551]\n",
      "Neurona 49: [-0.20145927 -0.42604266 -0.02490833 -0.03113749  0.38690153 -0.27131812\n",
      " -0.07747745  0.01772197  0.20061377 -0.30149092  0.25598806 -0.40272344\n",
      " -0.10898506  0.24576298 -0.37692685 -0.15461107  0.29090925  0.02314521\n",
      " -0.34214071  0.30030338 -0.00422827  0.30223021 -0.16391378  0.30807523\n",
      " -0.38645165  0.01403765  0.01845677 -0.37411795  0.16828221  0.13937396\n",
      " -0.38132779  0.32203112]\n",
      "Neurona 50: [ 0.39153396  0.06161125  0.06966408  0.1707752   0.08652016 -0.17999135\n",
      "  0.08642023 -0.30326448  0.18023057 -0.11266727  0.31670944 -0.11618686\n",
      " -0.4028277  -0.3923167   0.11280044 -0.05436706  0.03734632 -0.40443974\n",
      "  0.3083297   0.17257303 -0.36748004 -0.22459991  0.09261979  0.01976863\n",
      " -0.30587062  0.3863592   0.23183426 -0.16398877  0.11082504  0.2441408\n",
      "  0.36603006 -0.12226726]\n",
      "Neurona 51: [ 0.21581536 -0.07353265 -0.10706526  0.11361802 -0.20011931  0.00988646\n",
      "  0.15153183  0.1136291  -0.14286206  0.37328849 -0.04896182 -0.24722895\n",
      "  0.06611657 -0.29319605  0.16802015 -0.36884888  0.15337172  0.22249586\n",
      " -0.30576869  0.15099363  0.01279747 -0.29415612  0.13535306 -0.32974971\n",
      "  0.20647457  0.42038719  0.0956625   0.40730048 -0.14692238 -0.00432512\n",
      "  0.08497955 -0.38993197]\n",
      "Neurona 52: [-0.21857611 -0.21627719  0.19237812  0.27518865  0.14861426 -0.32330219\n",
      "  0.02791585 -0.34625759 -0.18372917 -0.31355224 -0.30603332 -0.20041215\n",
      " -0.17610032  0.05588505 -0.19710572 -0.30361129 -0.23484934  0.31760715\n",
      "  0.18172654  0.01834198 -0.07116106 -0.09571801 -0.28453995 -0.14719183\n",
      " -0.11296642  0.06171551 -0.19835128 -0.35311034  0.38739567 -0.30046382\n",
      "  0.08390658  0.17685505]\n",
      "Neurona 53: [ 0.37572396 -0.03747733  0.03617357 -0.36122802 -0.11645131 -0.05727269\n",
      " -0.38008305  0.07894929 -0.41064942  0.19461913  0.094909   -0.00799098\n",
      " -0.37627462 -0.05873359 -0.12589711  0.34244529  0.36646953  0.18462895\n",
      " -0.36360248  0.39635998  0.35905758 -0.12804158 -0.26547675 -0.02977482\n",
      "  0.07540535 -0.30430844 -0.00267096 -0.09083696  0.16082758  0.0230563\n",
      "  0.40226222 -0.33632141]\n",
      "Neurona 54: [ 0.28812357 -0.07794005 -0.13077131 -0.06861884  0.08869991 -0.25913763\n",
      " -0.07969839 -0.13234526  0.10819408  0.42307324 -0.29370008 -0.08911776\n",
      " -0.02156031 -0.31974222  0.29924492  0.2001584   0.28205497 -0.24687755\n",
      "  0.3831926  -0.07189746 -0.07456032  0.23241653 -0.11818151 -0.42146214\n",
      "  0.37967941 -0.24362459 -0.06644494 -0.04131065 -0.19653079  0.21701353\n",
      "  0.20998595  0.13155245]\n",
      "Neurona 55: [-0.31740221  0.21758714 -0.26468933  0.33010943 -0.1485547  -0.37846126\n",
      "  0.28688245  0.01983126  0.40505668 -0.32110786  0.38010729 -0.27481398\n",
      "  0.37287562  0.03362087 -0.20658866 -0.06971234 -0.00098631 -0.01118543\n",
      "  0.39394095  0.20351449 -0.38808955 -0.26345333 -0.00437439 -0.35897675\n",
      "  0.26453617  0.3393249   0.33820175 -0.36848677  0.17435608 -0.00833726\n",
      " -0.12792293  0.36572825]\n",
      "Neurona 56: [-0.32361753  0.02400969 -0.19556651 -0.12721228  0.40523379 -0.26962138\n",
      "  0.08121687  0.05820896  0.00387493  0.31089058 -0.20712552  0.11337744\n",
      "  0.2130043   0.11852208  0.03556125 -0.35991059 -0.12608137 -0.35250409\n",
      " -0.13725878 -0.3905467  -0.28424319  0.06036318 -0.22497674 -0.33007577\n",
      "  0.34905582 -0.18088083  0.20857063 -0.22242259  0.11114226  0.23281161\n",
      "  0.09720395 -0.42265884]\n",
      "Neurona 57: [ 0.07582589  0.20493701 -0.06808814 -0.0293642   0.06884597  0.2299367\n",
      "  0.27868644  0.30442927  0.41007733  0.1844827  -0.38401032  0.36976923\n",
      "  0.0890912   0.37113208  0.14861621 -0.12978228  0.19274246  0.39859769\n",
      "  0.26629657 -0.14001862  0.05331778 -0.19299931 -0.05125934 -0.16104171\n",
      "  0.17091146 -0.28726859  0.12904361  0.02838205 -0.30807325  0.0142639\n",
      "  0.06842785 -0.12602603]\n",
      "Neurona 58: [-0.39880187  0.39808211  0.36507364  0.32656984  0.28256261 -0.04614088\n",
      " -0.38403634 -0.31102653  0.24779812 -0.29044318  0.03050457 -0.30611627\n",
      "  0.16641012  0.41022764  0.05211681  0.23935636 -0.37236021  0.10985848\n",
      " -0.17760862 -0.37078572  0.22693103 -0.40369213  0.19802098  0.07270815\n",
      " -0.31300595 -0.2709043  -0.05759853 -0.39730251 -0.3286107  -0.14610602\n",
      "  0.29750195  0.23568988]\n",
      "Neurona 59: [-0.25267111  0.12213044 -0.02743804  0.02508178  0.1521845  -0.29092453\n",
      "  0.18319872 -0.10613381  0.29666689 -0.07292732  0.26336161 -0.39338673\n",
      "  0.23622614 -0.01977374 -0.4047945  -0.35352721 -0.10849594  0.07475871\n",
      "  0.24818167  0.41960234 -0.34387259 -0.30439782  0.15618849  0.35600479\n",
      " -0.03737796 -0.28390536 -0.11228715 -0.04660689 -0.35888959  0.27603056\n",
      " -0.19300405 -0.23268705]\n",
      "Neurona 60: [ 0.4180686   0.22518429 -0.14773925  0.23444003  0.13700956  0.24521065\n",
      " -0.3971569   0.34552349 -0.33099736 -0.36439072  0.17392012  0.42296571\n",
      " -0.39059548  0.00539998  0.16248114  0.28946729 -0.02778295 -0.22013911\n",
      " -0.25575465  0.20961137 -0.21066196  0.04995775  0.29099086 -0.15319366\n",
      " -0.2215268  -0.39609679  0.28226317  0.0474713   0.07220167  0.05548241\n",
      "  0.03784073 -0.26508661]\n",
      "Neurona 61: [-0.20810726  0.30836061 -0.17172931 -0.15867024  0.02958973 -0.39564314\n",
      "  0.30292096  0.07887349 -0.32042094  0.37683307 -0.23432613  0.37823323\n",
      " -0.22261847 -0.04905075  0.18720663  0.27855967 -0.3695827  -0.19669031\n",
      " -0.31398506 -0.20372294  0.33473287 -0.0526156  -0.28377354 -0.32417201\n",
      "  0.38984421 -0.18836567 -0.1674205   0.36340431 -0.29512969 -0.16909237\n",
      "  0.13606933  0.42170367]\n",
      "Neurona 62: [-0.03716906 -0.12938928  0.25591224  0.18776457  0.16511403  0.30979979\n",
      " -0.0246476   0.06053814 -0.15179991  0.12442043  0.14987452 -0.36923688\n",
      " -0.22973443 -0.30343203  0.01755099 -0.08543606 -0.11321669  0.03042987\n",
      " -0.27817513 -0.22920809  0.01904637  0.33220735  0.39720273  0.31523666\n",
      " -0.12295245  0.05891195 -0.21424582  0.32713148  0.05650027  0.02176454\n",
      "  0.42350903 -0.10519324]\n",
      "Neurona 63: [-0.28178267 -0.04495422  0.41792286 -0.22892991  0.16984352 -0.04380691\n",
      " -0.22902262 -0.25116205  0.11782017  0.38034015  0.29916252 -0.12846907\n",
      " -0.26450185 -0.41469881  0.36282688  0.12775555  0.31343736  0.21158316\n",
      "  0.36075499  0.25983553 -0.12490049 -0.32951596  0.25769212  0.37338908\n",
      " -0.08933344  0.22293163  0.25301915  0.36616783 -0.20629931  0.19398885\n",
      "  0.15323281  0.07299445]\n",
      "Neurona 64: [-0.11071721  0.17855895 -0.35633339  0.41236802  0.13986288 -0.39465773\n",
      "  0.00160164  0.26467053  0.11041449  0.18315423 -0.35663612  0.34915475\n",
      " -0.33712512 -0.15676021  0.16494215  0.34611836 -0.1569121   0.20780914\n",
      "  0.06116947 -0.03810153  0.03502069  0.28428861 -0.13997393  0.02569573\n",
      "  0.17114845  0.35382135  0.21485643 -0.20544885  0.31700931  0.33630782\n",
      "  0.2953272  -0.36829094]\n",
      "Neurona 65: [ 0.16351184 -0.39673311  0.01808325  0.32529434  0.27572606  0.18125597\n",
      "  0.31471425  0.12399555 -0.14450351  0.37501693 -0.10863626  0.10330096\n",
      "  0.00382155  0.17709953 -0.09605895  0.10351627 -0.39701654  0.23087923\n",
      "  0.27882859 -0.20101767  0.01669701  0.23076165 -0.32794533 -0.40272089\n",
      "  0.17429932 -0.07951534 -0.39942743  0.00163831 -0.04300832  0.32765323\n",
      "  0.13857514  0.20618104]\n",
      "Neurona 66: [-0.31490316  0.09906415  0.34179132  0.12736172  0.19584733 -0.25771893\n",
      " -0.03855095  0.16390906  0.37466333  0.16042874  0.39947667 -0.02626483\n",
      "  0.05931498  0.14438793  0.28658551  0.37457     0.34546669 -0.04680574\n",
      "  0.02531394 -0.38902446 -0.41598126  0.39355031 -0.29118475  0.3518118\n",
      " -0.36803923 -0.3958322   0.12036994  0.18235191  0.02979443 -0.25995469\n",
      "  0.28705332 -0.20724903]\n",
      "Neurona 67: [-0.30621763 -0.2714942   0.01297565  0.32690151 -0.37794478  0.11597158\n",
      " -0.38618447 -0.07575346 -0.15050598 -0.07097197 -0.01581137  0.1668472\n",
      " -0.34807224  0.1368972   0.13807424 -0.1808847   0.14309742 -0.11667681\n",
      "  0.20799489  0.36067969  0.03373155 -0.01968111  0.35503659 -0.00902441\n",
      " -0.16621332 -0.10611576  0.25995589  0.40295088 -0.03980248 -0.37320281\n",
      " -0.12600656 -0.38514317]\n",
      "Neurona 68: [-0.1767462   0.04077035  0.05896468 -0.0522953   0.0817423  -0.29053498\n",
      "  0.22110201 -0.13386824  0.03041107  0.25846322  0.18665567  0.35841438\n",
      "  0.33038877 -0.08010126  0.2814935  -0.13963609 -0.22003007 -0.21610081\n",
      "  0.27691952  0.30829095  0.30396491 -0.05924124  0.42407316  0.23039776\n",
      "  0.34363362  0.22997127 -0.16922165  0.42629221 -0.28690004 -0.42634848\n",
      " -0.0324155  -0.24721538]\n",
      "Neurona 69: [ 0.33828441 -0.22785675  0.21715071 -0.27031168 -0.41618504  0.42122558\n",
      " -0.35843516  0.15342951  0.06921198  0.22211269 -0.38863315 -0.14077499\n",
      " -0.2291843   0.3583968   0.40814845 -0.31046633  0.37367009  0.10833277\n",
      " -0.23831441  0.37586014 -0.18093403  0.39818293 -0.10935415 -0.29605453\n",
      " -0.22816939  0.10619958 -0.10368821  0.3072143  -0.22370539 -0.24234685\n",
      "  0.23203692 -0.33180539]\n",
      "Neurona 70: [-0.14695476  0.00954985 -0.24446343  0.3577616  -0.21600934  0.2365855\n",
      "  0.20954232 -0.10601147  0.16081485  0.40267423 -0.39713979 -0.2456032\n",
      "  0.03202318 -0.30046777  0.40286188  0.37767337  0.40272317  0.02197768\n",
      " -0.16839969  0.380264    0.14203416 -0.07377265  0.0971244  -0.20459037\n",
      "  0.23347133  0.39223331 -0.24300676  0.27410891 -0.13509153 -0.20296641\n",
      "  0.12519959 -0.16451293]\n",
      "Neurona 71: [ 0.09730098  0.15791684  0.15925134 -0.00275607 -0.19973453 -0.04183284\n",
      " -0.29054544  0.06037514 -0.33412125 -0.16598858  0.04921173 -0.41791439\n",
      " -0.37732777  0.25826221  0.20737737  0.24174262 -0.04729288 -0.20131715\n",
      " -0.11627216 -0.12183736  0.38839829  0.17053047  0.11496711 -0.40985873\n",
      "  0.02055232 -0.32951371 -0.38216265  0.30670214 -0.02391497  0.42358549\n",
      " -0.20925275  0.11826567]\n",
      "Neurona 72: [ 0.29294992  0.10187313  0.12189927  0.2434986   0.09987296 -0.17443598\n",
      " -0.19981754 -0.17951861  0.24726674  0.33832575 -0.19416455  0.12711826\n",
      " -0.17364453 -0.25621418 -0.2997728  -0.09496791 -0.07150984 -0.30063096\n",
      " -0.12504906  0.00352627  0.08334198 -0.36992373 -0.0503139   0.16423389\n",
      " -0.24403505  0.18236969 -0.08823947  0.38705481  0.21274965  0.06836703\n",
      "  0.00958002  0.0291303 ]\n",
      "Neurona 73: [ 0.35225336  0.13562668 -0.10487124 -0.08734971 -0.01059285  0.34763036\n",
      " -0.19021505 -0.01405206 -0.3094215  -0.19793558  0.39743019 -0.13903244\n",
      " -0.15437239 -0.12531007  0.33076726  0.02855437 -0.28935364 -0.01042036\n",
      "  0.11591089 -0.00907078  0.40999051 -0.38695155  0.2934421  -0.35865265\n",
      " -0.14075193 -0.26964136 -0.31706747 -0.26930186  0.02747586  0.03079667\n",
      " -0.40713838  0.12327531]\n",
      "Neurona 74: [ 2.50095919e-02  3.65347338e-01  1.60521902e-02 -2.26963653e-01\n",
      " -2.59049340e-01 -1.50732893e-01  1.54474472e-01 -9.69684064e-02\n",
      "  4.16355599e-01 -4.19912405e-01 -2.51799208e-01 -5.29169141e-02\n",
      "  2.18106111e-02 -3.24816848e-01 -6.25647624e-02  2.98164739e-01\n",
      " -3.66432123e-01  1.26844138e-01  1.83119339e-01  3.11785323e-01\n",
      " -1.45151471e-01 -3.28808903e-01 -4.82573258e-03  1.67117598e-01\n",
      " -1.30870039e-04  2.33921342e-01 -4.21814447e-01 -3.70218509e-01\n",
      " -1.21067299e-02  2.84214533e-01  1.82191866e-01 -2.10707520e-01]\n",
      "Neurona 75: [-0.18788833 -0.1555846  -0.28792099 -0.01140622  0.24820291  0.07552789\n",
      " -0.01386546 -0.40154886 -0.02858091 -0.05749885  0.02188531 -0.2852442\n",
      "  0.10633509 -0.29444913 -0.05009694 -0.21418982  0.13470473  0.31019338\n",
      "  0.39168651 -0.36538921 -0.41721264  0.31600079 -0.42560292  0.36600597\n",
      " -0.40680561 -0.19297117  0.07574958 -0.13027164 -0.38308563  0.3395018\n",
      " -0.2985146  -0.14748769]\n",
      "Neurona 76: [ 0.12505356 -0.07595733 -0.2766405   0.18477815  0.27778255 -0.25042808\n",
      "  0.4026184  -0.3782403   0.25062663  0.251656    0.17865125  0.01654657\n",
      " -0.20637672  0.17125064 -0.23885967 -0.38158577 -0.07942772  0.2706812\n",
      " -0.06444547  0.07764061  0.24805851 -0.08203077 -0.07689181 -0.34272322\n",
      "  0.3487042   0.17261296  0.32345867  0.1022      0.42120953  0.39565716\n",
      " -0.03480064 -0.09626809]\n",
      "Neurona 77: [ 0.02076972  0.05552611  0.39255799  0.04311757 -0.12389399 -0.15518713\n",
      "  0.25570931  0.38370472 -0.09471701 -0.41045028  0.12247846  0.0804591\n",
      "  0.10288247  0.06267182  0.1351536  -0.23821205 -0.2503935  -0.25465077\n",
      " -0.41053725 -0.17399277 -0.40140593 -0.33574562 -0.12597578  0.15418089\n",
      "  0.3702978   0.17430984  0.16343841 -0.12505008 -0.28492495  0.1222099\n",
      " -0.24064735  0.36405163]\n",
      "Neurona 78: [-0.31674737  0.15582436 -0.39075688  0.27833035  0.01740234 -0.24856634\n",
      "  0.18437234  0.06101299 -0.13164568 -0.38492508 -0.12475914  0.15733819\n",
      " -0.3603628  -0.2058561  -0.03329425  0.09203137 -0.12677944 -0.15821442\n",
      " -0.2732147  -0.00498429  0.0422823   0.14415879  0.27742201  0.13155578\n",
      " -0.20818917  0.24922482  0.24823349 -0.1774771  -0.20905246 -0.41815289\n",
      " -0.08029117  0.30630408]\n",
      "Neurona 79: [-0.0741183   0.32526448 -0.39887116 -0.07437829  0.16691541 -0.17051043\n",
      " -0.0430337  -0.09972578  0.20195436 -0.03485032  0.16566345  0.24218181\n",
      " -0.06302749  0.34941629 -0.13190866 -0.0825674   0.11774282 -0.05516054\n",
      "  0.01478256  0.42370619 -0.14237855 -0.17216794 -0.20954893  0.30197085\n",
      " -0.20087922 -0.25404889 -0.1537577  -0.22090624 -0.14577996  0.28380196\n",
      " -0.09970928  0.27007905]\n",
      "Neurona 80: [-0.03709911 -0.42525984  0.38929935 -0.07133471 -0.18373817  0.27197817\n",
      " -0.07092811  0.08714449  0.41415878  0.25941226 -0.04126171 -0.16345846\n",
      " -0.27254217  0.12290138  0.18565565 -0.36375425  0.15965897  0.10915237\n",
      " -0.08857299  0.01831902  0.31753991  0.31303914  0.02481035  0.15512493\n",
      " -0.30526466 -0.02444952 -0.42132612 -0.35385442  0.19479637  0.07132516\n",
      " -0.11357656  0.00641752]\n",
      "Neurona 81: [ 0.19345537  0.23692316  0.1555606   0.02353277 -0.01765507  0.21826525\n",
      " -0.2895946   0.18859292  0.3994729  -0.17575658  0.23890605 -0.2230599\n",
      " -0.22098828  0.29263463 -0.23580335 -0.19490768  0.15862884 -0.22667804\n",
      " -0.09648048  0.16888933 -0.31957616 -0.08776665  0.11800384 -0.27657211\n",
      "  0.08285055  0.13610265  0.09733202 -0.00318621 -0.33364854 -0.29635528\n",
      " -0.2984405  -0.27201919]\n",
      "Neurona 82: [ 0.27269944 -0.39812002 -0.2975919   0.04334057  0.23974801  0.0164778\n",
      " -0.02095519  0.19371203 -0.40549954 -0.05131799 -0.09074085 -0.10967525\n",
      " -0.02864129 -0.34753412  0.01946932  0.267389    0.40412438 -0.02509007\n",
      "  0.30568574  0.42261282  0.03087392  0.1638335  -0.41969174  0.37795309\n",
      "  0.296544   -0.388057    0.28985478  0.31576658 -0.08549552 -0.30726636\n",
      " -0.19942187  0.29781277]\n",
      "Neurona 83: [-0.39167812 -0.41611082 -0.00532489  0.05805155  0.26509412 -0.0418554\n",
      "  0.36331963 -0.34462436  0.07698547  0.29842688  0.20337909  0.38186584\n",
      " -0.3207065  -0.09803921  0.27924709  0.27160361 -0.41571812 -0.0557454\n",
      "  0.06657602  0.2820591   0.25313327 -0.15121     0.40631735 -0.0048306\n",
      " -0.38932616 -0.29022486 -0.38140603  0.38179783  0.21072127 -0.11054473\n",
      "  0.38829804 -0.10605029]\n",
      "Neurona 84: [-0.18180016  0.36799044  0.08782973 -0.05975827  0.04986987  0.11822006\n",
      " -0.30600903 -0.00600832 -0.08137872  0.2083107  -0.20350866 -0.28511054\n",
      " -0.10077164 -0.07125108 -0.3614637  -0.03739206  0.31321763  0.30171541\n",
      "  0.40745271  0.2466636  -0.3876495  -0.02784438  0.23166961  0.26312677\n",
      "  0.32016282 -0.2729662   0.02282952  0.2359956  -0.32665196 -0.09789683\n",
      "  0.32570087  0.35629026]\n",
      "Neurona 85: [-0.3336562  -0.15141594  0.17396454 -0.09528343  0.1953726  -0.26352868\n",
      "  0.20141608  0.0171594  -0.05306424  0.29822499  0.3031949  -0.22599254\n",
      " -0.05654627  0.35474007  0.1119583   0.32995231  0.39219362  0.29956246\n",
      "  0.21586498  0.2396999   0.17234155 -0.26068701  0.24008561  0.06651811\n",
      "  0.3304189  -0.23317454  0.22754333 -0.17210125 -0.254961    0.13662998\n",
      "  0.32218352 -0.16698779]\n",
      "Neurona 86: [ 0.28068591  0.37653575 -0.18421892 -0.37417438  0.07983976 -0.14979298\n",
      " -0.16116246  0.3635281   0.39401613 -0.19510047 -0.2762459   0.23917205\n",
      " -0.19628634 -0.37185878  0.23658651 -0.42576696 -0.08314549 -0.03000721\n",
      " -0.04071228 -0.24237056  0.14117149  0.27312683  0.41026293 -0.04145619\n",
      " -0.32847471  0.17459427 -0.14022081 -0.35323892 -0.13570109  0.38493687\n",
      " -0.21643346  0.10844009]\n",
      "Neurona 87: [-0.32186338  0.03574658  0.34846119 -0.32752669  0.28669215  0.0498712\n",
      " -0.38547571  0.41797166 -0.40156646 -0.41633171 -0.12924186  0.08165015\n",
      " -0.06044873  0.0279995   0.1534009   0.09747232  0.05599276 -0.00947931\n",
      "  0.39303582 -0.42315952 -0.01451327  0.17124038  0.41819303  0.11090647\n",
      " -0.31843504 -0.11124707  0.34426877 -0.20973669  0.30056456  0.39742785\n",
      "  0.37700713 -0.40024147]\n",
      "Neurona 88: [-0.12783044 -0.04591846  0.22866164  0.39237379 -0.29195169 -0.13228705\n",
      "  0.27008294  0.11016689  0.28916414 -0.03324114  0.42231696 -0.38059082\n",
      " -0.39702536  0.25523884 -0.20273484  0.05148483  0.00717209  0.18887483\n",
      "  0.06375872  0.1599204  -0.2239817   0.32401418  0.10106869 -0.06108008\n",
      "  0.2596369   0.3877445  -0.074973   -0.32553144  0.16398673 -0.2160955\n",
      "  0.16211989 -0.32500969]\n",
      "Neurona 89: [ 0.40155009 -0.00358124  0.25281265  0.213951    0.06907409 -0.30347343\n",
      "  0.02731484  0.29033894 -0.08920389 -0.01079277  0.35651165 -0.01109793\n",
      " -0.22335946  0.16089112  0.39354808 -0.26785588 -0.01001739 -0.05287736\n",
      " -0.11893536 -0.27754874 -0.4117857  -0.12504156 -0.1512548  -0.08076705\n",
      " -0.4157261   0.20016015 -0.35881628 -0.03267944  0.04153951  0.39713742\n",
      "  0.11346329 -0.25436603]\n",
      "Neurona 90: [-0.1105349  -0.33527038 -0.33526263 -0.20357592  0.28698384  0.1492435\n",
      "  0.06211739  0.25060047 -0.38714407  0.14457883  0.06860843  0.31564771\n",
      "  0.23088343 -0.29471542 -0.01598176 -0.21875082  0.19816395 -0.11923636\n",
      " -0.35872463 -0.20699971 -0.22784502 -0.28406791  0.13810016  0.04982944\n",
      "  0.00582934 -0.24591322  0.40827358  0.19908988  0.0693779   0.20636892\n",
      "  0.15705485 -0.38268214]\n",
      "Neurona 91: [ 0.34717365 -0.32576327 -0.35961899 -0.1817532   0.26682755  0.16473108\n",
      "  0.37427573  0.17008484  0.39599399 -0.1295991   0.42260055  0.40288762\n",
      " -0.05024953  0.39317835 -0.01257732  0.02617683 -0.22169939  0.26265623\n",
      " -0.17911697 -0.27129092  0.08601754  0.12731666  0.20078009  0.33516259\n",
      "  0.3350538  -0.27876208 -0.21204082  0.3589588   0.37492242  0.00253389\n",
      "  0.22366702  0.09542923]\n",
      "Neurona 92: [-0.20277628 -0.05384991 -0.23204605  0.20511102 -0.30843945  0.10566453\n",
      "  0.20529054  0.24108966  0.40077545 -0.3788499   0.11642681  0.08557857\n",
      "  0.12027132 -0.28508144  0.29173765  0.10968767  0.31569893 -0.27404857\n",
      "  0.21932719  0.02251562 -0.02477059 -0.03233335  0.41374187  0.02132301\n",
      "  0.41285811  0.25709125 -0.35955788 -0.05719116 -0.11989905  0.13373608\n",
      "  0.06552256 -0.2233194 ]\n",
      "Neurona 93: [-0.00594642 -0.08028256 -0.00607463  0.08996938 -0.34027026 -0.13866055\n",
      " -0.18510234 -0.09296945 -0.42031795 -0.33051627  0.30962024  0.34119783\n",
      "  0.34766779 -0.07837561 -0.04223725 -0.09629914 -0.29508125  0.06133316\n",
      " -0.36097629 -0.29857603 -0.36562221  0.41453054 -0.18968576 -0.20981016\n",
      "  0.35998129  0.04274249 -0.37583843 -0.02284187 -0.23377144  0.42490122\n",
      " -0.16527871  0.28027669]\n",
      "Neurona 94: [ 0.37557311  0.24896286 -0.13687086 -0.10973556  0.39311702 -0.17768513\n",
      " -0.41675366 -0.15635253 -0.17579877  0.41486189  0.04327513 -0.15638868\n",
      " -0.10152128  0.32891542  0.29568236 -0.09812031 -0.05951777 -0.01587168\n",
      "  0.05154786  0.20771912 -0.20130332 -0.34576696  0.16684263 -0.17417389\n",
      " -0.07782916  0.31078698  0.02038487  0.22069884  0.30066313 -0.17919355\n",
      " -0.10179715 -0.13410764]\n",
      "Neurona 95: [-0.02907467 -0.37630826 -0.10992641  0.33126133  0.1105894   0.18131513\n",
      " -0.35525311  0.4071025   0.28184323  0.32921638 -0.41143574  0.08809206\n",
      " -0.32593224  0.08610197  0.02761924 -0.30617046  0.05788007 -0.08172135\n",
      " -0.20921598 -0.40543616  0.18151532 -0.07076283  0.24760086 -0.27175991\n",
      " -0.2413387   0.25497216 -0.30873351 -0.40510614 -0.0535705   0.19157418\n",
      " -0.39289675 -0.14020722]\n",
      "Neurona 96: [ 0.39429746  0.19703814  0.00772216  0.13231031  0.1338776  -0.0810927\n",
      " -0.11310851  0.21870451 -0.25917744 -0.1767757   0.27995294 -0.05791357\n",
      "  0.17281446  0.10679924  0.31971419  0.34882264  0.06136564  0.35954985\n",
      " -0.04328216  0.23328618  0.32619449 -0.31484774 -0.07831274  0.18149927\n",
      " -0.21363463 -0.06375953  0.21698564  0.2991311  -0.41632079  0.00661008\n",
      "  0.03204231 -0.16360331]\n",
      "Neurona 97: [-0.1745303  -0.02466123  0.2068112   0.25390981 -0.17342285  0.36378636\n",
      "  0.36029808 -0.19120475 -0.36263986 -0.01150927  0.0026549   0.17050848\n",
      " -0.12211356  0.37363222 -0.01759317  0.41861161 -0.36112303 -0.40038748\n",
      "  0.41800915 -0.21133592  0.22561225  0.42121401 -0.07441118 -0.14515042\n",
      " -0.41932225  0.2799499   0.24406832 -0.30445068  0.30645561 -0.35964592\n",
      " -0.1871147  -0.22192989]\n",
      "Neurona 98: [ 0.19935924  0.23032571  0.27838054 -0.40712448 -0.27101867 -0.24841185\n",
      " -0.05440814 -0.06975567 -0.01575099 -0.1730384  -0.08103335  0.18668264\n",
      " -0.13774347  0.40609919  0.40513083  0.27970319  0.18639173  0.22962803\n",
      " -0.26269314 -0.23772009 -0.03120076 -0.20659712 -0.09066975  0.12102104\n",
      "  0.2373968  -0.03702763 -0.33932302  0.14819121 -0.06776493  0.25046724\n",
      " -0.14729889 -0.07997881]\n",
      "Neurona 99: [-0.32728328  0.10310348  0.27490657  0.07870853 -0.37955165  0.21878108\n",
      " -0.34794607  0.28918307  0.05704703 -0.28877954  0.40608888 -0.06114893\n",
      "  0.40186204 -0.40955411  0.16164316  0.38571538  0.08544129 -0.0630706\n",
      " -0.33345613  0.33295503  0.37483808  0.26558733 -0.33191439  0.01942221\n",
      "  0.12045439  0.35826249 -0.28916364 -0.21910643  0.29930736  0.30970968\n",
      " -0.00974666 -0.07097468]\n",
      "Epoch 1/100 completada\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m mlp \u001b[38;5;241m=\u001b[39m NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Entrenamos la red\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, epochs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m         activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# activations es una lista de arrays, donde cada array es la salida de cada capa (función de activación sigmoidea aplicada)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         deltas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m activations \u001b[38;5;241m=\u001b[39m [x]   \u001b[38;5;66;03m# Inicialmente arranca con el input (x, vector de entrada)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m capa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 33\u001b[0m     salida_capa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m capa])\n\u001b[0;32m     34\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(salida_capa)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mPerceptron.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Propagación hacia adelante de una sola muestra\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Calcula la suma ponderada y aplica la función de activación\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "X_train, y_train, X_test, y_test = load_cifar100(f\"data/CIFAR-100\")\n",
    "\n",
    "# Aplanamos las imágenes (de 32x32x3 a 3072)\n",
    "X_train = X_train.reshape(-1, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = X_train.reshape((X_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "X_train = X\n",
    "y_train = Y\n",
    "\n",
    "# Definimos la arquitectura: dos capas ocultas\n",
    "input_size = X_train.shape[1]  # 3072 para CIFAR-100\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 100  # 100 clases para CIFAR-100\n",
    "\n",
    "# Creamos la red con dos capas ocultas\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación con Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5cefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360782e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
