{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c58837",
   "metadata": {},
   "source": [
    "# Implementación en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581ec229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Perceptron con python: https://pyimagesearch.com/2021/05/06/implementing-the-perceptron-neural-network-with-python/#pyis-cta-modal\n",
    "# Perceptron con PyTorch: https://www.geeksforgeeks.org/what-is-perceptron-the-simplest-artificial-neural-network/\n",
    "# Perceptrones multicapa con PyTorch: https://github.com/rasbt/machine-learning-book/blob/main/ch11/ch11.ipynb\n",
    "# Implementación Forward Propagation: https://www.geeksforgeeks.org/what-is-forward-propagation-in-neural-networks/\n",
    "# Implementación Back Propagation: https://www.geeksforgeeks.org/backpropagation-in-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e47521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/annisin/classification-task\n",
    "# Imagenes blanco y negro\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenes a color RGB\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_cifar10(data_dir='cifar-10-batches-py'):\n",
    "    # Cargar datos de entrenamiento\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        batch = unpickle(os.path.join(data_dir, f'data_batch_{i}'))\n",
    "        data.append(batch[b'data'])\n",
    "        labels.append(batch[b'labels'])\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    labels = np.eye(10)[labels]  # One-hot encoding\n",
    "\n",
    "    # Cargar datos de prueba\n",
    "    test_batch = unpickle(os.path.join(data_dir, 'test_batch'))\n",
    "    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    test_labels = np.eye(10)[test_batch[b'labels']]\n",
    "\n",
    "    return data, labels, test_data, test_labels\n",
    "\n",
    "def load_cifar100(data_dir='cifar-100-python'):\n",
    "    data = unpickle(os.path.join(data_dir, 'train'))\n",
    "    x_train = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_train = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    data = unpickle(os.path.join(data_dir, 'test'))\n",
    "    x_test = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_test = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenes a color RGB\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_cifar10(data_dir='cifar-10-batches-py'):\n",
    "    # Cargar datos de entrenamiento\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        batch = unpickle(os.path.join(data_dir, f'data_batch_{i}'))\n",
    "        data.append(batch[b'data'])\n",
    "        labels.append(batch[b'labels'])\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    labels = np.eye(10)[labels]  # One-hot encoding\n",
    "\n",
    "    # Cargar datos de prueba\n",
    "    test_batch = unpickle(os.path.join(data_dir, 'test_batch'))\n",
    "    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    test_labels = np.eye(10)[test_batch[b'labels']]\n",
    "\n",
    "    return data, labels, test_data, test_labels\n",
    "\n",
    "def load_cifar100(data_dir='cifar-100-python'):\n",
    "    data = unpickle(os.path.join(data_dir, 'train'))\n",
    "    x_train = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_train = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    data = unpickle(os.path.join(data_dir, 'test'))\n",
    "    x_test = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0\n",
    "    y_test = np.eye(100)[data[b'fine_labels']]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b7d05",
   "metadata": {},
   "source": [
    "¡Perfecto! Aquí tienes una guía paso a paso para implementar una red neuronal multicapa (MLP) para clasificación de imágenes, siguiendo la consigna y los recursos que mencionaste:\n",
    "\n",
    "1. **Preparar los datos**\n",
    "   - Descarga y carga los datasets (MNIST, CIFAR-10, CIFAR-100).\n",
    "   - Preprocesa: normaliza los valores de píxeles y convierte las etiquetas a formato numérico o one-hot.\n",
    "\n",
    "2. **Implementación en numpy (bajo nivel)**\n",
    "   - Define la arquitectura: decide el número de capas ocultas y neuronas por capa.\n",
    "   - Inicializa los pesos y sesgos aleatoriamente.\n",
    "   - Implementa la propagación hacia adelante (forward propagation) usando funciones de activación (sigmoide, ReLU, etc.).\n",
    "   - Implementa la retropropagación (backpropagation) para actualizar los pesos usando el gradiente descendente.\n",
    "   - Entrena la red: itera sobre los datos, calcula la pérdida y ajusta los pesos.\n",
    "\n",
    "3. **Implementación en PyTorch**\n",
    "   - Usa `torch.nn.Module` para definir la red.\n",
    "   - Usa `torch.utils.data.DataLoader` para manejar los datasets.\n",
    "   - Define la función de pérdida y el optimizador.\n",
    "   - Entrena la red y evalúa el desempeño.\n",
    "\n",
    "4. **Experimentación**\n",
    "   - Prueba diferentes arquitecturas: número de capas, neuronas, funciones de activación, tasas de aprendizaje, momentum, inicialización de pesos.\n",
    "   - Compara los resultados y documenta cuál configuración funciona mejor.\n",
    "\n",
    "5. **Comparación con el paper NoProp**\n",
    "   - Lee el artículo y compara tus resultados con los reportados.\n",
    "   - Si encuentras implementaciones públicas de NoProp, prueba y compara su desempeño.\n",
    "\n",
    "6. **Informe**\n",
    "   - Documenta el proceso, los experimentos y las conclusiones.\n",
    "   - Incluye gráficos de precisión, pérdida, etc.\n",
    "\n",
    "¿Te gustaría que te ayude a armar la estructura inicial del notebook o prefieres avanzar por tu cuenta y consultarme dudas puntuales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e72a32",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Implementación de un perceptrón simple.\n",
    "    Cada perceptrón tiene sus propios pesos y bias, y utiliza la función de activación sigmoide.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        # Inicialización de pesos y bias con valores pequeños aleatorios\n",
    "        # Esto ayuda a romper la simetría y permite que cada neurona aprenda cosas distintas\n",
    "        # self.weights = np.random.randn(input_size)\n",
    "        # self.bias = np.random.randn()\n",
    "        limit = np.sqrt(6 / (input_size + 1))\n",
    "        self.weights = np.random.uniform(-limit, limit, input_size)\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        # Convierte la suma ponderada en un valor entre 0 y 1\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        # https://interactivechaos.com/es/manual/tutorial-de-deep-learning/derivada-de-la-funcion-sigmoide\n",
    "        # Derivada de la sigmoide\n",
    "        # x es el valor de la activación sigmoide. Es decir, se ejecutará esto en realidad => sigmoid(x) * (1 - sigmoid(x))\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante de una sola muestra\n",
    "        # Calcula la suma ponderada y aplica la función de activación\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "\n",
    "    def update(self, x, delta):\n",
    "        # Actualiza los pesos y bias usando el gradiente calculado (delta)\n",
    "        # delta ya incluye la derivada de la sigmoide (por la regla de la cadena)\n",
    "        # La actualización sigue la dirección del gradiente descendente\n",
    "        self.weights += self.learning_rate * delta * x  # probar con -= capaz\n",
    "        self.bias += self.learning_rate * delta # probar con -= capaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657721fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Red neuronal multicapa compuesta por capas de perceptrones.\n",
    "    Permite definir cualquier cantidad de capas y neuronas por capa.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1):\n",
    "        # layer_sizes: lista con el tamaño de cada capa, EJEMPLO: [784, 100, 10]\n",
    "        self.layers: list[list[Perceptron]] = []\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            capa = [Perceptron(layer_sizes[i-1], learning_rate) for _ in range(layer_sizes[i])]\n",
    "            # EJEMPLO:\n",
    "            # - Para la primera capa oculta generaria 100 neuronas que aceptan vectores inputs de 784 dimensiones (input layer).\n",
    "            # - Para la capa de input no se crea un perceptrón, ya que es la entrada de la red.\n",
    "            # - La capa de salida generaria 10 neuronas que aceptan vectores inputs de 100 dimensiones (capa oculta).\n",
    "            self.layers.append(capa)\n",
    "        \n",
    "        # Mostrar la estructura de la red\n",
    "        # print(\"Estructura de la red:\")\n",
    "        # for i, capa in enumerate(self.layers):\n",
    "        #     print(f\"Capa {i+1}: {len(capa)} neuronas, cada una con {capa[0].weights.shape[0]} pesos\")\n",
    "        # print(\"\\nPesos de la capa intermedia:\")\n",
    "        # for idx, neuron in enumerate(self.layers[0]):\n",
    "        #     print(f\"Neurona {idx}: {neuron.weights}\")\n",
    "        print(\"\\nPesos de la capa de salida:\")\n",
    "        for idx, neuron in enumerate(self.layers[-1]):\n",
    "            print(f\"Neurona {idx}: {neuron.weights}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante para una muestra\n",
    "        activations = [x]   # Inicialmente arranca con el input (x, vector de entrada)\n",
    "        for capa in self.layers:\n",
    "            salida_capa = np.array([neuron.forward(activations[-1]) for neuron in capa])\n",
    "            activations.append(salida_capa)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predicción para una muestra\n",
    "        activations = self.forward(x)\n",
    "        return activations[-1]\n",
    "\n",
    "    def train(self, X, Y, epochs=10):\n",
    "        # Entrenamiento usando backpropagation (solo para fines didácticos, no optimizado)\n",
    "        for epoch in range(epochs):\n",
    "            for xi, yi in zip(X, Y):\n",
    "                # Forward\n",
    "                activations = self.forward(xi)  # activations es una lista de arrays, donde cada array es la salida de cada capa (función de activación sigmoidea aplicada)\n",
    "                # Backward\n",
    "                deltas = [None] * len(self.layers)\n",
    "                # Capa de salida\n",
    "                error = yi - activations[-1]    # Resta de vectores\n",
    "                deltas[-1] = error * np.array([\n",
    "                    neuron.activation_derivative(activations[-1][j])\n",
    "                    for j, neuron in enumerate(self.layers[-1])\n",
    "                ])\n",
    "                # Capas ocultas\n",
    "                for l in reversed(range(len(self.layers)-1)):\n",
    "                    delta_next = deltas[l+1]    # lista de deltas de la capa siguiente (son 10 deltas)\n",
    "                    pesos_next = np.array([neuron.weights for neuron in self.layers[l+1]])  # En la primera iteracion para MNIST, es un array de arrays de pesos de la capa siguiente -> matriz de 10 filas y 100 columnas\n",
    "                    # deprecated: deltas[l] = self.layers[l][0].activation_derivative(activations[l+1]) * np.dot(pesos_next.T, delta_next)\n",
    "                    deltas[l] = np.array([\n",
    "                        neuron.activation_derivative(activations[l+1][j])\n",
    "                        for j, neuron in enumerate(self.layers[l])\n",
    "                    ]) * np.dot(pesos_next.T, delta_next)   # el resultado del producto escalar (np.dot) es un vector de 100 dimensiones (deltas de la capa oculta)\n",
    "\n",
    "                # Actualización de pesos\n",
    "                for l, capa in enumerate(self.layers):\n",
    "                    for j, neuron in enumerate(capa):\n",
    "                        neuron.update(activations[l], deltas[l][j])\n",
    "                        \n",
    "            if epoch % max(1, epochs//10) == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "62c5ec30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos un subconjunto pequeño de MNIST para entrenamiento rápido\n",
    "# Usamos solo las primeras 100 muestras para entrenamiento y 10 para test\n",
    "# X_train = x_train.reshape((x_train.shape[0], -1))[:1000]\n",
    "# y_train = y_train[:1000]\n",
    "# X_test = x_test.reshape((x_test.shape[0], -1))[:100]\n",
    "# y_test = y_test[:100]\n",
    "\n",
    "# 1. Seleccionamos un porcentaje del total (por ejemplo, 200 muestras)\n",
    "n_samples = 10000  # Cambia este valor para usar más o menos datos\n",
    "X = x_train.reshape((x_train.shape[0], -1))[:n_samples]\n",
    "Y = y_train[:n_samples]\n",
    "\n",
    "# 2. Hacemos un split 80/20 sobre ese subconjunto\n",
    "split = int(0.8 * n_samples)\n",
    "X_train = X[:split]\n",
    "y_train = Y[:split]\n",
    "X_test = X[split:]\n",
    "y_test = Y[split:]\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0a3615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [ 0.25277594 -0.25901185 -0.23589625 -0.21621475  0.04147355 -0.19284623\n",
      "  0.29746064  0.21780454  0.11796198  0.24362143  0.19985841 -0.13193108\n",
      "  0.20957625  0.09096833  0.21613426 -0.08973384 -0.15433086 -0.13624272\n",
      " -0.27474413  0.23855722 -0.00573226  0.28681965 -0.23959232 -0.04457245\n",
      " -0.05709928 -0.22808522 -0.02219191  0.24869431  0.08852247 -0.29075464\n",
      " -0.16868892  0.03969206  0.07012052 -0.21624815  0.22912235 -0.09576322\n",
      "  0.19148913 -0.07847247  0.22975221  0.13776957 -0.1418637   0.20953545\n",
      " -0.19184685 -0.02113817  0.01590189 -0.1263852   0.14541526  0.26099336\n",
      "  0.27415673  0.07376886  0.23037205 -0.11027986  0.21199524 -0.13335125\n",
      " -0.13598982 -0.27195143 -0.14451862 -0.27361537  0.21877152  0.09996908\n",
      "  0.01048936 -0.11595579  0.25824701  0.11299453]\n",
      "Neurona 1: [ 0.07853277  0.06193662 -0.11827783  0.25410143 -0.21954503  0.25838437\n",
      "  0.29308215 -0.02211279  0.11740586 -0.10005625  0.12220754  0.01493195\n",
      " -0.02497028  0.22842021 -0.11268941  0.10200676  0.01065908  0.12233856\n",
      "  0.13380868  0.02016571 -0.26001065 -0.26659328  0.26419694 -0.01518578\n",
      " -0.17055188  0.09655763  0.18178313  0.18364809 -0.0238837   0.15187798\n",
      " -0.02853111  0.13161825 -0.27900293  0.01675922  0.23478284 -0.12178363\n",
      "  0.27905171  0.19163223 -0.2039595   0.26281852 -0.00361642 -0.1697909\n",
      "  0.14351404  0.10731654 -0.2797552  -0.30092609 -0.25235826  0.00779664\n",
      " -0.09624768  0.09311129 -0.08825061 -0.206105    0.03465681  0.09265616\n",
      "  0.2989705   0.223049   -0.13717248 -0.19776274 -0.09665129  0.24784089\n",
      " -0.02653377 -0.15367801 -0.24780692  0.27749169]\n",
      "Neurona 2: [-0.14384496 -0.27233159  0.20182815  0.25142339  0.0248551  -0.07108008\n",
      "  0.07124325 -0.28028657 -0.24407771 -0.17755083  0.23551057  0.22611546\n",
      "  0.06002899  0.20229754 -0.04592427  0.1093786  -0.13712264 -0.29790585\n",
      "  0.15410533  0.16634683  0.06449465  0.12896361  0.06145378 -0.02937718\n",
      "  0.18570408  0.07458237  0.28489916  0.01812674 -0.15318647  0.10436575\n",
      "  0.12862273  0.26965492 -0.02159565 -0.1685991  -0.09530627 -0.15970264\n",
      " -0.13482262  0.06581219 -0.22743925 -0.12116416 -0.01502519  0.24026135\n",
      " -0.19650166 -0.2011102   0.01839396  0.15978453  0.26662373  0.2030404\n",
      "  0.25309111  0.26430231 -0.03960968 -0.18317558  0.15490654 -0.07643912\n",
      "  0.11785004  0.29479468 -0.0714387   0.07831187 -0.27777212  0.17578019\n",
      "  0.25102228  0.26200933 -0.04523837 -0.07375452]\n",
      "Neurona 3: [ 0.02085853  0.19880009 -0.29177217 -0.2430731   0.09694619  0.06240473\n",
      " -0.17107789  0.22558006 -0.2079814  -0.04739114 -0.01215462 -0.22277778\n",
      " -0.24052353  0.09784017  0.30088747  0.09834033 -0.15284782 -0.03964137\n",
      "  0.26207228 -0.14362294 -0.06703046 -0.2509886   0.16002071 -0.09804447\n",
      "  0.19462196  0.10553477  0.2607626   0.24766225 -0.21874232 -0.19702202\n",
      "  0.29235546  0.05523857  0.01923023 -0.21477754  0.15896907  0.18837976\n",
      "  0.03707015 -0.04993086 -0.09279453 -0.03319907  0.11030957  0.21984874\n",
      " -0.23355106  0.00726206 -0.10867736  0.02843242 -0.08740511  0.22155104\n",
      " -0.19475536 -0.03204874 -0.02441954 -0.03011731  0.12485001  0.14975649\n",
      "  0.13163459 -0.09889141  0.14688877 -0.22808154  0.08839062 -0.14684619\n",
      " -0.00771698 -0.21032365  0.07001632  0.14771129]\n",
      "Neurona 4: [ 0.00414993  0.29014022  0.2615971  -0.00890142  0.05149198  0.18610546\n",
      " -0.28955265  0.05970383 -0.19324072 -0.28813611  0.24128006 -0.29538596\n",
      " -0.16007272  0.12752734 -0.09965279 -0.23641908 -0.28909216  0.16222038\n",
      " -0.17259122  0.17135423 -0.10206014 -0.03474882  0.06041375  0.10863239\n",
      "  0.16529003 -0.07679601  0.03704202  0.17381989 -0.10515749  0.20370803\n",
      " -0.06365857  0.04804335 -0.21165297 -0.23533715  0.15333045  0.30286935\n",
      "  0.17578371  0.23833757 -0.15198861 -0.25606329 -0.2484609   0.12706858\n",
      "  0.01220539 -0.21232767 -0.0299012  -0.12355509 -0.23986017 -0.09418178\n",
      "  0.23286213  0.05999448  0.16137452  0.2011695   0.15214089 -0.04042431\n",
      " -0.24405663  0.25006068  0.09965746 -0.13372468  0.15171371 -0.2173068\n",
      " -0.23526706  0.20914966 -0.15692643  0.04286904]\n",
      "Neurona 5: [ 7.68274358e-05 -1.43971100e-01 -2.83503597e-01  2.09279542e-01\n",
      "  2.72376534e-01  1.48328641e-01 -7.12815682e-02 -2.96949422e-01\n",
      " -2.08111385e-01 -2.51943846e-01  2.70077557e-02  1.02691132e-02\n",
      " -1.23884215e-01 -1.26865528e-01 -1.20092940e-01 -2.76738592e-01\n",
      "  5.13546122e-03 -1.33712651e-01  1.58039373e-01  2.80004637e-01\n",
      " -1.87973727e-01 -8.80885489e-02  2.50356829e-01 -2.91779037e-01\n",
      " -5.24781883e-02  1.79853509e-01 -2.79354146e-02  2.80563277e-02\n",
      "  2.47530177e-01  1.43988340e-01  1.47248839e-01  2.10723512e-01\n",
      " -1.93771026e-02 -2.60153309e-01  2.84346796e-01  1.44737089e-01\n",
      " -1.50058834e-01  2.00058594e-01 -1.21494030e-03  2.14591562e-01\n",
      "  9.10115558e-02  3.07500679e-02  1.45064141e-01  2.80200874e-01\n",
      "  9.72829584e-02 -9.17841176e-02 -5.62869457e-02 -5.98920825e-02\n",
      " -1.77561527e-01 -3.03366841e-01 -1.51668401e-01  3.01695399e-01\n",
      " -9.51236264e-02 -2.76818753e-01 -1.53476643e-01  1.95082271e-01\n",
      " -2.14372537e-01  1.24645882e-01 -8.69710747e-02  2.01201782e-01\n",
      "  1.02051929e-01 -1.49824447e-01  7.91542882e-02  4.54522208e-02]\n",
      "Neurona 6: [ 1.33936470e-01  9.87745258e-03  1.90466034e-01 -1.65002419e-01\n",
      "  6.41784522e-03  9.94693471e-02  2.88670437e-01 -2.18757109e-01\n",
      "  1.12165437e-01 -4.84285020e-02 -1.05861696e-01  1.07567477e-01\n",
      "  2.96990454e-01 -2.81432492e-01  1.02482526e-01 -1.29330791e-02\n",
      "  6.34242807e-02  2.57982977e-02  2.45443872e-02  1.81131652e-01\n",
      " -1.72680569e-01  2.17070051e-01 -1.08591998e-01 -1.17089848e-01\n",
      "  3.86261554e-02  3.03173219e-01  2.99326279e-01 -2.32802329e-01\n",
      " -1.12635507e-01  1.26502510e-02  2.21370553e-01  1.64650096e-01\n",
      "  1.70040594e-01 -4.96408903e-02 -1.04399501e-01 -2.21519425e-01\n",
      "  2.68413088e-01 -1.85998913e-01 -2.75070025e-01 -4.07541367e-02\n",
      "  1.29733170e-01  3.37793670e-03 -9.26442766e-02  1.85113253e-04\n",
      "  4.31773499e-02 -8.02752564e-02  4.74377188e-02 -1.16394152e-01\n",
      " -2.78532701e-01  3.69951922e-02  2.59514574e-01 -2.37107964e-02\n",
      " -1.70272690e-01  1.64930372e-01  4.87932992e-02  2.08074683e-01\n",
      "  6.37290744e-02 -1.86362075e-01 -2.43651444e-02 -1.75131490e-01\n",
      "  2.57489639e-01  2.40260523e-01 -2.20350643e-01 -2.07529901e-02]\n",
      "Neurona 7: [-0.05451112 -0.03868541 -0.05132964  0.22079085  0.22911734  0.22979881\n",
      " -0.21683307 -0.15610498  0.05266878 -0.23833083 -0.29576701  0.15505148\n",
      "  0.17878724 -0.03944228  0.20443444 -0.16156468 -0.08294457 -0.11021514\n",
      " -0.18157381  0.18230768 -0.25773407  0.03396326  0.20912393 -0.21233892\n",
      "  0.1913108  -0.11980234 -0.14579867 -0.22603598 -0.25012452 -0.21011219\n",
      " -0.03942672  0.15796129  0.15787164 -0.01609437  0.11677597  0.1499853\n",
      "  0.02952814 -0.0725538   0.21106715  0.13495396  0.03711725  0.07783406\n",
      "  0.17866706  0.08279568 -0.18697581 -0.11904733 -0.13236653  0.04442901\n",
      " -0.16650263  0.17090815  0.1427369  -0.17899215 -0.03705959 -0.1591319\n",
      "  0.06804955 -0.22901817  0.05975219  0.10665501 -0.25110073 -0.14565854\n",
      "  0.14290979  0.29111666 -0.19721941  0.22360654]\n",
      "Neurona 8: [-0.01326933 -0.16249692 -0.17867027  0.03852909  0.20058101 -0.30164689\n",
      " -0.11964196 -0.08676494  0.27526292  0.26026343 -0.22148053  0.28352341\n",
      "  0.03919118  0.124234    0.13626346  0.12865705 -0.29246258  0.26325075\n",
      " -0.30307763 -0.05946931 -0.25993244  0.01167689  0.20043421 -0.2623142\n",
      " -0.21729413 -0.02427535 -0.25774676 -0.10198843 -0.01147533  0.21555765\n",
      " -0.28620972  0.13111058 -0.05629031  0.1665826   0.27629898 -0.18203208\n",
      " -0.18917002  0.18986314 -0.21257138 -0.11686717  0.27011069 -0.14329157\n",
      " -0.0426577  -0.19931852 -0.25220644 -0.29456073  0.26884781  0.07919546\n",
      " -0.25092329  0.25188382 -0.16251822  0.06745338  0.02277519 -0.15084184\n",
      " -0.27284165  0.16684936 -0.17441373  0.10675277  0.23749689 -0.06634504\n",
      " -0.29369793 -0.21584092 -0.01480437  0.0529854 ]\n",
      "Neurona 9: [-0.26685729  0.25059174 -0.25818501 -0.10552737  0.03743652  0.29408718\n",
      " -0.06020739 -0.179202    0.14202868 -0.17084907 -0.17511896  0.12060162\n",
      "  0.04533199  0.2656046   0.27824625  0.04913459 -0.15000427  0.05919537\n",
      "  0.11915057  0.1703984   0.13004627  0.12344983  0.25984181  0.00646952\n",
      " -0.19369795 -0.24498068  0.00728535  0.2077401   0.0038718  -0.21893213\n",
      "  0.14473072  0.11836295 -0.01327789  0.24331089  0.23528343  0.20095695\n",
      "  0.05096437  0.21837892  0.02583897 -0.20765385 -0.24405144 -0.16524644\n",
      " -0.0488659  -0.11589728  0.25791973  0.05985619 -0.24934077 -0.23575264\n",
      "  0.02348571 -0.13832243 -0.05104563 -0.22242567  0.0588598  -0.23869694\n",
      " -0.23071884  0.15755023  0.11795267  0.21089132  0.09824791 -0.15767976\n",
      " -0.06887727  0.08845684  0.05822982 -0.22024027]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m mlp \u001b[38;5;241m=\u001b[39m NeuralNetwork([input_size, hidden_size1, output_size], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Entrenamos la red (puede tardar unos minutos)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, epochs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l, capa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j, neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(capa):\n\u001b[1;32m---> 69\u001b[0m             neuron\u001b[38;5;241m.\u001b[39mupdate(activations[l], deltas[l][j])\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura: 1 capa oculta de 32 neuronas\n",
    "input_size = X_train.shape[1]  # 784 para MNIST\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32  # Capa oculta adicional\n",
    "output_size = 10  # 10 clases\n",
    "\n",
    "# Creamos la red\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red (puede tardar unos minutos)\n",
    "mlp.train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0e3d8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el subconjunto de test: 1851/2000\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de la capa de salida:\n",
      "Neurona 0: [-0.21095034  0.21576708  0.29364339  0.1400519   0.41996989 -0.22393205\n",
      "  0.2913735   0.23646594  0.26530075  0.18394691  0.41360217  0.15914489\n",
      "  0.28223752  0.36638669 -0.39185923 -0.37938075 -0.20005329 -0.30621715\n",
      "  0.05675134 -0.22032994 -0.21634552 -0.26723514 -0.30643823  0.13575966\n",
      " -0.33102104 -0.06599616 -0.3861617   0.20792701 -0.00717329 -0.28160243\n",
      "  0.09158698 -0.103872  ]\n",
      "Neurona 1: [-0.16258689 -0.31891716  0.25810096 -0.16217864  0.34511453  0.1320015\n",
      "  0.40096686 -0.40238846 -0.39156597  0.30287841  0.17026449  0.19696797\n",
      "  0.14788114  0.24497191  0.29448075  0.26878502 -0.16070508  0.35668421\n",
      " -0.0928961  -0.08774669  0.11814335 -0.24390586 -0.34729804  0.31618796\n",
      "  0.11423405  0.09544808 -0.07302767 -0.11148791  0.22848843  0.31001171\n",
      "  0.42480494  0.1443552 ]\n",
      "Neurona 2: [ 0.37127601  0.4249363  -0.28742857  0.06093525 -0.01758693  0.10126086\n",
      " -0.23223909  0.40852539 -0.03028234 -0.3604767   0.31772827  0.28893932\n",
      " -0.24312769  0.05129578  0.26969175  0.0391985   0.15313626  0.42144068\n",
      "  0.2688853  -0.07143612 -0.26404693 -0.01328568  0.21585275 -0.04788367\n",
      "  0.19792212 -0.2090628  -0.37031537  0.33391938  0.30134625  0.0052522\n",
      "  0.33459007  0.27804229]\n",
      "Neurona 3: [ 0.37899396 -0.4171307   0.03556479  0.19019152  0.3976786   0.06010846\n",
      "  0.17749125  0.1757819   0.16057187  0.22591123  0.15829333  0.05768311\n",
      " -0.26815013  0.26840693 -0.37357163  0.1771219  -0.28806002 -0.07394565\n",
      " -0.2889565  -0.04331997  0.39321045  0.39709885  0.00931518 -0.31389818\n",
      " -0.25168349  0.04967137 -0.02652176  0.41753616 -0.0051249  -0.0943857\n",
      " -0.27780121 -0.4234821 ]\n",
      "Neurona 4: [ 0.13136734 -0.37765904 -0.01117547 -0.42033265 -0.08396205  0.40387544\n",
      " -0.28032701 -0.39390667 -0.38347844 -0.38587728 -0.37038742  0.13606928\n",
      "  0.09340295 -0.38956987 -0.18398203 -0.35075348 -0.06326136 -0.07690168\n",
      "  0.25356676 -0.11211355 -0.13741584  0.02941849 -0.31429329  0.22104724\n",
      " -0.37960923  0.0005301   0.34562062 -0.03598007 -0.38636027  0.06016858\n",
      "  0.04419886 -0.055823  ]\n",
      "Neurona 5: [-0.25398973  0.34070841  0.33971344  0.3729176   0.32961663 -0.39374246\n",
      "  0.23360773  0.39997873 -0.35013111 -0.2914727   0.18733553  0.00162065\n",
      " -0.35072766 -0.35808819  0.0880193  -0.3319652   0.22829149  0.0048053\n",
      " -0.12838057  0.28476636 -0.27998124 -0.38404058  0.2079675  -0.38490558\n",
      "  0.2208227  -0.01060411  0.00755888  0.27210442  0.42154229 -0.36157065\n",
      "  0.06929336 -0.33175603]\n",
      "Neurona 6: [ 0.24993973 -0.22881339  0.1958947   0.09738864  0.3338206  -0.28984932\n",
      " -0.01882179 -0.18354443  0.08611941 -0.4165235   0.11205135  0.11834813\n",
      "  0.0757015   0.17601785  0.04826836 -0.10377596  0.07101011 -0.29632669\n",
      " -0.15428948  0.11789555  0.13328866 -0.23732501  0.12122868  0.22930756\n",
      " -0.27512026 -0.30463433  0.10709841 -0.39412784 -0.19821314  0.26734109\n",
      "  0.22620067 -0.42139944]\n",
      "Neurona 7: [ 0.03178387 -0.27597541  0.03608078 -0.21599902  0.12731676  0.31226166\n",
      "  0.22060737 -0.05643481 -0.3253856   0.22639979  0.40099584  0.39518362\n",
      " -0.24702676 -0.37346139 -0.13261443 -0.09449139 -0.14571098 -0.06968641\n",
      "  0.08123439 -0.05632223 -0.3007713   0.42324467 -0.10429501 -0.2325538\n",
      " -0.32983832 -0.21854451 -0.06562586 -0.04302399 -0.42297443 -0.37942697\n",
      "  0.4101528   0.08924909]\n",
      "Neurona 8: [ 0.37485796  0.17468789 -0.31226224 -0.35414041  0.27418693 -0.25611066\n",
      " -0.20797296  0.05968424 -0.16049738 -0.31808827  0.11569943  0.28468906\n",
      " -0.09227756 -0.23528943 -0.14808744 -0.26464792 -0.17678512  0.31628624\n",
      " -0.25758471 -0.29718335 -0.11908136  0.42029681  0.37547671  0.29717792\n",
      " -0.42465423 -0.08020291  0.08566429 -0.32355356  0.21404256 -0.39315969\n",
      " -0.34274392 -0.04930978]\n",
      "Neurona 9: [ 0.38958511 -0.18976558  0.18849205  0.10407142 -0.07242062 -0.3444531\n",
      " -0.19201962 -0.38527309 -0.00344177  0.19301879 -0.2262147  -0.26746905\n",
      "  0.03663826  0.02268849 -0.2459798   0.41623933  0.41788777  0.38572505\n",
      "  0.23937462 -0.06480964 -0.0331468   0.11570269  0.24507327  0.19830637\n",
      "  0.20595668  0.18609696 -0.27580442 -0.01747599 -0.21762411 -0.30015613\n",
      "  0.18567205 -0.14850412]\n",
      "Epoch 1/100 completada\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m mlp \u001b[38;5;241m=\u001b[39m NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Entrenamos la red\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (Opcional) Evaluar en el conjunto de prueba\u001b[39;00m\n\u001b[0;32m     21\u001b[0m mlp\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, epochs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m         activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         deltas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m activations \u001b[38;5;241m=\u001b[39m [x]   \u001b[38;5;66;03m# Inicialmente arranca con el input (x, vector de entrada)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m capa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 33\u001b[0m     salida_capa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m capa])\n\u001b[0;32m     34\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(salida_capa)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mPerceptron.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Propagación hacia adelante de una sola muestra\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Calcula la suma ponderada y aplica la función de activación\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cargamos los datos\n",
    "X_train, y_train, X_test, y_test = load_cifar10(f\"data/CIFAR-10\")\n",
    "\n",
    "# Aplanamos las imágenes (de 32x32x3 a 3072)\n",
    "X_train = X_train.reshape(-1, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "# Definimos la arquitectura: dos capas ocultas\n",
    "input_size = X_train.shape[1]  # 3072 para CIFAR-10\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 10  # 10 clases para CIFAR-10\n",
    "\n",
    "# Creamos la red con dos capas ocultas\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red\n",
    "mlp.train(X_train, y_train, epochs=100)\n",
    "\n",
    "# (Opcional) Evaluar en el conjunto de prueba\n",
    "mlp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la red en el subconjunto de test\n",
    "correct = 0\n",
    "for xi, yi in zip(X_test, y_test):\n",
    "    pred = mlp.predict(xi)\n",
    "    if np.argmax(pred) == np.argmax(yi):\n",
    "        correct += 1\n",
    "print(f\"Precisión en el subconjunto de test: {correct}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de dataset CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "X_train, y_train, X_test, y_test = load_cifar100(f\"data/CIFAR-100\")\n",
    "\n",
    "# Aplanamos las imágenes (de 32x32x3 a 3072)\n",
    "X_train = X_train.reshape(-1, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "# Definimos la arquitectura: dos capas ocultas\n",
    "input_size = X_train.shape[1]  # 3072 para CIFAR-100\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 100  # 100 clases para CIFAR-100\n",
    "\n",
    "# Creamos la red con dos capas ocultas\n",
    "mlp = NeuralNetwork([input_size, hidden_size1, hidden_size2, output_size], learning_rate=0.01)\n",
    "\n",
    "# Entrenamos la red\n",
    "mlp.train(X_train, y_train, epochs=100)\n",
    "\n",
    "# (Opcional) Evaluar en el conjunto de prueba\n",
    "mlp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación con Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5cefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360782e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
